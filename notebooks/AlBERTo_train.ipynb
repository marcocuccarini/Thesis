{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "stralcio + domanda -> repertorio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import neptune.new as neptune\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Info (NVML): NVML Shared Library Not Found. GPU usage metrics may not be reported. For more information, see https://docs-legacy.neptune.ai/logging-and-managing-experiment-results/logging-experiment-data.html#hardware-consumption \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/mibo8/Rep/e/REP-66\n",
      "Remember to stop your run once you’ve finished logging your metadata (https://docs.neptune.ai/api-reference/run#stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n"
     ]
    }
   ],
   "source": [
    "#Neptune initialization\n",
    "run = neptune.init(\n",
    "    project=\"mibo8/Rep\",\n",
    "    api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJmZmRkYThiZi1mZGNlLTRlMTktODQwNS1hNWFlMWQ2Mjc4N2IifQ==\",\n",
    ")  # your credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deterministic mode\n",
    "\n",
    "def seed_everything(seed=1234):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "#seed_everything()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset loading and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_null_features(df):\n",
    "    for c in ['Domanda','Testo']:\n",
    "        for i in range(0,len(df.index)):  \n",
    "            if not df[c][i]:\n",
    "                j=i\n",
    "                while j>0: \n",
    "                    j-=1\n",
    "                    if df[c][j]:\n",
    "                        df[c][i] = df[c][j]\n",
    "                        break\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero stralci: 35474\n",
      "Numero stralci dopo eliminazione: 35148\n"
     ]
    }
   ],
   "source": [
    "#Hyperion dataset\n",
    "df = pd.read_csv('../data/Original_csv/Hyperion.csv', na_filter=False)\n",
    "df = fill_null_features(df)\n",
    "\n",
    "print('Numero stralci: ' + str(len(df.index)))\n",
    "filter = df[\"Repertorio\"] != \"\"\n",
    "df = df[filter]\n",
    "print('Numero stralci dopo eliminazione: ' + str(len(df.index)))\n",
    "\n",
    "# lower entire dataset\n",
    "df = df.applymap(str.lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lables uniformation uncased\n",
    "\n",
    "df['Repertorio'].replace('implicazioni','implicazione', inplace=True)\n",
    "df['Repertorio'].replace('previsioni','previsione', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split\n",
    "test_set_perc = 0.2\n",
    "val_set_perc = 0.1\n",
    "\n",
    "gb = df.groupby('Repertorio')\n",
    "train_list = []\n",
    "test_list = []\n",
    "val_list = []\n",
    "\n",
    "for x in gb.groups:\n",
    "    class_df = gb.get_group(x)\n",
    "\n",
    "    # Test set creation\n",
    "    test = class_df.sample(frac=test_set_perc, random_state=1464)\n",
    "    train = pd.concat([class_df,test]).drop_duplicates(keep=False)\n",
    "\n",
    "    # Validation set creation\n",
    "    val = train.sample(frac=val_set_perc)\n",
    "    train = pd.concat([train,val]).drop_duplicates(keep=False)\n",
    "\n",
    "    train_list.append(train)\n",
    "    test_list.append(test)\n",
    "    val_list.append(val)\n",
    "\n",
    "train_df = pd.concat(train_list)\n",
    "test_df = pd.concat(test_list)\n",
    "val_df = pd.concat(val_list)\n",
    "\n",
    "dataset_info = {\n",
    "    'training_set_size' : 1 - test_set_perc,\n",
    "    'validation_set_size' : val_set_perc,\n",
    "    'test_set_size' : test_set_perc\n",
    "}\n",
    "run['dataset'] = dataset_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class HyperionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        \n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X = Domanda + stralcio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 625/625 [00:00<00:00, 223kB/s]\n",
      "Downloading: 100%|██████████| 1.06M/1.06M [00:04<00:00, 264kB/s] \n",
      "Downloading: 100%|██████████| 706M/706M [02:12<00:00, 5.61MB/s] \n",
      "Some weights of the model checkpoint at m-polignano-uniba/bert_uncased_L-12_H-768_A-12_italian_alb3rt0 were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at m-polignano-uniba/bert_uncased_L-12_H-768_A-12_italian_alb3rt0 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "  \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"m-polignano-uniba/bert_uncased_L-12_H-768_A-12_italian_alb3rt0\")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"m-polignano-uniba/bert_uncased_L-12_H-768_A-12_italian_alb3rt0\", num_labels=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "run['model'] = \"bert-base-multilingual-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "training_encodings = tokenizer(\n",
    "            train_df['Stralcio'][:100].tolist(),\n",
    "            add_special_tokens=True,\n",
    "            return_attention_mask=True,\n",
    "            padding=True,\n",
    "            truncation=True\n",
    "        )\n",
    "test_encodings = tokenizer(\n",
    "            test_df['Stralcio'][:100].tolist(),\n",
    "            add_special_tokens=True,\n",
    "            return_attention_mask=True,\n",
    "            padding=True,\n",
    "            truncation=True\n",
    ")\n",
    "val_encodings = tokenizer(\n",
    "            val_df['Stralcio'][:100].tolist(),\n",
    "            add_special_tokens=True,\n",
    "            return_attention_mask=True,\n",
    "            padding=True,\n",
    "            truncation=True\n",
    ")\n",
    "\n",
    "\n",
    "labels = [\n",
    "    'anticipazione',\n",
    "    'causa',\n",
    "    'commento',\n",
    "    'conferma',\n",
    "    'considerazione',\n",
    "    'contrapposizione',\n",
    "    'deresponsabilizzazione',\n",
    "    'descrizione',\n",
    "    'dichiarazione di intenti',\n",
    "    'generalizzazione',\n",
    "    'giudizio',\n",
    "    'giustificazione',\n",
    "    'implicazione',\n",
    "    'non risposta',\n",
    "    'opinione',\n",
    "    'possibilità',\n",
    "    'prescrizione',\n",
    "    'previsione',\n",
    "    'proposta',\n",
    "    'ridimensionamento',\n",
    "    'sancire',\n",
    "    'specificazione',\n",
    "    'valutazione']\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(labels)\n",
    "\n",
    "train_dataset = HyperionDataset(training_encodings,le.transform(train_df['Repertorio'][:100]))\n",
    "test_dataset = HyperionDataset(test_encodings,le.transform(test_df['Repertorio'][:100]))\n",
    "val_dataset = HyperionDataset(val_encodings,le.transform(val_df['Repertorio'][:100]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  torchmetrics\n",
    "\n",
    "# Metrics initialization\n",
    "metric_collection = torchmetrics.MetricCollection({\n",
    "\n",
    "    'accuracy_micro' : torchmetrics.Accuracy(num_classes=23, multiclass=True, average='micro'),\n",
    "    'accuracy_macro' : torchmetrics.Accuracy(num_classes=23, multiclass=True, average='macro'),\n",
    "    'accuracy_weighted' : torchmetrics.Accuracy(num_classes=23, multiclass=True, average='weighted'),\n",
    "    'accuracy_none' : torchmetrics.Accuracy(num_classes=23, multiclass=True, average='none'),\n",
    "\n",
    "    'f1_micro' : torchmetrics.F1(num_classes=23, multiclass=True, average='micro'),\n",
    "    'f1_macro' : torchmetrics.F1(num_classes=23, multiclass=True, average='macro'),\n",
    "    'f1_weighted' : torchmetrics.F1(num_classes=23, multiclass=True, average='weighted'),\n",
    "    'f1_none' : torchmetrics.F1(num_classes=23, multiclass=True, average='none'),\n",
    "\n",
    "    'precision_micro' : torchmetrics.Precision(num_classes=23, multiclass=True, average='micro'),\n",
    "    'precision_macro' : torchmetrics.Precision(num_classes=23, multiclass=True, average='macro'),\n",
    "    'precision_weighted' : torchmetrics.Precision(num_classes=23, multiclass=True, average='weighted'),\n",
    "    'precision_none' : torchmetrics.Precision(num_classes=23, multiclass=True, average='none'),\n",
    "\n",
    "    'recall_micro' : torchmetrics.Recall(num_classes=23, multiclass=True, average='micro'),\n",
    "    'recall_macro' : torchmetrics.Recall(num_classes=23, multiclass=True, average='macro'),\n",
    "    'recall_weighted' : torchmetrics.Recall(num_classes=23, multiclass=True, average='weighted'),\n",
    "    'recall_none' : torchmetrics.Recall(num_classes=23, multiclass=True, average='none')\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 1e-5\n",
    "batch_size = 4\n",
    "n_epochs = 1\n",
    "\n",
    "params_info = {\n",
    "    'learning_rate' : learning_rate,\n",
    "    'batch_size' : batch_size,\n",
    "    'n_epochs' : n_epochs\n",
    "}\n",
    "run['params'] = params_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Repertorio\n",
       "anticipazione                 38\n",
       "causa                        340\n",
       "commento                    3153\n",
       "conferma                     460\n",
       "considerazione               225\n",
       "contrapposizione            1015\n",
       "deresponsabilizzazione       455\n",
       "descrizione                 3502\n",
       "dichiarazione di intenti     463\n",
       "generalizzazione             768\n",
       "giudizio                    1986\n",
       "giustificazione              316\n",
       "implicazione                 509\n",
       "non risposta                 835\n",
       "opinione                     965\n",
       "possibilità                  444\n",
       "prescrizione                1381\n",
       "previsione                   840\n",
       "proposta                     185\n",
       "ridimensionamento            792\n",
       "sancire                     4045\n",
       "specificazione              1004\n",
       "valutazione                 1528\n",
       "Name: Repertorio, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.groupby('Repertorio')['Repertorio'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 1 ========\n",
      "Training...\n",
      "  Batch    10  of     25.    Elapsed: 0:00:36.\n",
      "  Batch    20  of     25.    Elapsed: 0:01:15.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8473/2779242922.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;31m# Update parameters and take a step using the computed gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/transformers/optimization.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    349\u001b[0m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m                 \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"eps\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"lr\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import  AdamW\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Measure the total training time for the whole run.\n",
    "total_t0 = time.time()\n",
    "\n",
    "epochs = n_epochs\n",
    "\n",
    "# Creation of Pytorch DataLoaders with shuffle=True for the traing phase\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "validation_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "#Adam algorithm optimized for tranfor architectures\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Setup for training with gpu\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "# For each epoch...\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    # Perform one full pass over the training set.\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_train_loss = 0\n",
    "\n",
    "    # Put the model into training mode: Dropout layers are active\n",
    "    model.train()\n",
    "    \n",
    "    # For each batch of training data...\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        # Progress update every 40 batches.\n",
    "        if step % 10 == 0 and not step == 0:\n",
    "            # Compute time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        # Unpack this training batch from the dataloader. \n",
    "        #\n",
    "        #  copy each tensor to the GPU using the 'to()' method\n",
    "        #\n",
    "        # 'batch' contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch['input_ids'].to(device)\n",
    "        b_input_mask = batch['attention_mask'].to(device)\n",
    "        b_labels = batch['labels'].to(device)\n",
    "\n",
    "        # clear any previously calculated gradients before performing a\n",
    "        # backward pass\n",
    "        model.zero_grad()        \n",
    "\n",
    "        # Perform a forward pass (evaluate the model on this training batch).\n",
    "        # This will return the loss (rather than the model output) because we\n",
    "        # have provided the `labels`.\n",
    "        outputs = model(b_input_ids, \n",
    "                             attention_mask=b_input_mask, \n",
    "                             labels=b_labels)\n",
    "        \n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "\n",
    "        # Accumulate the training loss over all of the batches so that we can\n",
    "        # calculate the average loss at the end.\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu()\n",
    "        label_ids = b_labels.to('cpu')\n",
    "\n",
    "        batch_metric = metric_collection.update(logits.softmax(dim=1), label_ids)\n",
    "        #print(batch_metric)\n",
    "\n",
    "        # Perform a backward pass to compute the gradients.\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the norm of the gradients to 1.0.\n",
    "        # This helps and prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Update parameters and take a step using the computed gradient\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "    # Compute the average loss over all of the batches.\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "\n",
    "    final_metrics = metric_collection.compute()\n",
    "    print(final_metrics)\n",
    "    \n",
    "    # Measure how long this epoch took.\n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.3f}\".format(avg_train_loss))\n",
    "    print(\"  Training epoch took: {:}\".format(training_time))\n",
    "\n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure performance on\n",
    "    # the validation set.\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    metric_collection.reset()\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Put the model in evaluation mode: the dropout layers behave differently\n",
    "    model.eval()\n",
    "\n",
    "    total_val_loss = 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "        \n",
    "        # Unpack this training batch from our dataloader. \n",
    "        #\n",
    "        # copy each tensor to the GPU using the 'to()' method\n",
    "        #\n",
    "        # 'batch' contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch['input_ids'].to(device)\n",
    "        b_input_mask = batch['attention_mask'].to(device)\n",
    "        b_labels = batch['labels'].to(device)\n",
    "        \n",
    "        # Tell pytorch not to bother with constructing the compute graph during\n",
    "        # the forward pass, since this is only needed for training.\n",
    "        with torch.no_grad():        \n",
    "\n",
    "            # Forward pass, calculate logits\n",
    "            # argmax(logits) = argmax(Softmax(logits))\n",
    "            outputs = model(b_input_ids, \n",
    "                                   attention_mask=b_input_mask,\n",
    "                                   labels=b_labels)\n",
    "            loss = outputs[0]\n",
    "            logits = outputs[1]\n",
    "            \n",
    "        # Accumulate the validation loss.\n",
    "        total_val_loss += loss.item()\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu()\n",
    "        label_ids = b_labels.to('cpu')\n",
    "\n",
    "        # metric on current batch\n",
    "        batch_metric = metric_collection.update(logits.softmax(dim=1), label_ids)\n",
    "\n",
    "    # Report the final metrics for this validation phase.\n",
    "    # metric on all batches using custom accumulation from torchmetrics library\n",
    "\n",
    "    final_metrics = metric_collection.compute()\n",
    "    print('VALIDATION: ')\n",
    "    print(final_metrics)\n",
    "    # Compute the average loss over all of the batches.\n",
    "    avg_val_loss = total_val_loss / len(validation_dataloader)\n",
    "    \n",
    "    # Measure how long the validation run took.\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "    \n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running Tets...\n",
      " Test metrics: \n",
      "{'accuracy_macro': tensor(0.3262), 'accuracy_micro': tensor(0.4600), 'accuracy_none': tensor([0.0000, 0.9787, 0.0000,    nan,    nan,    nan,    nan,    nan,    nan,\n",
      "           nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
      "           nan,    nan,    nan,    nan,    nan]), 'accuracy_weighted': tensor(0.4600), 'f1_macro': tensor(0.2145), 'f1_micro': tensor(0.4600), 'f1_none': tensor([0.0000, 0.6434, 0.0000,    nan,    nan,    nan,    nan,    nan,    nan,\n",
      "           nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
      "           nan,    nan,    nan,    nan,    nan]), 'f1_weighted': tensor(0.3024), 'precision_macro': tensor(0.1597), 'precision_micro': tensor(0.4600), 'precision_none': tensor([0.0000, 0.4792, 0.0000,    nan,    nan,    nan,    nan,    nan,    nan,\n",
      "           nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
      "           nan,    nan,    nan,    nan,    nan]), 'precision_weighted': tensor(0.2252), 'recall_macro': tensor(0.3262), 'recall_micro': tensor(0.4600), 'recall_none': tensor([0.0000, 0.9787, 0.0000,    nan,    nan,    nan,    nan,    nan,    nan,\n",
      "           nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
      "           nan,    nan,    nan,    nan,    nan]), 'recall_weighted': tensor(0.4600)}\n",
      "  Test Loss: 2.09\n",
      "  Test took: 0:00:17\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "#               Test\n",
    "# ========================================\n",
    "# Measure performance on\n",
    "# the validation set.\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(\"\")\n",
    "print(\"Running Test...\")\n",
    "\n",
    "metric_collection.reset()\n",
    "t0 = time.time()\n",
    "\n",
    "# Put the model in evaluation mode: the dropout layers behave differently\n",
    "model.eval()\n",
    "\n",
    "total_test_loss = 0\n",
    "\n",
    "# Evaluate data for one epoch\n",
    "for batch in test_dataloader:\n",
    "    \n",
    "    # Unpack this training batch from our dataloader. \n",
    "    #\n",
    "    # copy each tensor to the GPU using the 'to()' method\n",
    "    #\n",
    "    # 'batch' contains three pytorch tensors:\n",
    "    #   [0]: input ids \n",
    "    #   [1]: attention masks\n",
    "    #   [2]: labels \n",
    "    b_input_ids = batch['input_ids'].to(device)\n",
    "    b_input_mask = batch['attention_mask'].to(device)\n",
    "    b_labels = batch['labels'].to(device)\n",
    "    \n",
    "    # Tell pytorch not to bother with constructing the compute graph during\n",
    "    # the forward pass, since this is only needed for training.\n",
    "    with torch.no_grad():        \n",
    "\n",
    "        # Forward pass, calculate logits\n",
    "        # argmax(logits) = argmax(Softmax(logits))\n",
    "        outputs = model(b_input_ids, \n",
    "                                attention_mask=b_input_mask,\n",
    "                                labels=b_labels)\n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "        \n",
    "    # Accumulate the validation loss.\n",
    "    total_test_loss += loss.item()\n",
    "\n",
    "    # Move logits and labels to CPU\n",
    "    logits = logits.detach().cpu()\n",
    "    label_ids = b_labels.to('cpu')\n",
    "\n",
    "    # metric on current batch\n",
    "\n",
    "    batch_metric = metric_collection.update(logits.softmax(dim=1), label_ids)\n",
    "\n",
    "# Report the final metrics for this validation phase.\n",
    "# metric on all batches using custom accumulation from torchmetrics library\n",
    "\n",
    "test_metrics = metric_collection.compute()\n",
    "print(' Test metrics: ')\n",
    "print(final_metrics)\n",
    "\n",
    "run['metrics'] = final_metrics\n",
    "# Compute the average loss over all of the batches.\n",
    "avg_test_loss = total_test_loss / len(test_dataloader)\n",
    "\n",
    "run['test/loss'] = avg_test_loss\n",
    "\n",
    "# Measure how long the validation run took.\n",
    "test_time = format_time(time.time() - t0)\n",
    "\n",
    "print(\"  Test Loss: {0:.2f}\".format(avg_test_loss))\n",
    "print(\"  Test took: {:}\".format(test_time))\n",
    "\n",
    "#torch.save(model.state_dict(), \"./\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Waiting for the remaining 1 operations to synchronize with Neptune. Do not kill this process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All 1 operations synced, thanks for waiting!\n"
     ]
    }
   ],
   "source": [
    "run.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7500)\n",
      "tensor(0.7667)\n",
      "tensor(0.7500)\n",
      "tensor(0.7500)\n",
      "tensor(0.7667)\n",
      "tensor(0.7500)\n",
      "tensor(0.7500)\n",
      "tensor(0.7667)\n",
      "tensor(0.7500)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import  torchmetrics\n",
    "# Metrics initialization\n",
    "metric_collection = torchmetrics.MetricCollection({\n",
    "\n",
    "    'accuracy_micro' : torchmetrics.Accuracy(num_classes=3, multiclass=True, average='micro'),\n",
    "    'accuracy_macro' : torchmetrics.Accuracy(num_classes=3, multiclass=True, average='macro'),\n",
    "    'accuracy_weighted' : torchmetrics.Accuracy(num_classes=3, multiclass=True, average='weighted'),\n",
    "    'accuracy_none' : torchmetrics.Accuracy(num_classes=3, multiclass=True, average='none'),\n",
    "\n",
    "    'f1_micro' : torchmetrics.F1(num_classes=3, multiclass=True, average='micro'),\n",
    "    'f1_macro' : torchmetrics.F1(num_classes=3, multiclass=True, average='macro'),\n",
    "    'f1_weighted' : torchmetrics.F1(num_classes=3, multiclass=True, average='weighted'),\n",
    "    'f1_none' : torchmetrics.F1(num_classes=3, multiclass=True, average='none'),\n",
    "\n",
    "    'precision_micro' : torchmetrics.Precision(num_classes=3, multiclass=True, average='micro'),\n",
    "    'precision_macro' : torchmetrics.Precision(num_classes=3, multiclass=True, average='macro'),\n",
    "    'precision_weighted' : torchmetrics.Precision(num_classes=3, multiclass=True, average='weighted'),\n",
    "    'precision_none' : torchmetrics.Precision(num_classes=3, multiclass=True, average='none'),\n",
    "\n",
    "    'recall_micro' : torchmetrics.Recall(num_classes=3, multiclass=True, average='micro'),\n",
    "    'recall_macro' : torchmetrics.Recall(num_classes=3, multiclass=True, average='macro'),\n",
    "    'recall_weighted' : torchmetrics.Recall(num_classes=3, multiclass=True, average='weighted'),\n",
    "    'recall_none' : torchmetrics.Recall(num_classes=3, multiclass=True, average='none')\n",
    "})\n",
    "metric_collection.reset()\n",
    "\n",
    "#target = torch.tensor([0, 2, 0, 2, 0, 1, 0, 2])\n",
    "#preds = torch.tensor([2, 1, 2, 0, 1, 2, 2, 2])\n",
    "\n",
    "#metric_collection.update(preds, target)\n",
    "\n",
    "target =torch.tensor([2, 2, 2, 2, 1, 1, 0, 2])\n",
    "preds = torch.tensor([2, 1, 2, 2, 1, 2, 0, 2])\n",
    "\n",
    "metric_collection.update(preds, target)\n",
    "\n",
    "metric = metric_collection.compute()\n",
    "print(metric['accuracy_micro'])     \n",
    "print(metric['accuracy_macro'])\n",
    "print(metric['accuracy_weighted'])     \n",
    "print(metric['precision_micro'])    \n",
    "print(metric['precision_macro'])  \n",
    "print(metric['precision_weighted']) # 1/1*1/8 + 1/2*2/8 + 4/5*5/8\n",
    "print(metric['recall_micro'])\n",
    "print(metric['recall_macro'])\n",
    "print(metric['recall_weighted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7777777777777777"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4/4*4/9 + 2/3*3/9 + 1/2*2/9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7777777777777778"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "7/9"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "531bf1ddd0b9ee64f0e2ebd6527c520de4e2159f2ffcc26ba5e4b06431092b93"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
