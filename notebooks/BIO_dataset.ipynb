{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('../data/Original_csv/Hyperion.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Domanda</th>\n",
       "      <th>Testo</th>\n",
       "      <th>Stralcio</th>\n",
       "      <th>Repertorio</th>\n",
       "      <th>Contesto</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A cosa serve questo testo per il perseguimento...</td>\n",
       "      <td>Dunque vediamo se ho capito: Conte le canta a ...</td>\n",
       "      <td>Dunque vediamo se ho capito:</td>\n",
       "      <td>dichiarazione di intenti</td>\n",
       "      <td>Dunque vediamo se ho capito: Conte le canta a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Conte le canta a Salvini e alla Meloni</td>\n",
       "      <td>sancire</td>\n",
       "      <td>Dunque vediamo se ho capito:  questi reagiscon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>questi reagiscono e le cantano a loro volta a ...</td>\n",
       "      <td>giustificazione</td>\n",
       "      <td>Conte le canta a Salvini e alla Meloni  Mentan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mentana si smarca da Conte e finisce con foto ...</td>\n",
       "      <td>sancire</td>\n",
       "      <td>questi reagiscono e le cantano a loro volta a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Siamo tornati per un attimo alla normalità:</td>\n",
       "      <td>commento</td>\n",
       "      <td>Mentana si smarca da Conte e finisce con foto ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Domanda  \\\n",
       "0  A cosa serve questo testo per il perseguimento...   \n",
       "1                                                NaN   \n",
       "2                                                NaN   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "\n",
       "                                               Testo  \\\n",
       "0  Dunque vediamo se ho capito: Conte le canta a ...   \n",
       "1                                                NaN   \n",
       "2                                                NaN   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "\n",
       "                                            Stralcio  \\\n",
       "0                       Dunque vediamo se ho capito:   \n",
       "1             Conte le canta a Salvini e alla Meloni   \n",
       "2  questi reagiscono e le cantano a loro volta a ...   \n",
       "3  Mentana si smarca da Conte e finisce con foto ...   \n",
       "4        Siamo tornati per un attimo alla normalità:   \n",
       "\n",
       "                 Repertorio                                           Contesto  \n",
       "0  dichiarazione di intenti  Dunque vediamo se ho capito: Conte le canta a ...  \n",
       "1                   sancire  Dunque vediamo se ho capito:  questi reagiscon...  \n",
       "2           giustificazione  Conte le canta a Salvini e alla Meloni  Mentan...  \n",
       "3                   sancire  questi reagiscono e le cantano a loro volta a ...  \n",
       "4                  commento  Mentana si smarca da Conte e finisce con foto ...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def find_word_bounds(spans: list, text: str) -> list:\n",
    "    bounds = []\n",
    "    end = 0\n",
    "    for span in spans:\n",
    "        s = span.translate(str.maketrans('', '', string.punctuation))\n",
    "        word_list = s.split()\n",
    "        text_list = text.translate(str.maketrans('', '', string.punctuation)).split()\n",
    "        try:\n",
    "            start = text_list.index(word_list[0], end)\n",
    "        except:\n",
    "            if not bounds:\n",
    "                start = 0\n",
    "            else:\n",
    "                \n",
    "                start = bounds[-1][1] + 1\n",
    "        end = start + len(word_list) - 1\n",
    "            \n",
    "        bounds.append((start, end))\n",
    "    return bounds\n",
    "\n",
    "def IE_gen(bounds:list) -> str:\n",
    "    end = [bound[1] for bound in bounds]\n",
    "    x = ['E' if i in end else 'I' for i in range(end[-1]+1)]\n",
    "    return ''.join(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "\n",
    "for row in df.itertuples():\n",
    "    text = row.Testo\n",
    "    if pd.isna(text):\n",
    "        sample['Stralci'].append(row.Stralcio)\n",
    "    else: \n",
    "        sample = {}\n",
    "        sample['Testo'] = text\n",
    "        sample['Stralci'] = [row.Stralcio]\n",
    "        dataset.append(sample)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "for sample in dataset:\n",
    "    sample['Bounds'] = find_word_bounds(sample['Stralci'], sample['Testo'])\n",
    "    sample['Tags'] = IE_gen(sample['Bounds'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "IE_dict = {\n",
    "    'Testo' : [sample['Testo'] for sample in dataset],\n",
    "    'Tags' : [sample['Tags'] for sample in dataset] \n",
    "}\n",
    "IE_df = pd.DataFrame(IE_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Testo</th>\n",
       "      <th>Tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dunque vediamo se ho capito: Conte le canta a ...</td>\n",
       "      <td>IIIIEIIIIIIIEIIIIIIIIIEIIIIIIIIIIIIIIEIIIIIIEIE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#Conte è un pericolo per la libertà e la democ...</td>\n",
       "      <td>IIIIIIIIIE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Comunque, se tutti i giornalisti che oggi si s...</td>\n",
       "      <td>EIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIEIIIIE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"Ha suscitato scandalo la proposta di Salvini ...</td>\n",
       "      <td>IIIIIIIIIIIIEIIIIIIIIIIIIIIIIIIIIEIIIIIIIIIIII...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Non era mai successo nella lunga storia del Pi...</td>\n",
       "      <td>IIIIIIIIIIIIIIIIIIIIIEIIIIIIIIIIIIIEIIEIIIIIII...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Testo  \\\n",
       "0  Dunque vediamo se ho capito: Conte le canta a ...   \n",
       "1  #Conte è un pericolo per la libertà e la democ...   \n",
       "2  Comunque, se tutti i giornalisti che oggi si s...   \n",
       "3  \"Ha suscitato scandalo la proposta di Salvini ...   \n",
       "4  Non era mai successo nella lunga storia del Pi...   \n",
       "\n",
       "                                                Tags  \n",
       "0    IIIIEIIIIIIIEIIIIIIIIIEIIIIIIIIIIIIIIEIIIIIIEIE  \n",
       "1                                         IIIIIIIIIE  \n",
       "2         EIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIEIIIIE  \n",
       "3  IIIIIIIIIIIIEIIIIIIIIIIIIIIIIIIIIEIIIIIIIIIIII...  \n",
       "4  IIIIIIIIIIIIIIIIIIIIIEIIIIIIIIIIIIIEIIEIIIIIII...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IE_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "LABELS = [\n",
    "                'I',\n",
    "                'E'\n",
    "        ]\n",
    "\n",
    "def encode_labels(labels_list):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(LABELS)\n",
    "    return le.transform(labels_list)\n",
    "\n",
    "def first_token_label_econding(sample, labels):\n",
    "    encoded_labels = np.ones(len(sample[\"offset_mapping\"]), dtype=int) * -100\n",
    "    # set only labels whose first offset position is 0 and the second is not 0\n",
    "    i = 0\n",
    "    for idx, mapping in enumerate(sample[\"offset_mapping\"]):\n",
    "        if mapping[0] == 0 and mapping[1] != 0:\n",
    "            # overwrite label\n",
    "            encoded_labels[idx] = labels[i]\n",
    "            i += 1\n",
    "\n",
    "\n",
    "class IE_Hyperion_dataset(Dataset):\n",
    "    def __init__(self, df, tokenizer_name):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "        self.df = df\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.df['Testo'].iloc[idx]\n",
    "        encoding = self.tokenizer(text,\n",
    "                            #is_pretokenized=True, \n",
    "                            return_offsets_mapping=True, \n",
    "                            add_special_tokens=True,\n",
    "                            return_attention_mask=True,\n",
    "                            padding='max_lenght',\n",
    "                            truncation=True,\n",
    "                        )\n",
    "        labels = encode_labels(list(self.df['Tags'].iloc[idx]))\n",
    "        encoded_labels = np.ones(len(encoding[\"offset_mapping\"]), dtype=int) * -100\n",
    "        \n",
    "        # set only labels whose first offset position is 0 and the second is not 0\n",
    "        i = 0\n",
    "        print(encoding[\"offset_mapping\"])\n",
    "        for idx, mapping in enumerate(encoding[\"offset_mapping\"]):\n",
    "            if mapping[0] == 0 and mapping[1] != 0:\n",
    "                # overwrite label\n",
    "                encoded_labels[idx] = labels[i]\n",
    "                i += 1\n",
    "        \n",
    "        item = {key: torch.as_tensor(val) for key, val in encoding.items()}\n",
    "        item['labels'] = torch.as_tensor(encoded_labels)\n",
    "        return item\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL Dataset: (15332, 2)\n",
      "TRAIN Dataset: (12266, 2)\n",
      "TEST Dataset: (3066, 2)\n"
     ]
    }
   ],
   "source": [
    "model_name = \"bert-base-multilingual-cased\"\n",
    "train_size = 0.8\n",
    "train_df = IE_df.sample(frac=train_size,random_state=200)\n",
    "test_df = IE_df.drop(train_df.index).reset_index(drop=True)\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "\n",
    "print(\"FULL Dataset: {}\".format(IE_df.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_df.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_df.shape))\n",
    "\n",
    "train_dataset = IE_Hyperion_dataset(IE_df, model_name) \n",
    "test_dataset = IE_Hyperion_dataset(IE_df, model_name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0), (0, 2), (2, 6), (7, 10), (10, 14), (15, 17), (18, 20), (21, 24), (24, 27), (27, 28), (29, 34), (35, 37), (38, 43), (44, 45), (46, 49), (49, 53), (54, 55), (56, 60), (61, 65), (65, 67), (67, 68), (69, 75), (76, 78), (78, 81), (81, 86), (87, 88), (89, 91), (92, 97), (97, 99), (100, 101), (102, 106), (107, 112), (113, 114), (115, 120), (120, 121), (122, 125), (125, 129), (130, 132), (133, 135), (135, 137), (137, 139), (140, 142), (143, 148), (149, 150), (151, 155), (155, 158), (159, 162), (163, 167), (168, 175), (175, 179), (180, 185), (186, 192), (193, 199), (200, 205), (206, 210), (210, 212), (212, 213), (214, 218), (218, 219), (220, 225), (225, 227), (228, 231), (232, 234), (235, 239), (239, 241), (242, 246), (247, 253), (253, 256), (256, 257), (258, 262), (262, 265), (265, 268), (269, 277), (277, 278), (0, 0)]\n",
      "tensor([-100,    1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100])\n",
      "[(0, 0), (0, 2), (2, 6), (7, 10), (10, 14), (15, 17), (18, 20), (21, 24), (24, 27), (27, 28), (29, 34), (35, 37), (38, 43), (44, 45), (46, 49), (49, 53), (54, 55), (56, 60), (61, 65), (65, 67), (67, 68), (69, 75), (76, 78), (78, 81), (81, 86), (87, 88), (89, 91), (92, 97), (97, 99), (100, 101), (102, 106), (107, 112), (113, 114), (115, 120), (120, 121), (122, 125), (125, 129), (130, 132), (133, 135), (135, 137), (137, 139), (140, 142), (143, 148), (149, 150), (151, 155), (155, 158), (159, 162), (163, 167), (168, 175), (175, 179), (180, 185), (186, 192), (193, 199), (200, 205), (206, 210), (210, 212), (212, 213), (214, 218), (218, 219), (220, 225), (225, 227), (228, 231), (232, 234), (235, 239), (239, 241), (242, 246), (247, 253), (253, 256), (256, 257), (258, 262), (262, 265), (265, 268), (269, 277), (277, 278), (0, 0)]\n",
      "[(0, 0), (0, 2), (2, 6), (7, 10), (10, 14), (15, 17), (18, 20), (21, 24), (24, 27), (27, 28), (29, 34), (35, 37), (38, 43), (44, 45), (46, 49), (49, 53), (54, 55), (56, 60), (61, 65), (65, 67), (67, 68), (69, 75), (76, 78), (78, 81), (81, 86), (87, 88), (89, 91), (92, 97), (97, 99), (100, 101), (102, 106), (107, 112), (113, 114), (115, 120), (120, 121), (122, 125), (125, 129), (130, 132), (133, 135), (135, 137), (137, 139), (140, 142), (143, 148), (149, 150), (151, 155), (155, 158), (159, 162), (163, 167), (168, 175), (175, 179), (180, 185), (186, 192), (193, 199), (200, 205), (206, 210), (210, 212), (212, 213), (214, 218), (218, 219), (220, 225), (225, 227), (228, 231), (232, 234), (235, 239), (239, 241), (242, 246), (247, 253), (253, 256), (256, 257), (258, 262), (262, 265), (265, 268), (269, 277), (277, 278), (0, 0)]\n",
      "[CLS]       -100\n",
      "Du          1\n",
      "##nque      -100\n",
      "ved         -100\n",
      "##iamo      -100\n",
      "se          -100\n",
      "ho          -100\n",
      "cap         -100\n",
      "##ito       -100\n",
      ":           -100\n",
      "Conte       -100\n",
      "le          -100\n",
      "canta       -100\n",
      "a           -100\n",
      "Sal         -100\n",
      "##vini      -100\n",
      "e           -100\n",
      "alla        -100\n",
      "Melo        -100\n",
      "##ni        -100\n",
      ",           -100\n",
      "questi      -100\n",
      "re          -100\n",
      "##agi       -100\n",
      "##scono     -100\n",
      "e           -100\n",
      "le          -100\n",
      "canta       -100\n",
      "##no        -100\n",
      "a           -100\n",
      "loro        -100\n",
      "volta       -100\n",
      "a           -100\n",
      "Conte       -100\n",
      ",           -100\n",
      "Men         -100\n",
      "##tana      -100\n",
      "si          -100\n",
      "sm          -100\n",
      "##ar        -100\n",
      "##ca        -100\n",
      "da          -100\n",
      "Conte       -100\n",
      "e           -100\n",
      "fini        -100\n",
      "##sce       -100\n",
      "con         -100\n",
      "foto        -100\n",
      "celebra     -100\n",
      "##tiva      -100\n",
      "nelle       -100\n",
      "pagine      -100\n",
      "social      -100\n",
      "della       -100\n",
      "Best        -100\n",
      "##ia        -100\n",
      ".           -100\n",
      "Siam        -100\n",
      "##o         -100\n",
      "torna       -100\n",
      "##ti        -100\n",
      "per         -100\n",
      "un          -100\n",
      "atti        -100\n",
      "##mo        -100\n",
      "alla        -100\n",
      "normal      -100\n",
      "##ità       -100\n",
      ":           -100\n",
      "mera        -100\n",
      "##vig       -100\n",
      "##lia       -100\n",
      "assoluta    -100\n",
      "!           -100\n",
      "[SEP]       -100\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0][\"labels\"])\n",
    "for token, label in zip(train_dataset.tokenizer.convert_ids_to_tokens(train_dataset[0][\"input_ids\"]), train_dataset[0][\"labels\"]):\n",
    "  print('{0:10}  {1}'.format(token, label))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "531bf1ddd0b9ee64f0e2ebd6527c520de4e2159f2ffcc26ba5e4b06431092b93"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
