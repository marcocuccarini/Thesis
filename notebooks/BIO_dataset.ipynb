{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.is_available()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('../data/Union/Hyperion.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "#Find bounds starting froma text\n",
    "def find_char_bounds(spans: list, text: str) -> list:\n",
    "    '''\n",
    "    Given a list of spans and a text, find the start and end indices of each span in the text.\n",
    "    Indeces are computed counting CHARS\n",
    "    \n",
    "    :param spans: a list of strings to search for\n",
    "    :type spans: list\n",
    "    :param text: the text to search\n",
    "    :type text: str\n",
    "    :return: A list of tuples, where each tuple contains the start and end index of a span.\n",
    "    '''\n",
    "    start = 0\n",
    "    bounds = []\n",
    "    last_char = -1\n",
    "    for span in spans:\n",
    "        start = text.find(span)\n",
    "        if start == -1:\n",
    "            start = last_char + 1\n",
    "        last_char = start + len(span)\n",
    "        bounds.append((start, last_char))\n",
    "        \n",
    "    return bounds\n",
    "\n",
    "\n",
    "def find_word_bounds(spans: list, text: str) -> list:\n",
    "    '''\n",
    "    Given a list of spans and a text, find the start and end indices of each span in the text.\n",
    "    Indeces are computed counting WORDS.\n",
    "\n",
    "    :param spans: a list of strings, each string is a span of text\n",
    "    :type spans: list\n",
    "    :param text: the text to be searched\n",
    "    :type text: str\n",
    "    :return: A list of tuples, where each tuple is the start and end index of a word in the text.\n",
    "    '''\n",
    "    bounds = []\n",
    "    end = 0\n",
    "    for span in spans:\n",
    "        s = span.translate(str.maketrans('', '', string.punctuation))\n",
    "        word_list = s.split()\n",
    "        if word_list:   \n",
    "            text_list = text.translate(str.maketrans('', '', string.punctuation)).split()\n",
    "            try:\n",
    "                start = text_list.index(word_list[0], end)\n",
    "            except:\n",
    "                if not bounds:\n",
    "                    start = 0\n",
    "                else:\n",
    "\n",
    "                    start = bounds[-1][1] + 1\n",
    "            end = start + len(word_list) - 1\n",
    "\n",
    "            bounds.append((start, end))\n",
    "    return bounds\n",
    "\n",
    "def find_segmentation(bounds, text):\n",
    "    text_list = text.translate(str.maketrans('', '', string.punctuation)).split()\n",
    "    segmentation = ['0' for i in range(len(text_list))]\n",
    "    segmentation[-1] = '1'\n",
    "    \n",
    "    ends = []\n",
    "    end = 0\n",
    "    for span in spans:\n",
    "        word_list = span.translate(str.maketrans('', '', string.punctuation)).split()\n",
    "        try:\n",
    "            end = text_list.index(word_list[-1], end)\n",
    "        except:\n",
    "                end = end + len(word_list) -1\n",
    "        if end < len(text_list):\n",
    "            ends.append(end)\n",
    "    for i in ends:\n",
    "        segmentation[i] = '1'\n",
    "    \n",
    "    return ''.join(segmentation)\n",
    "\n",
    "def find_segmentation_by_bounds(bounds: list) -> str:\n",
    "    segmentation = ['0' for i in range(bounds[-1][1] + 1)]\n",
    "    for bound in bounds:\n",
    "        if bound[1] < len(segmentation):\n",
    "            segmentation[bound[1]] = '1'\n",
    "    segmentation[-1] = '1'\n",
    "    return ''.join(segmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import re\n",
    "\n",
    "def clean_text(text:str) -> str:\n",
    "    #delete \\n\n",
    "    text = text.replace('\\n', ' ')\n",
    "    #delete double punctuation\n",
    "    text =  re.sub(r'[\\?\\.\\!]+(?=[\\?\\.\\!])', '', text)\n",
    "    # add space between a word and punctuation\n",
    "    text = re.sub('(?<! )(?=[.,!?()])|(?<=[.,!?()])(?! )', r' ', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "dataset = []\n",
    "\n",
    "for row in df.itertuples():\n",
    "    text = row.Testo\n",
    "    \n",
    "    if pd.isna(text):\n",
    "        sample['Stralci'].append(clean_text(row.Stralcio))\n",
    "        sample['Repertori'].append(row.Repertorio)\n",
    "\n",
    "    else:\n",
    "        sample = {}\n",
    "        sample['Testo'] = clean_text(text)\n",
    "        sample['Stralci'] = [clean_text(row.Stralcio)]\n",
    "\n",
    "        sample['Repertori'] = [row.Repertorio]\n",
    "        dataset.append(sample)\n",
    "\n",
    "\n",
    "for sample in dataset:\n",
    "    sample['Char_Bounds'] = find_char_bounds(sample['Stralci'], sample['Testo'])\n",
    "    sample['Bounds'] = find_word_bounds(sample['Stralci'], sample['Testo'])\n",
    "    sample['Char_Segmentation'] = find_segmentation_by_bounds(sample['Char_Bounds'])\n",
    "    sample['Segmentation'] = find_segmentation_by_bounds(sample['Bounds'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "IE_dict = {\n",
    "    'Testo': [sample['Testo'] for sample in dataset],\n",
    "    'Char_Segmentation': [sample['Char_Segmentation'] for sample in dataset],\n",
    "    'Segmentation': [sample['Segmentation'] for sample in dataset],\n",
    "    'Bounds': [sample['Bounds'] for sample in dataset],\n",
    "    'Char_Bounds': [sample['Char_Bounds'] for sample in dataset],\n",
    "    'Repertori': [sample['Repertori'] for sample in dataset],\n",
    "    'Stralci': [sample['Stralci'] for sample in dataset]\n",
    "}\n",
    "IE_df = pd.DataFrame(IE_dict)\n",
    "IE_df = IE_df.head(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "class IE_Hyperion_dataset(Dataset):\n",
    "    def __init__(self, df, tokenizer_name):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "        self.df = df\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.df['Testo'].iloc[idx]\n",
    "        encoding = self.tokenizer(text,\n",
    "                                  # is_pretokenized=True,\n",
    "                                  return_special_tokens_mask=True,\n",
    "                                  return_offsets_mapping=True,\n",
    "                                  add_special_tokens=True,\n",
    "                                  return_attention_mask=True,\n",
    "                                  padding='max_length',\n",
    "                                  truncation=True,\n",
    "                                  )\n",
    "        char_labels = list(self.df['Char_Segmentation'].iloc[idx])\n",
    "        ends = [i for i in range(len(char_labels)) if char_labels[i] == '1']\n",
    "\n",
    "        last_token_idx = max(index for index, item in enumerate(encoding['special_tokens_mask']) if item == 0)\n",
    "\n",
    "        encoded_labels = np.ones(len(encoding['input_ids']), dtype=int) * -100\n",
    "        x = ends.pop(0)\n",
    "        for i,e in enumerate(encoding['offset_mapping']):\n",
    "            if e[1] != 0:\n",
    "                # overwrite label\n",
    "                if x >= e[0] and x <= e[1]:# Doubt if insert < e[1] because of offset mapping composition\n",
    "                    encoded_labels[i] = 1\n",
    "                    if ends: \n",
    "                        x = ends.pop(0)\n",
    "                    else:\n",
    "                        x = -1\n",
    "                else:\n",
    "                    encoded_labels[i] = 0\n",
    "\n",
    "        encoded_labels[last_token_idx] = 1\n",
    "\n",
    "\n",
    "        item = {key: torch.as_tensor(val) for key, val in encoding.items()}\n",
    "        item['labels'] = torch.as_tensor(encoded_labels)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL Dataset: (1000, 7)\n",
      "TRAIN Dataset: (640, 7)\n",
      "VALIDATION Dataset: (160, 7)\n",
      "TEST Dataset: (200, 7)\n"
     ]
    }
   ],
   "source": [
    "model_name = \"dbmdz/bert-base-italian-xxl-uncased\"\n",
    "train_size = 0.8\n",
    "train_df = IE_df.sample(frac=train_size, random_state=200)\n",
    "test_df = IE_df.drop(train_df.index).reset_index(drop=True)\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "val_size = 0.2\n",
    "val_df = train_df.sample(frac=val_size, random_state=200)\n",
    "train_df = train_df.drop(val_df.index).reset_index(drop=True)\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "\n",
    "print(\"FULL Dataset: {}\".format(IE_df.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_df.shape))\n",
    "print(\"VALIDATION Dataset: {}\".format(val_df.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_df.shape))\n",
    "\n",
    "train_dataset = IE_Hyperion_dataset(train_df, model_name)\n",
    "val_dataset = IE_Hyperion_dataset(val_df, model_name)\n",
    "test_dataset = IE_Hyperion_dataset(test_df, model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]       -100\n",
      "capisco     0\n",
      "la          0\n",
      "situazione  0\n",
      ".           0\n",
      "ma          0\n",
      "almeno      0\n",
      "dare        0\n",
      "la          0\n",
      "possibilita  0\n",
      "ai          0\n",
      "clienti     0\n",
      "di          0\n",
      "venire      0\n",
      "a           0\n",
      "prendere    0\n",
      "per         0\n",
      "asp         0\n",
      "##orto      0\n",
      "nei         0\n",
      "ristoranti  0\n",
      "con         0\n",
      "dovute      0\n",
      "precauzioni  0\n",
      "(           0\n",
      "distanze    0\n",
      ",           0\n",
      "masch       0\n",
      "##eri       0\n",
      "##ne        0\n",
      ",           0\n",
      "guanti      0\n",
      ")           0\n",
      "non         0\n",
      "tutte       0\n",
      "le          0\n",
      "attivita    0\n",
      "di          0\n",
      "ristorazione  0\n",
      "possono     0\n",
      "agi         0\n",
      "##lmente    0\n",
      "adegua      0\n",
      "##rsi       0\n",
      "alle        0\n",
      "consegne    0\n",
      "a           0\n",
      "domicilio   0\n",
      ".           0\n",
      "o           0\n",
      "che         0\n",
      "facciamo    0\n",
      "assum       0\n",
      "##iamo      0\n",
      "il          0\n",
      "fattori     0\n",
      "##no        0\n",
      "?           0\n",
      "gia         0\n",
      "non         0\n",
      "inca        0\n",
      "##ssia      0\n",
      "##mo        0\n",
      "per         0\n",
      "pagare      0\n",
      "le          0\n",
      "spese       0\n",
      ".           0\n",
      "se          0\n",
      "si          0\n",
      "puo         0\n",
      "andare      0\n",
      "a           0\n",
      "comprare    0\n",
      "in          0\n",
      "libreria    0\n",
      ",           0\n",
      "carto       0\n",
      "##ler       0\n",
      "##ia        0\n",
      "o           0\n",
      "altro       0\n",
      ",           0\n",
      "si          0\n",
      "puo         0\n",
      "ritirare    0\n",
      "il          0\n",
      "proprio     0\n",
      "pasto       0\n",
      "'           0\n",
      "take        0\n",
      "aw          0\n",
      "##ay        0\n",
      "\"           0\n",
      "e           0\n",
      "non         0\n",
      "costringere  0\n",
      "i           0\n",
      "poveri      0\n",
      "risto       0\n",
      "##ratori    0\n",
      "di          0\n",
      "po          0\n",
      "##ccole     0\n",
      "realta      0\n",
      "a           0\n",
      "correre     0\n",
      "in          0\n",
      "lungo       0\n",
      "e           0\n",
      "in          0\n",
      "largo       0\n",
      "per         0\n",
      "portare     0\n",
      "non         0\n",
      "si          0\n",
      "sa          0\n",
      "nemmeno     0\n",
      "se          0\n",
      "la          0\n",
      "quantita    0\n",
      "sufficiente  0\n",
      "a           0\n",
      "coprire     0\n",
      "la          0\n",
      "giornata    0\n",
      ".           0\n",
      "bisogna     0\n",
      "pensare     0\n",
      "a           0\n",
      "tutti       0\n",
      ".           0\n",
      "non         0\n",
      "solo        0\n",
      "alcuni      0\n",
      ".           0\n",
      "sono        0\n",
      "37          0\n",
      "anni        0\n",
      "che         0\n",
      "lavoriamo   0\n",
      "onestamente  0\n",
      "e           0\n",
      "paghi       0\n",
      "##amo       0\n",
      "le          0\n",
      "tasse       0\n",
      ".           0\n",
      "almeno      0\n",
      "render      0\n",
      "##ci        0\n",
      "possibile   0\n",
      "provare     0\n",
      "a           0\n",
      "lavorare    0\n",
      ",           0\n",
      "senza       0\n",
      "rischi      0\n",
      "per         0\n",
      "nessuno     0\n",
      ",           0\n",
      "ma          0\n",
      "sperando    0\n",
      "di          0\n",
      "farcela     0\n",
      ".           1\n",
      "[SEP]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n"
     ]
    }
   ],
   "source": [
    "for token, label in zip(test_dataset.tokenizer.convert_ids_to_tokens(test_dataset[10][\"input_ids\"]), test_dataset[10][\"labels\"]):\n",
    "  print('{0:10}  {1}'.format(token, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-base-italian-xxl-uncased were not used when initializing BertForTokenClassification: ['cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at dbmdz/bert-base-italian-xxl-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_name, num_labels=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "# Deterministic mode\n",
    "def seed_everything(seed=1464):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "\n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n",
    "\n",
    "def plot_loss(loss, val_loss):\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    plt.xticks(range(1, len(loss)+1))\n",
    "    plt.plot(range(1, len(loss)+1), loss, label='train')\n",
    "    plt.plot(range(1, len(val_loss)+1), val_loss, label='val')\n",
    "    plt.title('loss')\n",
    "    plt.legend()\n",
    "    # plt.savefig('loss.png')\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_f1(f1, val_f1):\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    plt.xticks(range(1, len(f1)+1))\n",
    "    plt.plot(range(1, len(f1)+1), f1, label='train')\n",
    "    plt.plot(range(1, len(val_f1)+1), val_f1, label='val')\n",
    "    plt.title('f1')\n",
    "    plt.legend()\n",
    "    # plt.savefig('f1.png')\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(y_true, pred, labels):\n",
    "    fig, ax = plt.subplots(figsize=(20, 20))\n",
    "    disp = ConfusionMatrixDisplay.from_predictions(\n",
    "        y_true, pred, display_labels=labels, normalize='true', values_format='.2f')\n",
    "    disp.plot(cmap=\"Blues\", values_format='.2g',\n",
    "              xticks_rotation='vertical', ax=ax)\n",
    "    return disp.figure_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neptune.new as neptune\n",
    "\n",
    "\n",
    "class NeptuneLogger():\n",
    "    def __init__(self) -> None:\n",
    "        # Neptune initialization\n",
    "        self.run = neptune.init(\n",
    "            project=\"mibo8/Rep\",\n",
    "            api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJmZmRkYThiZi1mZGNlLTRlMTktODQwNS1hNWFlMWQ2Mjc4N2IifQ==\",\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "from torch.nn import utils\n",
    "\n",
    "import torchmetrics\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW\n",
    "from transformers import get_constant_schedule_with_warmup\n",
    "\n",
    "\n",
    "import neptune.new as neptune\n",
    "\n",
    "\n",
    "class IE_MPTrainer():\n",
    "    def __init__(self, batch_size, lr, n_epochs) -> None:\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = lr\n",
    "        self.n_epochs = n_epochs\n",
    "\n",
    "        self.logger = NeptuneLogger()\n",
    "\n",
    "    def fit(self, model, train_dataset, val_dataset):\n",
    "        self.logger.run['model'] = model_name\n",
    "\n",
    "        params_info = {\n",
    "            'learning_rate': self.learning_rate,\n",
    "            'batch_size': self.batch_size,\n",
    "            'n_epochs': self.n_epochs\n",
    "        }\n",
    "        #self.logger.run['params'] = params_info\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        # ----------TRAINING\n",
    "\n",
    "        # Measure the total training time for the whole run.\n",
    "        total_t0 = time.time()\n",
    "\n",
    "        epochs_train_loss = []\n",
    "        epochs_val_loss = []\n",
    "\n",
    "        epochs = self.n_epochs\n",
    "\n",
    "        # Creation of Pytorch DataLoaders with shuffle=True for the traing phase\n",
    "        train_dataloader = DataLoader(\n",
    "            train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        validation_dataloader = DataLoader(\n",
    "            val_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "        # Adam algorithm optimized for tranfor architectures\n",
    "        optimizer = AdamW(model.parameters(), lr=self.learning_rate)\n",
    "        #scheduler = get_constant_schedule_with_warmup(optimizer, num_warmup_steps=300)\n",
    "\n",
    "        # Scaler for mixed precision\n",
    "        scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "        # Setup for training with gpu\n",
    "        device = torch.device(\n",
    "            'cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "        model.to(device)\n",
    "\n",
    "        # For each epoch...\n",
    "        for epoch_i in range(0, epochs):\n",
    "\n",
    "            # ========================================\n",
    "            #               Training\n",
    "            # ========================================\n",
    "\n",
    "            # Perform one full pass over the training set.\n",
    "\n",
    "            print(\"\")\n",
    "            print(\n",
    "                '======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "            print('Training...')\n",
    "\n",
    "            # Measure how long the training epoch takes.\n",
    "            t0 = time.time()\n",
    "\n",
    "            # Reset the total loss for this epoch.\n",
    "            total_train_loss = 0\n",
    "\n",
    "            # Put the model into training mode: Dropout layers are active\n",
    "            model.train()\n",
    "\n",
    "            # For each batch of training data...\n",
    "            for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "                # Progress update every 40 batches.\n",
    "                if step % 10 == 0 and not step == 0:\n",
    "                    # Compute time in minutes.\n",
    "                    elapsed = format_time(time.time() - t0)\n",
    "\n",
    "                    # Report progress.\n",
    "                    print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(\n",
    "                        step, len(train_dataloader), elapsed))\n",
    "\n",
    "                # Unpack this training batch from the dataloader.\n",
    "                #\n",
    "                #  copy each tensor to the GPU using the 'to()' method\n",
    "                #\n",
    "                # 'batch' contains three pytorch tensors:\n",
    "                #   [0]: input ids\n",
    "                #   [1]: attention masks\n",
    "                #   [2]: labels\n",
    "                b_input_ids = batch['input_ids'].to(device)\n",
    "                b_input_mask = batch['attention_mask'].to(device)\n",
    "                b_labels = batch['labels'].to(device)\n",
    "\n",
    "                # clear any previously calculated gradients before performing a\n",
    "                # backward pass\n",
    "                model.zero_grad()\n",
    "\n",
    "                # Perform a forward pass in mixed precision\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    outputs = model(b_input_ids,\n",
    "                                    attention_mask=b_input_mask,\n",
    "                                    labels=b_labels)\n",
    "\n",
    "                    loss = outputs[0]\n",
    "                    logits = outputs[1]\n",
    "\n",
    "                # Move logits and labels to CPU\n",
    "                logits = logits.detach().cpu()\n",
    "                label_ids = b_labels.to('cpu')\n",
    "\n",
    "                # Perform a backward pass to compute the gradients in MIXED precision\n",
    "                scaler.scale(loss).backward()\n",
    "\n",
    "                # Accumulate the training loss over all of the batches so that we can\n",
    "                # calculate the average loss at the end.\n",
    "                total_train_loss += loss.item()\n",
    "\n",
    "                # Unscales the gradients of optimizer's assigned params in-place before the gradient clipping\n",
    "                scaler.unscale_(optimizer)\n",
    "\n",
    "                # Clip the norm of the gradients to 1.0.\n",
    "                # This helps and prevent the \"exploding gradients\" problem.\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "                # Update parameters and take a step using the computed gradient in MIXED precision\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                # scheduler.step()\n",
    "\n",
    "            # Compute the average loss over all of the batches.\n",
    "            avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "            epochs_train_loss.append(avg_train_loss)\n",
    "\n",
    "            # Measure how long this epoch took.\n",
    "            training_time = format_time(time.time() - t0)\n",
    "\n",
    "            print(\"\")\n",
    "            print(\"  Average training loss: {0:.3f}\".format(avg_train_loss))\n",
    "            print(\"  Training epoch took: {:}\".format(training_time))\n",
    "\n",
    "            # ========================================\n",
    "            #               Validation\n",
    "            # ========================================\n",
    "            # After the completion of each training epoch, measure performance on\n",
    "            # the validation set.\n",
    "\n",
    "            print(\"\")\n",
    "            print(\"Running Validation...\")\n",
    "\n",
    "            t0 = time.time()\n",
    "\n",
    "            # Put the model in evaluation mode: the dropout layers behave differently\n",
    "            model.eval()\n",
    "\n",
    "            total_val_loss = 0\n",
    "\n",
    "            # Evaluate data for one epoch\n",
    "            for batch in validation_dataloader:\n",
    "\n",
    "                # Unpack this training batch from our dataloader.\n",
    "                #\n",
    "                # copy each tensor to the GPU using the 'to()' method\n",
    "                #\n",
    "                # 'batch' contains three pytorch tensors:\n",
    "                #   [0]: input ids\n",
    "                #   [1]: attention masks\n",
    "                #   [2]: labels\n",
    "                b_input_ids = batch['input_ids'].to(device)\n",
    "                b_input_mask = batch['attention_mask'].to(device)\n",
    "                b_labels = batch['labels'].to(device)\n",
    "\n",
    "                # Tell pytorch not to bother with constructing the compute graph during\n",
    "                # the forward pass, since this is only needed for training.\n",
    "                with torch.no_grad():\n",
    "\n",
    "                    # Forward pass, calculate logits\n",
    "                    # argmax(logits) = argmax(Softmax(logits))\n",
    "                    outputs = model(b_input_ids,\n",
    "                                    attention_mask=b_input_mask,\n",
    "                                    labels=b_labels)\n",
    "                    loss = outputs[0]\n",
    "                    logits = outputs[1]\n",
    "\n",
    "                # Accumulate the validation loss.\n",
    "                total_val_loss += loss.item()\n",
    "\n",
    "                # Move logits and labels to CPU\n",
    "                logits = logits.detach().cpu()\n",
    "                label_ids = b_labels.to('cpu')\n",
    "\n",
    "            print('VALIDATION: ')\n",
    "\n",
    "            # Compute the average loss over all of the batches.\n",
    "            avg_val_loss = total_val_loss / len(validation_dataloader)\n",
    "            epochs_val_loss.append(avg_val_loss)\n",
    "\n",
    "            # Measure how long the validation run took.\n",
    "            validation_time = format_time(time.time() - t0)\n",
    "\n",
    "            print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "            print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "        loss_fig = plot_loss(epochs_train_loss, epochs_val_loss)\n",
    "\n",
    "        self.logger.run[\"loss\"].upload(neptune.types.File.as_image(loss_fig))\n",
    "        print(\"\")\n",
    "        print(\"Training complete!\")\n",
    "\n",
    "        print(\"Total training took {:} (h:mm:ss)\".format(\n",
    "            format_time(time.time()-total_t0)))\n",
    "\n",
    "    def test(self, model, test_dataset):\n",
    "        # ========================================\n",
    "        #               Test\n",
    "        # ========================================\n",
    "        test_dataloader = DataLoader(\n",
    "            test_dataset, batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "        # Setup for testing with gpu\n",
    "        device = torch.device(\n",
    "            'cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "        print(\"\")\n",
    "        print(\"Running Test...\")\n",
    "        t0 = time.time()\n",
    "\n",
    "        # Save prediction for confusion matrix\n",
    "        preds = []\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        total_test_loss = 0\n",
    "\n",
    "        # Evaluate data for one epoch\n",
    "        for batch in test_dataloader:\n",
    "            b_input_ids = batch['input_ids'].to(device)\n",
    "            b_input_mask = batch['attention_mask'].to(device)\n",
    "            b_labels = batch['labels'].to(device)\n",
    "            b_special_tokens_mask = batch['special_tokens_mask'].to(device)\n",
    "            with torch.no_grad():\n",
    "\n",
    "                # Forward pass, calculate logits\n",
    "                # argmax(logits) = argmax(Softmax(logits))\n",
    "                outputs = model(b_input_ids,\n",
    "                                attention_mask=b_input_mask,\n",
    "                                labels=b_labels)\n",
    "                loss = outputs[0]\n",
    "                logits = outputs[1]\n",
    "\n",
    "            # Accumulate the test loss.\n",
    "            total_test_loss += loss.item()\n",
    "\n",
    "            # Move logits and labels to CPU\n",
    "            logits = logits.detach().cpu()  # shape (batch_size, seq_len, num_labels\n",
    "            full_probs = logits.softmax(dim=-1)\n",
    "\n",
    "            for i, sample_prob in enumerate(full_probs):\n",
    "                active_prob = []\n",
    "                for j, e in enumerate(b_special_tokens_mask[i]):\n",
    "                    if(e == 0):\n",
    "                        active_prob.append(sample_prob[j].tolist())\n",
    "                preds.append(active_prob)\n",
    "\n",
    "        avg_test_loss = total_test_loss / len(test_dataloader)\n",
    "        #self.logger.run['test/loss'] = avg_test_loss\n",
    "        test_time = format_time(time.time() - t0)\n",
    "\n",
    "        print(\"  Test Loss: {0:.2f}\".format(avg_test_loss))\n",
    "        print(\"  Test took: {:}\".format(test_time))\n",
    "\n",
    "        return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "https://app.neptune.ai/mibo8/Rep/e/REP-292\n",
      "Remember to stop your run once you’ve finished logging your metadata (https://docs.neptune.ai/api-reference/run#.stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 1e-5\n",
    "batch_size = 2\n",
    "n_epochs = 1\n",
    "trainer = IE_MPTrainer(batch_size, learning_rate, n_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 1 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch    10  of    320.    Elapsed: 0:00:01.\n",
      "  Batch    20  of    320.    Elapsed: 0:00:03.\n",
      "  Batch    30  of    320.    Elapsed: 0:00:04.\n",
      "  Batch    40  of    320.    Elapsed: 0:00:05.\n",
      "  Batch    50  of    320.    Elapsed: 0:00:06.\n",
      "  Batch    60  of    320.    Elapsed: 0:00:08.\n",
      "  Batch    70  of    320.    Elapsed: 0:00:09.\n",
      "  Batch    80  of    320.    Elapsed: 0:00:10.\n",
      "  Batch    90  of    320.    Elapsed: 0:00:12.\n",
      "  Batch   100  of    320.    Elapsed: 0:00:13.\n",
      "  Batch   110  of    320.    Elapsed: 0:00:14.\n",
      "  Batch   120  of    320.    Elapsed: 0:00:15.\n",
      "  Batch   130  of    320.    Elapsed: 0:00:17.\n",
      "  Batch   140  of    320.    Elapsed: 0:00:18.\n",
      "  Batch   150  of    320.    Elapsed: 0:00:19.\n",
      "  Batch   160  of    320.    Elapsed: 0:00:20.\n",
      "  Batch   170  of    320.    Elapsed: 0:00:22.\n",
      "  Batch   180  of    320.    Elapsed: 0:00:23.\n",
      "  Batch   190  of    320.    Elapsed: 0:00:24.\n",
      "  Batch   200  of    320.    Elapsed: 0:00:26.\n",
      "  Batch   210  of    320.    Elapsed: 0:00:27.\n",
      "  Batch   220  of    320.    Elapsed: 0:00:28.\n",
      "  Batch   230  of    320.    Elapsed: 0:00:29.\n",
      "  Batch   240  of    320.    Elapsed: 0:00:31.\n",
      "  Batch   250  of    320.    Elapsed: 0:00:32.\n",
      "  Batch   260  of    320.    Elapsed: 0:00:33.\n",
      "  Batch   270  of    320.    Elapsed: 0:00:34.\n",
      "  Batch   280  of    320.    Elapsed: 0:00:36.\n",
      "  Batch   290  of    320.    Elapsed: 0:00:37.\n",
      "  Batch   300  of    320.    Elapsed: 0:00:38.\n",
      "  Batch   310  of    320.    Elapsed: 0:00:40.\n",
      "\n",
      "  Average training loss: 0.041\n",
      "  Training epoch took: 0:00:41\n",
      "\n",
      "Running Validation...\n",
      "VALIDATION: \n",
      "  Validation Loss: 0.04\n",
      "  Validation took: 0:00:05\n",
      "\n",
      "Training complete!\n",
      "Total training took 0:00:46 (h:mm:ss)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(model, train_dataset, val_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running Test...\n",
      "  Test Loss: 0.04\n",
      "  Test took: 0:00:09\n"
     ]
    }
   ],
   "source": [
    "probs = trainer.test(model, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_segmentation(probs, threshold):  #one sample\n",
    "    if threshold < 0 or threshold > 1:\n",
    "        return None\n",
    "    segmentation = []\n",
    "    for prob in probs:\n",
    "        if prob[1] >= threshold:\n",
    "            segmentation.append(1)\n",
    "        else:\n",
    "            segmentation.append(0)\n",
    "    segmentation[-1] = 1\n",
    "    return segmentation\n",
    "\n",
    "def split_by_prediction(pred:list, input:dict, text:str, tokenizer) -> list:\n",
    "    offset_mapping = input['offset_mapping'].tolist()\n",
    "    i=0\n",
    "    subword_flags = []\n",
    "    while i < len(offset_mapping):\n",
    "        if offset_mapping[i][1] != 0:\n",
    "            if tokenizer.decode(input['input_ids'][i])[:2] == '##':\n",
    "                subword_flags.append(True)\n",
    "            else:\n",
    "                subword_flags.append(False)\n",
    "        i+=1\n",
    "        \n",
    "    for i in range(len(pred)-1):\n",
    "        if pred[i] == 1:\n",
    "            if subword_flags[i + 1]:\n",
    "                pred[i] = 0\n",
    "                pred[i + 1] =1\n",
    "        \n",
    "            \n",
    "    spans = []\n",
    "    start = 0\n",
    "    i=0\n",
    "    while i < len(offset_mapping):\n",
    "        if offset_mapping[i][1] != 0:\n",
    "            x = pred.pop(0)\n",
    "            if x == 1:\n",
    "                spans.append(text[start:offset_mapping[i][1]])\n",
    "                start = offset_mapping[i][1]\n",
    "        i+=1\n",
    "\n",
    "    return spans\n",
    "\n",
    "    #another way to proceed: takes text decoding token_ids\n",
    "    \"\"\"\n",
    "    start = 0\n",
    "    end = 0\n",
    "    spans = []\n",
    "    for i,e in enumerate(pred):\n",
    "        if e == 1:\n",
    "            end = i\n",
    "            span = tokenizer.decode(input['input_ids'][start:end + 1], skip_special_tokens= True, clean_up_tokenization_spaces= False)\n",
    "            spans.append(span)\n",
    "            start = end + 1\n",
    "            end = end + 1\n",
    "    if not spans:\n",
    "        spans.append(tokenizer.decode(input['input_ids'], skip_special_tokens= True, clean_up_tokenization_spaces= False))\n",
    "    return spans\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "preds = [decode_segmentation(e, 0.1) for e in probs]\n",
    "\n",
    "bert_preds = []\n",
    "bert_spans = []\n",
    "for i,e in enumerate(preds):\n",
    "    spans = split_by_prediction(e, test_dataset[i], test_dataset.df.iloc[i]['Testo'], test_dataset.tokenizer)\n",
    "    bert_preds.append(find_word_bounds(spans, test_dataset.df.iloc[i]['Testo']))\n",
    "    bert_spans.append(spans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# A è B sono tupe con i bound dello span\n",
    "def IoU(A, B):\n",
    "    if A == B:\n",
    "        return 1\n",
    "    start = max(A[0], B[0])\n",
    "    end = min(A[1], B[1])\n",
    "    if(start > end):\n",
    "        return 0\n",
    "    intersection = end - start\n",
    "    return intersection / (A[1] - A[0] + B[1] - B[0] - intersection)\n",
    "\n",
    "def compute_IoUs(pred_bounds, gt_spans):\n",
    "    \"\"\"\n",
    "    Given a list of predicted spans and a list of ground truth spans, \n",
    "    compute the IoU between each pair of spans\n",
    "    \n",
    "    :param pred_bounds: a tuple of (start, end) denoting the predicted answer\n",
    "    :param gt_spans: a list of tuples of the form (start, end) representing the spans of each ground\n",
    "    truth annotation\n",
    "    :return: a list of IoUs for each ground truth span.\n",
    "    \"\"\"\n",
    "    IoUs = []\n",
    "    for gt_bounds in gt_spans:\n",
    "        IoUs.append(IoU(pred_bounds, gt_bounds)) \n",
    "    return IoUs\n",
    "\n",
    "\n",
    "def intersection(A, B):\n",
    "    if A == B:\n",
    "        return 1\n",
    "    start = max(A[0], B[0])\n",
    "    end = min(A[1], B[1])\n",
    "    if(start > end):\n",
    "        return 0\n",
    "    return end - start\n",
    "\n",
    "def normalize_bounds_by_repertoire(bounds, sample):\n",
    "    bounds_w_rep = []\n",
    "    for bound in bounds:\n",
    "        intersections = []\n",
    "        for gt_bound in sample['Bounds']:\n",
    "            intersections.append(intersection(bound, gt_bound))\n",
    "        rep_idx = np.argmax(intersections)\n",
    "        bounds_w_rep.append({\n",
    "            'Bounds': bound,\n",
    "            'Repertorio': sample['Repertori'][rep_idx]\n",
    "            })\n",
    "    normalized = []\n",
    "    for i in range(len(bounds_w_rep)):\n",
    "        #normalized is not empty\n",
    "        if normalized:\n",
    "            if normalized[-1]['Repertorio'] == bounds_w_rep[i]['Repertorio']:\n",
    "                new_span = (normalized[-1]['Bounds'][0], bounds_w_rep[i]['Bounds'][1])\n",
    "                new_span_features = {\n",
    "                    'Bounds' : new_span, \n",
    "                    'Repertorio' : bounds_w_rep[i]['Repertorio']\n",
    "                    }\n",
    "                del normalized[-1]\n",
    "                normalized.append(new_span_features)\n",
    "            else:\n",
    "                normalized.append(bounds_w_rep[i])\n",
    "        else:\n",
    "            normalized.append(bounds_w_rep[i])\n",
    "    return [e['Bounds'] for e in normalized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.metrics.segmentation import windowdiff, ghd, pk\n",
    "\n",
    "met_list = []\n",
    "counter=0\n",
    "\n",
    "for i in range(len(test_dataset.df.index)):\n",
    "    if len(test_dataset.df['Segmentation'].iloc[i]) >= 20:\n",
    "        \n",
    "        \n",
    "        counter += 1\n",
    "        \n",
    "        seg_pred = find_segmentation_by_bounds(bert_preds[i])\n",
    "        seg_pred = seg_pred[:len(test_dataset.df['Segmentation'].iloc[i])]\n",
    "        \n",
    "        seg_gt_trunk = test_dataset.df['Segmentation'].iloc[i][:len(seg_pred)] # manages predictiones in text with n_tokens  > 512\n",
    "        \"\"\"\n",
    "        print(len(test_dataset.df['Segmentation'].iloc[i]))\n",
    "        print(len(seg_pred))\n",
    "        print(test_dataset.df['Testo'].iloc[i])\n",
    "        print('####')\n",
    "        print(test_dataset.df['Stralci'].iloc[i])\n",
    "        print('####')\n",
    "        print(bert_spans[i])\n",
    "        print('----------')\n",
    "        \"\"\"\n",
    "        wd_value = windowdiff(seg_gt_trunk, seg_pred,  20)\n",
    "\n",
    "        ghd_value = ghd(seg_gt_trunk, seg_pred)\n",
    "\n",
    "        pk_value = pk(seg_gt_trunk, seg_pred, 20)\n",
    "\n",
    "        text_IoUs = []\n",
    "        for bound in bert_preds[i]:\n",
    "            IoUs = compute_IoUs(bound, test_dataset.df['Bounds'].iloc[i])\n",
    "            best = np.argmax(IoUs)\n",
    "            text_IoUs.append(IoUs[best])\n",
    "\n",
    "        met_dict = {\n",
    "            'windowdiff' : wd_value,\n",
    "            'ghd' : ghd_value,\n",
    "            'pk' : pk_value,\n",
    "            'iou' : text_IoUs\n",
    "            }\n",
    "        met_list.append(met_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_met_list = []\n",
    "norm_span_counter = 0\n",
    "\n",
    "\n",
    "for i in range(len(test_dataset.df.index)):\n",
    "    if len(test_dataset.df['Segmentation'].iloc[i]) >= 20:\n",
    "        norm_pred_bounds = normalize_bounds_by_repertoire(bert_preds[i], test_dataset.df.iloc[i])\n",
    "        norm_span_counter += len(norm_pred_bounds)\n",
    "\n",
    "        seg_pred = find_segmentation_by_bounds(norm_pred_bounds)\n",
    "        seg_pred = seg_pred[:len(test_dataset.df['Segmentation'].iloc[i])] #artificioso, sarebbe meglio risolvere ed avere le strighe uguali\n",
    "        seg_gt_trunk = test_dataset.df['Segmentation'].iloc[i][:len(seg_pred)] # manages predictiones in text with n_tokens  > 512\n",
    "\n",
    "        wd_value = windowdiff(seg_gt_trunk, seg_pred,  20)\n",
    "\n",
    "        ghd_value = ghd(seg_gt_trunk, seg_pred)\n",
    "\n",
    "        pk_value = pk(seg_gt_trunk, seg_pred, 20)\n",
    "\n",
    "        text_IoUs = []\n",
    "        for bound in norm_pred_bounds:\n",
    "            IoUs = compute_IoUs(bound, test_dataset.df['Bounds'].iloc[i])\n",
    "            best = np.argmax(IoUs)\n",
    "            text_IoUs.append(IoUs[best])\n",
    "\n",
    "        norm_met_dict = {\n",
    "            'windowdiff' : wd_value,\n",
    "            'ghd' : ghd_value,\n",
    "            'pk' : pk_value,\n",
    "            'iou' : text_IoUs\n",
    "            }\n",
    "        norm_met_list.append(norm_met_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------\n",
      "Risultati labels GT e stralci non uniti\n",
      "Numero testi nel dataset: 15332\n",
      "Numero testi cointati nel calcolo metriche (len >= 20) 184\n",
      "Numero stralci nel dataset: 35148\n",
      "Numero stralci predetti: 1132\n",
      "Percentuale span perfetti:  0.05339366515837104\n",
      "Media IoU: 0.42947760030978854\n",
      "Media Windowdiff: 0.5663517756103071\n",
      "Media Pk: 0.33235103671287547\n",
      "Media ghd: 8.668478260869565\n"
     ]
    }
   ],
   "source": [
    "print('----------------------------------------------------------')\n",
    "print('Risultati labels GT e stralci non uniti')\n",
    "\n",
    "print('Numero testi nel dataset:', str(len(dataset)))\n",
    "print('Numero testi cointati nel calcolo metriche (len >= 20)', str(counter))\n",
    "\n",
    "n_spans = 0\n",
    "for e in dataset:\n",
    "    n_spans += len(e['Bounds'])\n",
    "print('Numero stralci nel dataset:', str(n_spans))\n",
    "\n",
    "n_spans = 0\n",
    "for e in bert_preds:\n",
    "    n_spans += len(e)\n",
    "print('Numero stralci predetti:', str(n_spans))\n",
    "\n",
    "IoUs = [e['iou'] for e in met_list]\n",
    "flat_IoUs = [item for sublist in IoUs for item in sublist]\n",
    "mean_IoU = np.mean(flat_IoUs)\n",
    "mean_wd = np.mean([e['windowdiff'] for e in met_list])\n",
    "mean_pk = np.mean([e['pk'] for e in met_list])\n",
    "mean_ghd = np.mean([e['ghd'] for e in met_list])\n",
    "\n",
    "perfect_spans = flat_IoUs.count(1)\n",
    "print('Percentuale span perfetti: ', str(perfect_spans / len(flat_IoUs)))\n",
    "\n",
    "print('Media IoU:', str(mean_IoU))\n",
    "print('Media Windowdiff:', str(mean_wd))\n",
    "print('Media Pk:', str(mean_pk))\n",
    "print('Media ghd:', str(mean_ghd))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------\n",
      "Risultati labels GT e stralci uniti\n",
      "Numero testi nel dataset: 15332\n",
      "Numero stralci nel dataset: 35148\n",
      "Numero stralci predetti: 595\n",
      "Percentuale span perfetti:  0.1781512605042017\n",
      "Media IoU: 0.7721682550625731\n",
      "Media Windowdiff: 0.18784616344561764\n",
      "Media Pk: 0.07011654702625082\n",
      "Media ghd: 3.875\n"
     ]
    }
   ],
   "source": [
    "print('----------------------------------------------------------')\n",
    "print('Risultati labels GT e stralci uniti')\n",
    "\n",
    "print('Numero testi nel dataset:', str(len(dataset)))\n",
    "\n",
    "n_spans = 0\n",
    "for e in dataset:\n",
    "    n_spans += len(e['Bounds'])\n",
    "print('Numero stralci nel dataset:', str(n_spans))\n",
    "\n",
    "print('Numero stralci predetti:', str(norm_span_counter))\n",
    "\n",
    "IoUs = [e['iou'] for e in norm_met_list]\n",
    "flat_IoUs = [item for sublist in IoUs for item in sublist]\n",
    "mean_IoU = np.mean(flat_IoUs)\n",
    "mean_wd = np.mean([e['windowdiff'] for e in norm_met_list])\n",
    "mean_pk = np.mean([e['pk'] for e in norm_met_list])\n",
    "mean_ghd = np.mean([e['ghd'] for e in norm_met_list])\n",
    "\n",
    "perfect_spans = flat_IoUs.count(1)\n",
    "\n",
    "print('Percentuale span perfetti: ', str(perfect_spans / len(flat_IoUs)))\n",
    "\n",
    "print('Media IoU:', str(mean_IoU))\n",
    "print('Media Windowdiff:', str(mean_wd))\n",
    "print('Media Pk:', str(mean_pk))\n",
    "print('Media ghd:', str(mean_ghd))\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "925faa4ca74e07e17e8807425b2222c7f6b32ec00bddad3c89cd83a7cae0c688"
  },
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
