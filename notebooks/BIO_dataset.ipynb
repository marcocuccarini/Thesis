{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('../data/Original_csv/Hyperion.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def find_char_bounds(spans: list, text: str) -> list:\n",
    "    '''\n",
    "    Given a list of spans and a text, find the start and end indices of each span in the text.\n",
    "    Indeces are computed counting chars\n",
    "    \n",
    "    :param spans: a list of strings to search for\n",
    "    :type spans: list\n",
    "    :param text: the text to search\n",
    "    :type text: str\n",
    "    :return: A list of tuples, where each tuple contains the start and end index of a span.\n",
    "    '''\n",
    "    bounds = []\n",
    "    last_char = 0\n",
    "    for span in spans:\n",
    "        start = text.find(span)\n",
    "        if start == -1:\n",
    "            start = last_char + 1\n",
    "        bounds.append((start, start + len(span)-1))\n",
    "        last_char = start + len(span)-1\n",
    "    return bounds\n",
    "\n",
    "\n",
    "def find_word_bounds(spans: list, text: str) -> list:\n",
    "    bounds = []\n",
    "    end = 0\n",
    "    for span in spans:\n",
    "        s = span.translate(str.maketrans('', '', string.punctuation))\n",
    "        word_list = s.split()\n",
    "        text_list = text.translate(str.maketrans(\n",
    "            '', '', string.punctuation)).split()\n",
    "        try:\n",
    "            start = text_list.index(word_list[0], end)\n",
    "        except:\n",
    "            if not bounds:\n",
    "                start = 0\n",
    "            else:\n",
    "\n",
    "                start = bounds[-1][1] + 1\n",
    "        end = start + len(word_list) - 1\n",
    "\n",
    "        bounds.append((start, end))\n",
    "    return bounds\n",
    "\n",
    "\n",
    "def find_subword_bounds(spans: list, text: str, tokenizer) -> list:\n",
    "    bounds = []\n",
    "    end = 0\n",
    "    text_tok_list = tokenizer.tokenize(text)\n",
    "    for span in spans:\n",
    "        tok_list = tokenizer.tokenize(span)\n",
    "        if not bounds:\n",
    "            start = 0\n",
    "        else:\n",
    "            start = bounds[-1][1] + 1\n",
    "        for i in range(start, len(text_tok_list)):\n",
    "            if tok_list[-1] == text_tok_list[i]:\n",
    "                end = i\n",
    "                break\n",
    "        bounds.append((start, end))\n",
    "    print(bounds[-1][1])\n",
    "    return bounds\n",
    "\n",
    "\n",
    "def find_subword_bounds2(spans: list, text: str, tokenizer) -> list:\n",
    "    bounds = []\n",
    "    end = 0\n",
    "    text_tok_list = tokenizer.tokenize(text)\n",
    "    text_tok_list = [token for token in text_tok_list if token!='[UNK]']\n",
    "    start = 0\n",
    "    tokenized_spans = []\n",
    "    for span in spans:\n",
    "        tokenized_span = tokenizer.tokenize(span)\n",
    "        tokenized_span = [token for token in tokenized_span if token!='[UNK]']\n",
    "        tokenized_spans.append(tokenized_span)\n",
    "    i =0\n",
    "    for j, token in enumerate(text_tok_list):\n",
    "        if token == tokenized_spans[i][-1]:\n",
    "                end = j\n",
    "                bounds.append((start, end))\n",
    "                start = end + 1\n",
    "                i += 1\n",
    "    \n",
    "    return bounds\n",
    "\n",
    "\n",
    "def IE_gen(bounds: list) -> str:\n",
    "    end = [bound[1] for bound in bounds]\n",
    "    x = ['E' if i in end else 'I' for i in range(end[-1]+1)]\n",
    "    return ''.join(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "dataset = []\n",
    "\n",
    "for row in df.itertuples():\n",
    "    text = row.Testo\n",
    "    if pd.isna(text):\n",
    "        sample['Stralci'].append(row.Stralcio)\n",
    "        sample['Repertori'].append(row.Repertorio)\n",
    "\n",
    "    else:\n",
    "        sample = {}\n",
    "        sample['Testo'] = text\n",
    "        sample['Stralci'] = [row.Stralcio]\n",
    "\n",
    "        sample['Repertori'] = [row.Repertorio]\n",
    "        dataset.append(sample)\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(\n",
    "    \"m-polignano-uniba/bert_uncased_L-12_H-768_A-12_italian_alb3rt0\")\n",
    "for i,sample in enumerate(dataset):\n",
    "    sample['Bounds'] = find_char_bounds(sample['Stralci'], sample['Testo'])\n",
    "    sample['Word_Bounds'] = find_word_bounds(sample['Stralci'], sample['Testo'])\n",
    "    sample['Tags'] = IE_gen(sample['Bounds'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15332\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "j = 0\n",
    "while j < len(dataset):\n",
    "\n",
    "    x = len(dataset[j]['Tags'])\n",
    "    y = len(tok.tokenize(dataset[j]['Testo']))\n",
    "    if x != y:\n",
    "        i += 1\n",
    "        #del dataset[j]\n",
    "        #j -= 1\n",
    "    j += 1\n",
    "print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IE_dict = {\n",
    "    'Testo': [sample['Testo'] for sample in dataset],\n",
    "    'Tokens': [tok.tokenize(sample['Testo']) for sample in dataset],\n",
    "    'Subword_Tags': [sample['Subword_Tags'] for sample in dataset],\n",
    "    'Subword_bounds': [sample['Subword_Bounds'] for sample in dataset],\n",
    "    'Bounds': [sample['Bounds'] for sample in dataset],\n",
    "    'Repertori': [sample['Repertori'] for sample in dataset]\n",
    "}\n",
    "IE_df = pd.DataFrame(IE_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "IE_dict = {\n",
    "    'Testo': [sample['Testo'] for sample in dataset],\n",
    "    'Tokens': [tok.tokenize(sample['Testo']) for sample in dataset],\n",
    "    'Tags': [sample['Tags'] for sample in dataset],\n",
    "    'Bounds': [sample['Bounds'] for sample in dataset],\n",
    "    'Repertori': [sample['Repertori'] for sample in dataset]\n",
    "}\n",
    "IE_df = pd.DataFrame(IE_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "278\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "278"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(IE_df.iloc[0]['Tags']))\n",
    "len(IE_df.iloc[0]['Testo'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IE_df.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "LABELS = [\n",
    "    'I',\n",
    "    'E'\n",
    "]\n",
    "\n",
    "\n",
    "def encode_labels(labels_list):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(LABELS)\n",
    "    return le.transform(labels_list)\n",
    "\n",
    "\n",
    "class IE_Hyperion_dataset(Dataset):\n",
    "    def __init__(self, df, tokenizer_name):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "        self.df = df\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.df['Testo'].iloc[idx]\n",
    "        encoding = self.tokenizer(text,\n",
    "                                  # is_pretokenized=True,\n",
    "                                  return_special_tokens_mask=True,\n",
    "                                  return_offsets_mapping=True,\n",
    "                                  add_special_tokens=True,\n",
    "                                  return_attention_mask=True,\n",
    "                                  padding='max_length',\n",
    "                                  truncation=True,\n",
    "                                  )\n",
    "        labels = list(self.df['Tags'].iloc[idx])\n",
    "\n",
    "        \n",
    "        encoded_labels = np.ones(len(encoding['input_ids']), dtype=int) * -100\n",
    "        i = 0\n",
    "        for idx, e in enumerate(encoding['offset_mapping']):\n",
    "            if e[1]!=0:\n",
    "                if 'E' in labels[int(e[0]):int(e[1])]:\n",
    "                    # overwrite label\n",
    "                    encoded_labels[idx] = encode_labels(['E'])[0]\n",
    "                    i += 1\n",
    "                else:\n",
    "                    encoded_labels[idx] = encode_labels(['I'])[0]\n",
    "        \n",
    "        item = {key: torch.as_tensor(val) for key, val in encoding.items()}\n",
    "        item['labels'] = torch.as_tensor(encoded_labels)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL Dataset: (15332, 5)\n",
      "TRAIN Dataset: (9813, 5)\n",
      "VALIDATION Dataset: (2453, 5)\n",
      "TEST Dataset: (3066, 5)\n"
     ]
    }
   ],
   "source": [
    "model_name = \"bert-base-multilingual-cased\"\n",
    "train_size = 0.8\n",
    "train_df = IE_df.sample(frac=train_size, random_state=200)\n",
    "test_df = IE_df.drop(train_df.index).reset_index(drop=True)\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "val_size = 0.2\n",
    "val_df = train_df.sample(frac=val_size, random_state=200)\n",
    "train_df = train_df.drop(val_df.index).reset_index(drop=True)\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "\n",
    "print(\"FULL Dataset: {}\".format(IE_df.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_df.shape))\n",
    "print(\"VALIDATION Dataset: {}\".format(val_df.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_df.shape))\n",
    "\n",
    "train_dataset = IE_Hyperion_dataset(train_df, model_name)\n",
    "val_dataset = IE_Hyperion_dataset(val_df, model_name)\n",
    "test_dataset = IE_Hyperion_dataset(test_df, model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0],\n",
       " [0, 3],\n",
       " [3, 6],\n",
       " [7, 11],\n",
       " [12, 15],\n",
       " [16, 18],\n",
       " [19, 24],\n",
       " [25, 28],\n",
       " [29, 30],\n",
       " [31, 33],\n",
       " [34, 37],\n",
       " [37, 40],\n",
       " [41, 48],\n",
       " [49, 55],\n",
       " [56, 57],\n",
       " [58, 61],\n",
       " [62, 66],\n",
       " [67, 70],\n",
       " [70, 72],\n",
       " [72, 76],\n",
       " [76, 78],\n",
       " [79, 80],\n",
       " [80, 84],\n",
       " [85, 88],\n",
       " [89, 96],\n",
       " [97, 102],\n",
       " [103, 111],\n",
       " [111, 113],\n",
       " [114, 119],\n",
       " [120, 122],\n",
       " [123, 124],\n",
       " [125, 127],\n",
       " [128, 130],\n",
       " [131, 138],\n",
       " [139, 144],\n",
       " [144, 145],\n",
       " [146, 149],\n",
       " [150, 153],\n",
       " [153, 157],\n",
       " [157, 158],\n",
       " [159, 161],\n",
       " [162, 167],\n",
       " [168, 174],\n",
       " [175, 176],\n",
       " [177, 185],\n",
       " [186, 188],\n",
       " [189, 195],\n",
       " [196, 201],\n",
       " [202, 204],\n",
       " [205, 210],\n",
       " [210, 211],\n",
       " [212, 215],\n",
       " [215, 217],\n",
       " [218, 220],\n",
       " [221, 230],\n",
       " [231, 234],\n",
       " [234, 236],\n",
       " [237, 239],\n",
       " [240, 242],\n",
       " [243, 245],\n",
       " [246, 250],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0]]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[2]['offset_mapping'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 55), (56, 78), (79, 119), (120, 145), (146, 184), (186, 211), (212, 249)]\n",
      "tensor([[0, 0],\n",
      "        [0, 3],\n",
      "        [3, 6],\n",
      "        ...,\n",
      "        [0, 0],\n",
      "        [0, 0],\n",
      "        [0, 0]])\n",
      "[CLS]       -100\n",
      "Vor         1\n",
      "##rei       1\n",
      "dire        1\n",
      "che         1\n",
      "il          1\n",
      "virus       1\n",
      "non         1\n",
      "l           1\n",
      "ha          1\n",
      "vol         1\n",
      "##uto       1\n",
      "nessuno     1\n",
      "quindi      1\n",
      "e           1\n",
      "una         1\n",
      "cosa        1\n",
      "ina         1\n",
      "##sp        1\n",
      "##etta      1\n",
      "##ta        1\n",
      "c           1\n",
      "##redo      1\n",
      "che         1\n",
      "nessuno     1\n",
      "possa       1\n",
      "pretende    1\n",
      "##re        1\n",
      "nulla       1\n",
      "se          1\n",
      "1           1\n",
      "ha          1\n",
      "un          1\n",
      "azienda     1\n",
      "solid       1\n",
      "##a         1\n",
      "non         1\n",
      "cam         1\n",
      "##bier      1\n",
      "##à         1\n",
      "se          1\n",
      "tiene       1\n",
      "chiuso      1\n",
      "1           1\n",
      "stagione    0\n",
      "la          1\n",
      "salute      1\n",
      "prima       1\n",
      "di          1\n",
      "tutto       1\n",
      ".           1\n",
      "For         1\n",
      "##se        1\n",
      "in          1\n",
      "primavera   1\n",
      "ved         1\n",
      "##rò        1\n",
      "un          1\n",
      "Po          1\n",
      "di          1\n",
      "luce        0\n",
      "[SEP]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.df.iloc[2]['Bounds'])\n",
    "print(train_dataset[2][\"offset_mapping\"])\n",
    "\n",
    "for token, label in zip(train_dataset.tokenizer.convert_ids_to_tokens(train_dataset[2][\"input_ids\"]), train_dataset[2][\"labels\"]):\n",
    "  print('{0:10}  {1}'.format(token, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_name, num_labels=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "# Deterministic mode\n",
    "def seed_everything(seed=1464):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "\n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n",
    "\n",
    "def plot_loss(loss, val_loss):\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    plt.xticks(range(1, len(loss)+1))\n",
    "    plt.plot(range(1, len(loss)+1), loss, label='train')\n",
    "    plt.plot(range(1, len(val_loss)+1), val_loss, label='val')\n",
    "    plt.title('loss')\n",
    "    plt.legend()\n",
    "    # plt.savefig('loss.png')\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_f1(f1, val_f1):\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    plt.xticks(range(1, len(f1)+1))\n",
    "    plt.plot(range(1, len(f1)+1), f1, label='train')\n",
    "    plt.plot(range(1, len(val_f1)+1), val_f1, label='val')\n",
    "    plt.title('f1')\n",
    "    plt.legend()\n",
    "    # plt.savefig('f1.png')\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(y_true, pred, labels):\n",
    "    fig, ax = plt.subplots(figsize=(20, 20))\n",
    "    disp = ConfusionMatrixDisplay.from_predictions(\n",
    "        y_true, pred, display_labels=labels, normalize='true', values_format='.2f')\n",
    "    disp.plot(cmap=\"Blues\", values_format='.2g',\n",
    "              xticks_rotation='vertical', ax=ax)\n",
    "    return disp.figure_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neptune.new as neptune\n",
    "\n",
    "\n",
    "class NeptuneLogger():\n",
    "    def __init__(self) -> None:\n",
    "        # Neptune initialization\n",
    "        self.run = neptune.init(\n",
    "            project=\"mibo8/Rep\",\n",
    "            api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJmZmRkYThiZi1mZGNlLTRlMTktODQwNS1hNWFlMWQ2Mjc4N2IifQ==\",\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "from torch.nn import utils\n",
    "\n",
    "import torchmetrics\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW\n",
    "from transformers import get_constant_schedule_with_warmup\n",
    "\n",
    "\n",
    "import neptune.new as neptune\n",
    "\n",
    "\n",
    "class IE_MPTrainer():\n",
    "    def __init__(self, batch_size, lr, n_epochs) -> None:\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = lr\n",
    "        self.n_epochs = n_epochs\n",
    "\n",
    "        self.logger = NeptuneLogger()\n",
    "\n",
    "    def fit(self, model, train_dataset, val_dataset):\n",
    "        self.logger.run['model'] = model_name\n",
    "\n",
    "        params_info = {\n",
    "            'learning_rate': self.learning_rate,\n",
    "            'batch_size': self.batch_size,\n",
    "            'n_epochs': self.n_epochs\n",
    "        }\n",
    "        #self.logger.run['params'] = params_info\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        # ----------TRAINING\n",
    "\n",
    "        # Measure the total training time for the whole run.\n",
    "        total_t0 = time.time()\n",
    "\n",
    "        epochs_train_loss = []\n",
    "        epochs_val_loss = []\n",
    "\n",
    "        epochs = self.n_epochs\n",
    "\n",
    "        # Creation of Pytorch DataLoaders with shuffle=True for the traing phase\n",
    "        train_dataloader = DataLoader(\n",
    "            train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        validation_dataloader = DataLoader(\n",
    "            val_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "        # Adam algorithm optimized for tranfor architectures\n",
    "        optimizer = AdamW(model.parameters(), lr=self.learning_rate)\n",
    "        #scheduler = get_constant_schedule_with_warmup(optimizer, num_warmup_steps=300)\n",
    "\n",
    "        # Scaler for mixed precision\n",
    "        scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "        # Setup for training with gpu\n",
    "        device = torch.device(\n",
    "            'cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "        model.to(device)\n",
    "\n",
    "        # For each epoch...\n",
    "        for epoch_i in range(0, epochs):\n",
    "\n",
    "            # ========================================\n",
    "            #               Training\n",
    "            # ========================================\n",
    "\n",
    "            # Perform one full pass over the training set.\n",
    "\n",
    "            print(\"\")\n",
    "            print(\n",
    "                '======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "            print('Training...')\n",
    "\n",
    "            # Measure how long the training epoch takes.\n",
    "            t0 = time.time()\n",
    "\n",
    "            # Reset the total loss for this epoch.\n",
    "            total_train_loss = 0\n",
    "\n",
    "            # Put the model into training mode: Dropout layers are active\n",
    "            model.train()\n",
    "\n",
    "            # For each batch of training data...\n",
    "            for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "                # Progress update every 40 batches.\n",
    "                if step % 10 == 0 and not step == 0:\n",
    "                    # Compute time in minutes.\n",
    "                    elapsed = format_time(time.time() - t0)\n",
    "\n",
    "                    # Report progress.\n",
    "                    print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(\n",
    "                        step, len(train_dataloader), elapsed))\n",
    "\n",
    "                # Unpack this training batch from the dataloader.\n",
    "                #\n",
    "                #  copy each tensor to the GPU using the 'to()' method\n",
    "                #\n",
    "                # 'batch' contains three pytorch tensors:\n",
    "                #   [0]: input ids\n",
    "                #   [1]: attention masks\n",
    "                #   [2]: labels\n",
    "                b_input_ids = batch['input_ids'].to(device)\n",
    "                b_input_mask = batch['attention_mask'].to(device)\n",
    "                b_labels = batch['labels'].to(device)\n",
    "\n",
    "                # clear any previously calculated gradients before performing a\n",
    "                # backward pass\n",
    "                model.zero_grad()\n",
    "\n",
    "                # Perform a forward pass in mixed precision\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    outputs = model(b_input_ids,\n",
    "                                    attention_mask=b_input_mask,\n",
    "                                    labels=b_labels)\n",
    "\n",
    "                    loss = outputs[0]\n",
    "                    logits = outputs[1]\n",
    "\n",
    "                # Move logits and labels to CPU\n",
    "                logits = logits.detach().cpu()\n",
    "                label_ids = b_labels.to('cpu')\n",
    "\n",
    "                # Perform a backward pass to compute the gradients in MIXED precision\n",
    "                scaler.scale(loss).backward()\n",
    "\n",
    "                # Accumulate the training loss over all of the batches so that we can\n",
    "                # calculate the average loss at the end.\n",
    "                total_train_loss += loss.item()\n",
    "\n",
    "                # Unscales the gradients of optimizer's assigned params in-place before the gradient clipping\n",
    "                scaler.unscale_(optimizer)\n",
    "\n",
    "                # Clip the norm of the gradients to 1.0.\n",
    "                # This helps and prevent the \"exploding gradients\" problem.\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "                # Update parameters and take a step using the computed gradient in MIXED precision\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                # scheduler.step()\n",
    "\n",
    "            # Compute the average loss over all of the batches.\n",
    "            avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "            epochs_train_loss.append(avg_train_loss)\n",
    "\n",
    "            # Measure how long this epoch took.\n",
    "            training_time = format_time(time.time() - t0)\n",
    "\n",
    "            print(\"\")\n",
    "            print(\"  Average training loss: {0:.3f}\".format(avg_train_loss))\n",
    "            print(\"  Training epoch took: {:}\".format(training_time))\n",
    "\n",
    "            # ========================================\n",
    "            #               Validation\n",
    "            # ========================================\n",
    "            # After the completion of each training epoch, measure performance on\n",
    "            # the validation set.\n",
    "\n",
    "            print(\"\")\n",
    "            print(\"Running Validation...\")\n",
    "\n",
    "            t0 = time.time()\n",
    "\n",
    "            # Put the model in evaluation mode: the dropout layers behave differently\n",
    "            model.eval()\n",
    "\n",
    "            total_val_loss = 0\n",
    "\n",
    "            # Evaluate data for one epoch\n",
    "            for batch in validation_dataloader:\n",
    "\n",
    "                # Unpack this training batch from our dataloader.\n",
    "                #\n",
    "                # copy each tensor to the GPU using the 'to()' method\n",
    "                #\n",
    "                # 'batch' contains three pytorch tensors:\n",
    "                #   [0]: input ids\n",
    "                #   [1]: attention masks\n",
    "                #   [2]: labels\n",
    "                b_input_ids = batch['input_ids'].to(device)\n",
    "                b_input_mask = batch['attention_mask'].to(device)\n",
    "                b_labels = batch['labels'].to(device)\n",
    "\n",
    "                # Tell pytorch not to bother with constructing the compute graph during\n",
    "                # the forward pass, since this is only needed for training.\n",
    "                with torch.no_grad():\n",
    "\n",
    "                    # Forward pass, calculate logits\n",
    "                    # argmax(logits) = argmax(Softmax(logits))\n",
    "                    outputs = model(b_input_ids,\n",
    "                                    attention_mask=b_input_mask,\n",
    "                                    labels=b_labels)\n",
    "                    loss = outputs[0]\n",
    "                    logits = outputs[1]\n",
    "\n",
    "                # Accumulate the validation loss.\n",
    "                total_val_loss += loss.item()\n",
    "\n",
    "                # Move logits and labels to CPU\n",
    "                logits = logits.detach().cpu()\n",
    "                label_ids = b_labels.to('cpu')\n",
    "\n",
    "            print('VALIDATION: ')\n",
    "\n",
    "            # Compute the average loss over all of the batches.\n",
    "            avg_val_loss = total_val_loss / len(validation_dataloader)\n",
    "            epochs_val_loss.append(avg_val_loss)\n",
    "\n",
    "            # Measure how long the validation run took.\n",
    "            validation_time = format_time(time.time() - t0)\n",
    "\n",
    "            print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "            print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "        loss_fig = plot_loss(epochs_train_loss, epochs_val_loss)\n",
    "\n",
    "        self.logger.run[\"loss\"].upload(neptune.types.File.as_image(loss_fig))\n",
    "        print(\"\")\n",
    "        print(\"Training complete!\")\n",
    "\n",
    "        print(\"Total training took {:} (h:mm:ss)\".format(\n",
    "            format_time(time.time()-total_t0)))\n",
    "\n",
    "    def test(self, model, test_dataset):\n",
    "        # ========================================\n",
    "        #               Test\n",
    "        # ========================================\n",
    "        test_dataloader = DataLoader(\n",
    "            test_dataset, batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "        # Setup for testing with gpu\n",
    "        device = torch.device(\n",
    "            'cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "        print(\"\")\n",
    "        print(\"Running Test...\")\n",
    "        t0 = time.time()\n",
    "\n",
    "        # Save prediction for confusion matrix\n",
    "        pred = []\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        total_test_loss = 0\n",
    "\n",
    "        # Evaluate data for one epoch\n",
    "        for batch in test_dataloader:\n",
    "            b_input_ids = batch['input_ids'].to(device)\n",
    "            b_input_mask = batch['attention_mask'].to(device)\n",
    "            b_labels = batch['labels'].to(device)\n",
    "            b_special_tokens_mask = batch['special_tokens_mask'].to(device)\n",
    "            with torch.no_grad():\n",
    "\n",
    "                # Forward pass, calculate logits\n",
    "                # argmax(logits) = argmax(Softmax(logits))\n",
    "                outputs = model(b_input_ids,\n",
    "                                attention_mask=b_input_mask,\n",
    "                                labels=b_labels)\n",
    "                loss = outputs[0]\n",
    "                logits = outputs[1]\n",
    "\n",
    "            # Accumulate the test loss.\n",
    "            total_test_loss += loss.item()\n",
    "\n",
    "            # Move logits and labels to CPU\n",
    "            logits = logits.detach().cpu()  # shape (batch_size, seq_len, num_labels)\n",
    "            label_ids = b_labels.to('cpu')\n",
    "\n",
    "            batch_pred = logits.softmax(dim=-1)\n",
    "            full_pred = batch_pred.argmax(dim=-1)\n",
    "\n",
    "            for i, sample_pred in enumerate(full_pred):\n",
    "                active_pred = []\n",
    "                for j, e in enumerate(b_special_tokens_mask[i]):\n",
    "                    if(e == 0):\n",
    "                        active_pred.append(int(sample_pred[j]))\n",
    "                pred.append(active_pred)\n",
    "\n",
    "        avg_test_loss = total_test_loss / len(test_dataloader)\n",
    "        #self.logger.run['test/loss'] = avg_test_loss\n",
    "        test_time = format_time(time.time() - t0)\n",
    "\n",
    "        print(\"  Test Loss: {0:.2f}\".format(avg_test_loss))\n",
    "        print(\"  Test took: {:}\".format(test_time))\n",
    "\n",
    "        return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 1e-5\n",
    "batch_size = 2\n",
    "n_epochs = 1\n",
    "trainer = IE_MPTrainer(batch_size, learning_rate, n_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model, train_dataset, val_dataset)\n",
    "pred = trainer.test(model, test_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_to_bounds(pred: list):\n",
    "    dataset_bounds = []\n",
    "    for e in pred:\n",
    "        start = 0\n",
    "        end = 0\n",
    "        bounds = []\n",
    "        for i, tok_pred in enumerate(e):\n",
    "            if tok_pred == 0:\n",
    "                end = i\n",
    "                bounds.append((start, end))\n",
    "                start = end + 1\n",
    "        dataset_bounds.append(bounds)\n",
    "    return dataset_bounds\n",
    "\n",
    "\n",
    "bert_pred = pred_to_bounds(pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# A è B sono tupe con i bound dello span\n",
    "\n",
    "\n",
    "def IoU(A, B):\n",
    "    if A == B:\n",
    "        return 1\n",
    "    start = max(A[0], B[0])\n",
    "    end = min(A[1], B[1])\n",
    "    if(start > end):\n",
    "        return 0\n",
    "    intersection = end - start\n",
    "    return intersection / (A[1] - A[0] + B[1] - B[0] - intersection)\n",
    "\n",
    "\n",
    "def compute_IoUs(pred_bounds, gt_spans):\n",
    "    IoUs = []\n",
    "    for gt_bounds in gt_spans:\n",
    "        IoUs.append(IoU(pred_bounds, gt_bounds))\n",
    "    return IoUs\n",
    "\n",
    "\n",
    "def normalize(text_spans_dict, gt_spans):\n",
    "    normalized = []\n",
    "    for i in range(len(text_spans_dict)):\n",
    "        #normalized is not empty\n",
    "        if normalized:\n",
    "            if normalized[-1]['Repertorio'] == text_spans_dict[i]['Repertorio']:\n",
    "                new_span = (normalized[-1]['Bounds'][0],\n",
    "                            text_spans_dict[i]['Bounds'][1])\n",
    "                new_span_features = {\n",
    "                    'Bounds': new_span,\n",
    "                    'IoU': None,\n",
    "                    'Repertorio': text_spans_dict[i]['Repertorio']\n",
    "                }\n",
    "                del normalized[-1]\n",
    "                normalized.append(new_span_features)\n",
    "            else:\n",
    "                normalized.append(text_spans_dict[i])\n",
    "        else:\n",
    "            normalized.append(text_spans_dict[i])\n",
    "\n",
    "    for i in range(len(normalized)):\n",
    "        normalized[i]['IoU'] = max(compute_IoUs(\n",
    "            normalized[i]['Bounds'], gt_spans['Subword_bounds']))\n",
    "    return normalized\n",
    "\n",
    "\n",
    "metrics = []\n",
    "normalized_metrics = []\n",
    "for i, pred_bounds in enumerate(bert_pred):\n",
    "    text_IoUs = []\n",
    "    for pred_span in pred_bounds:\n",
    "        IoUs = compute_IoUs(pred_span, test_df.iloc[i]['Subword_bounds'])\n",
    "        best = np.argmax(IoUs)\n",
    "        span_features = {\n",
    "            'Bounds': pred_span,\n",
    "            'IoU': IoUs[best],\n",
    "            'Repertorio': test_df.iloc[i]['Repertori'][best]\n",
    "        }\n",
    "\n",
    "        text_IoUs.append(span_features)\n",
    "    metrics.append(text_IoUs)\n",
    "    normalized_metrics.append(normalize(text_IoUs, test_df.iloc[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('----------------------------------------------------------')\n",
    "print('Risultati labels GT e stralci non uniti')\n",
    "\n",
    "\n",
    "n_spans = 0\n",
    "for e in test_df['Subword_bounds']:\n",
    "    n_spans += len(e)\n",
    "print('Numero stralci nel dataset:', str(n_spans))\n",
    "\n",
    "n_spans = 0\n",
    "for e in metrics:\n",
    "    n_spans += len(e)\n",
    "print('Numero stralci predetti:', str(n_spans))\n",
    "\n",
    "mean = 0\n",
    "long_spans = 0\n",
    "min_lenght = 0\n",
    "perfect_spans = 0\n",
    "for text in metrics:\n",
    "    for span in text:\n",
    "        if span['Bounds'][1] - span['Bounds'][0] >= min_lenght:\n",
    "            long_spans += 1\n",
    "            mean += span['IoU']\n",
    "            if span['IoU'] == 1:\n",
    "                perfect_spans += 1\n",
    "perfect_spans_perc = perfect_spans / long_spans\n",
    "mean_IoU = mean / long_spans\n",
    "print('Numero stralci con lunghezza minima = ',\n",
    "      str(min_lenght), ': ', str(long_spans))\n",
    "print('Media IoU:', str(mean_IoU))\n",
    "print('Percentuale span perfetti: ', str(perfect_spans_perc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('----------------------------------------------------------')\n",
    "print('Risultati labels GT e stralci uniti')\n",
    "\n",
    "\n",
    "n_spans = 0\n",
    "for e in test_df['Subword_bounds']:\n",
    "    n_spans += len(e)\n",
    "print('Numero stralci nel dataset:', str(n_spans))\n",
    "\n",
    "n_spans = 0\n",
    "for e in normalized_metrics:\n",
    "    n_spans += len(e)\n",
    "print('Numero stralci predetti:', str(n_spans))\n",
    "\n",
    "mean = 0\n",
    "long_spans = 0\n",
    "min_lenght = 0\n",
    "perfect_spans = 0\n",
    "for text in normalized_metrics:\n",
    "    for span in text:\n",
    "        if span['Bounds'][1] - span['Bounds'][0] >= min_lenght:\n",
    "            long_spans += 1\n",
    "            mean += span['IoU']\n",
    "            if span['IoU'] == 1:\n",
    "                perfect_spans += 1\n",
    "perfect_spans_perc = perfect_spans / long_spans\n",
    "mean_IoU = mean / long_spans\n",
    "print('Numero stralci con lunghezza minima = ',\n",
    "      str(min_lenght), ': ', str(long_spans))\n",
    "print('Media IoU:', str(mean_IoU))\n",
    "print('Percentuale span perfetti: ', str(perfect_spans_perc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained('MiBo/RepML')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"m-polignano-uniba/bert_uncased_L-12_H-768_A-12_italian_alb3rt0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_dataset = []\n",
    "\n",
    "\n",
    "for i, span_group in enumerate(bert_pred):\n",
    "    text_features = {}\n",
    "    text_features['Testo'] = test_df.iloc[i]['Testo']\n",
    "    text_features['Stralci'] = [span.lower() for span in span_group]\n",
    "    text_features['Bounds'] = bert_pred[i]\n",
    "    predicted_dataset.append(text_features)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "531bf1ddd0b9ee64f0e2ebd6527c520de4e2159f2ffcc26ba5e4b06431092b93"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
