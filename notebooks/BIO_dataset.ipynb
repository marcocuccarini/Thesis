{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('../data/Original_csv/Hyperion.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def find_char_bounds(spans: list, text: str) -> list:\n",
    "    '''\n",
    "    Given a list of spans and a text, find the start and end indices of each span in the text.\n",
    "    Indeces are computed counting chars\n",
    "    \n",
    "    :param spans: a list of strings to search for\n",
    "    :type spans: list\n",
    "    :param text: the text to search\n",
    "    :type text: str\n",
    "    :return: A list of tuples, where each tuple contains the start and end index of a span.\n",
    "    '''\n",
    "    bounds = []\n",
    "    last_char = 0\n",
    "    for span in spans:\n",
    "        start = text.find(span)\n",
    "        if start == -1:\n",
    "            start = last_char + 1\n",
    "        bounds.append((start, start + len(span)-1))\n",
    "        last_char = start + len(span)-1\n",
    "    return bounds\n",
    "\n",
    "\n",
    "def IE_gen(bounds: list, text:str) -> str:\n",
    "    tags = ['I' for i in range(len(text))]\n",
    "    for bound in bounds:\n",
    "        if bound[1] < len(text):\n",
    "            tags[bound[1]] = 'E'\n",
    "        else:\n",
    "            tags[-1] = 'E'\n",
    "    return ''.join(tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import re\n",
    "\n",
    "dataset = []\n",
    "\n",
    "for row in df.itertuples():\n",
    "    text = row.Testo\n",
    "    \n",
    "    if pd.isna(text):\n",
    "        span =  re.sub(r'[\\?\\.\\!]+(?=[\\?\\.\\!])', '', row.Stralcio)\n",
    "        sample['Stralci'].append(span)\n",
    "        sample['Repertori'].append(row.Repertorio)\n",
    "\n",
    "    else:\n",
    "        sample = {}\n",
    "        text = re.sub(r'[\\?\\.\\!]+(?=[\\?\\.\\!])', '', text)\n",
    "        sample['Testo'] = text\n",
    "        span =  re.sub(r'[\\?\\.\\!]+(?=[\\?\\.\\!])', '', row.Stralcio)\n",
    "        sample['Stralci'] = [span]\n",
    "\n",
    "        sample['Repertori'] = [row.Repertorio]\n",
    "        dataset.append(sample)\n",
    "\n",
    "\n",
    "\n",
    "for i,sample in enumerate(dataset):\n",
    "    sample['Bounds'] = find_char_bounds(sample['Stralci'], sample['Testo'])\n",
    "    sample['Tags'] = IE_gen(sample['Bounds'], sample['Testo'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "IE_dict = {\n",
    "    'Testo': [sample['Testo'] for sample in dataset],\n",
    "    'Tags': [sample['Tags'] for sample in dataset],\n",
    "    'Bounds': [sample['Bounds'] for sample in dataset],\n",
    "    'Repertori': [sample['Repertori'] for sample in dataset],\n",
    "    'Stralci': [sample['Stralci'] for sample in dataset]\n",
    "}\n",
    "IE_df = pd.DataFrame(IE_dict)\n",
    "IE_df = IE_df.head(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "LABELS = [\n",
    "    'I',\n",
    "    'E'\n",
    "]\n",
    "\n",
    "\n",
    "def encode_labels(labels_list):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(LABELS)\n",
    "    return le.transform(labels_list)\n",
    "\n",
    "\n",
    "class IE_Hyperion_dataset(Dataset):\n",
    "    def __init__(self, df, tokenizer_name):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "        self.df = df\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.df['Testo'].iloc[idx]\n",
    "        encoding = self.tokenizer(text,\n",
    "                                  # is_pretokenized=True,\n",
    "                                  return_special_tokens_mask=True,\n",
    "                                  return_offsets_mapping=True,\n",
    "                                  add_special_tokens=True,\n",
    "                                  return_attention_mask=True,\n",
    "                                  padding='max_length',\n",
    "                                  truncation=True,\n",
    "                                  )\n",
    "        char_labels = encode_labels(list(self.df['Tags'].iloc[idx]))\n",
    "        ends = [i for i in range(len(char_labels)) if char_labels[i] == 0]\n",
    "\n",
    "        last_token_idx = max(index for index, item in enumerate(encoding['special_tokens_mask']) if item == 0)\n",
    "\n",
    "        encoded_labels = np.ones(len(encoding['input_ids']), dtype=int) * -100\n",
    "        x = ends.pop(0)\n",
    "        for i,e in enumerate(encoding['offset_mapping']):\n",
    "            if e[1] != 0:\n",
    "                # overwrite label\n",
    "                if x >= e[0] and x <= e[1]:\n",
    "                    encoded_labels[i] = 0\n",
    "                    if ends: \n",
    "                        x = ends.pop(0)\n",
    "                    else:\n",
    "                        x = -1\n",
    "                else:\n",
    "                    encoded_labels[i] = 1\n",
    "        if not 0 in encoded_labels:\n",
    "            encoded_labels[last_token_idx] = 0\n",
    "\n",
    "        \n",
    "\n",
    "        item = {key: torch.as_tensor(val) for key, val in encoding.items()}\n",
    "        item['labels'] = torch.as_tensor(encoded_labels)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL Dataset: (1000, 5)\n",
      "TRAIN Dataset: (640, 5)\n",
      "VALIDATION Dataset: (160, 5)\n",
      "TEST Dataset: (200, 5)\n"
     ]
    }
   ],
   "source": [
    "model_name = \"dbmdz/bert-base-italian-xxl-uncased\"\n",
    "train_size = 0.8\n",
    "train_df = IE_df.sample(frac=train_size, random_state=200)\n",
    "test_df = IE_df.drop(train_df.index).reset_index(drop=True)\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "val_size = 0.2\n",
    "val_df = train_df.sample(frac=val_size, random_state=200)\n",
    "train_df = train_df.drop(val_df.index).reset_index(drop=True)\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "\n",
    "print(\"FULL Dataset: {}\".format(IE_df.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_df.shape))\n",
    "print(\"VALIDATION Dataset: {}\".format(val_df.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_df.shape))\n",
    "\n",
    "train_dataset = IE_Hyperion_dataset(train_df, model_name)\n",
    "val_dataset = IE_Hyperion_dataset(val_df, model_name)\n",
    "test_dataset = IE_Hyperion_dataset(test_df, model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 270), (271, 357), (360, 389), (390, 450), (439, 511), (512, 587), (577, 587)]\n",
      "['sancire', 'giudizio', 'commento', 'sancire', 'commento', 'giudizio', 'commento']\n",
      "Non passerà un bel niente ! La strage di morti non si può dimenticare come non si può dimenticare chi a gennaio diceva che non c’è pericolo che è poco più di un influenza! Con un influenza non muoiono in un mese e mezzo 23000 persone , non si svuotano le case di riposo !ci sono dei responsabili per tutto questo ! Responsabili di omicidio colposo epidemico ! E non mi rallegra le nascite ! Non rimpiazzano chi è morto solo come un cane ! Se io fossi giovane non metterei al mondo dei figli per offrirgli cosa ? Un mondo senza futuro di ingiustizie , intollerante e violenze ! No grazie !\n",
      "['Non passerà un bel niente ! La strage di morti non si può dimenticare come non si può dimenticare chi a gennaio diceva che non c’è pericolo che è poco più di un influenza! Con un influenza non muoiono in un mese e mezzo 23000 persone , non si svuotano le case di riposo !', 'ci sono dei responsabili per tutto questo !Responsabili di omicidio colposo epidemico !', 'E non mi rallegra le nascite !', '(le nascite)  Non rimpiazzano chi è morto solo come un cane !', 'Se io fossi giovane non metterei al mondo dei figli per offrirgli cosa ? ', 'Un mondo senza futuro di ingiustizie , intollerante e violenze ! No grazie !', 'No grazie !']\n"
     ]
    }
   ],
   "source": [
    "print(test_dataset.df['Bounds'].iloc[20])\n",
    "print(test_dataset.df['Repertori'].iloc[20])\n",
    "print(test_dataset.df['Testo'].iloc[20])\n",
    "print(test_dataset.df['Stralci'].iloc[20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]       -100\n",
      "la          1\n",
      "mappa       1\n",
      "del         1\n",
      "potere      1\n",
      "durante     1\n",
      "l           1\n",
      "’           1\n",
      "emergenza   1\n",
      "co          1\n",
      "##vid       1\n",
      "-           1\n",
      "19          1\n",
      "in          1\n",
      "veneto      1\n",
      "i           1\n",
      "politici    1\n",
      ",           1\n",
      "i           1\n",
      "dirigenti   1\n",
      "regionali   1\n",
      "e           1\n",
      "gli         1\n",
      "esperti     1\n",
      "che         1\n",
      "compongono  1\n",
      "l           1\n",
      "'           1\n",
      "unita       1\n",
      "di          1\n",
      "crisi       1\n",
      "e           1\n",
      "gli         1\n",
      "altri       1\n",
      "organi      1\n",
      "creati      1\n",
      "per         1\n",
      "gestire     1\n",
      "l           1\n",
      "'           1\n",
      "emergenza   1\n",
      "sanitaria   1\n",
      ".           1\n",
      "nella       1\n",
      "mappa       1\n",
      "del         1\n",
      "potere      1\n",
      "che         1\n",
      "ha          1\n",
      "gestito     1\n",
      "l           1\n",
      "’           1\n",
      "emergenza   1\n",
      "corona      1\n",
      "##virus     1\n",
      "il          1\n",
      "ruolo       1\n",
      "centrale    1\n",
      "e           1\n",
      "attribuito  1\n",
      "al          1\n",
      "presidente  1\n",
      "za          1\n",
      "##ia        1\n",
      "in          1\n",
      "qualita     1\n",
      "di          1\n",
      "soggetto    1\n",
      "attuato     1\n",
      "##re        1\n",
      ".           1\n",
      "a           1\n",
      "lui         1\n",
      "fanno       1\n",
      "capo        1\n",
      "due         1\n",
      "gruppi      1\n",
      "di          1\n",
      "supporto    1\n",
      ",           1\n",
      "uno         1\n",
      "generico    1\n",
      "per         1\n",
      "le          1\n",
      "attivita    1\n",
      "del         1\n",
      "soggetto    1\n",
      "attuato     1\n",
      "##re        1\n",
      "e           1\n",
      "uno         1\n",
      "legale      1\n",
      ".           1\n",
      "za          1\n",
      "##ia        1\n",
      "inoltre     1\n",
      "insieme     1\n",
      "ai          1\n",
      "due         1\n",
      "asse        1\n",
      "##ssori     1\n",
      "chiave      1\n",
      "fa          1\n",
      "parte       1\n",
      "direttamente  1\n",
      "dell        1\n",
      "’           1\n",
      "unita       1\n",
      "di          1\n",
      "crisi       1\n",
      ".           1\n",
      "il          1\n",
      "raccordo    1\n",
      "tra         1\n",
      "l           1\n",
      "’           1\n",
      "unita       1\n",
      "di          1\n",
      "crisi       1\n",
      "e           1\n",
      "la          1\n",
      "task        1\n",
      "force       1\n",
      "e           1\n",
      "svolto      1\n",
      "da          1\n",
      "francesca   1\n",
      "russo       1\n",
      "che         1\n",
      ",           1\n",
      "oltre       1\n",
      "ad          1\n",
      "essere      1\n",
      "componente  1\n",
      "dell        1\n",
      "’           1\n",
      "unita       1\n",
      "di          1\n",
      "crisi       1\n",
      ",           1\n",
      "presie      1\n",
      "##de        1\n",
      "anche       1\n",
      "la          1\n",
      "task        1\n",
      "force       1\n",
      ",           1\n",
      "che         1\n",
      "ha          1\n",
      "istituito   1\n",
      "con         1\n",
      "proprio     1\n",
      "decreto     1\n",
      ".           1\n",
      "due         1\n",
      "esper       1\n",
      "##te        1\n",
      "del         1\n",
      "settore     1\n",
      "inoltre     1\n",
      "fanno       1\n",
      "parte       1\n",
      "sia         1\n",
      "della       1\n",
      "task        1\n",
      "force       1\n",
      "che         1\n",
      "del         1\n",
      "comitato    1\n",
      "tecnico     1\n",
      "scientifico  1\n",
      "(           1\n",
      "di          1\n",
      "cui         1\n",
      "e           1\n",
      "membro      1\n",
      "anche       1\n",
      "andrea      1\n",
      "cri         1\n",
      "##santi     1\n",
      ")           1\n",
      "che         1\n",
      "ha          1\n",
      "il          1\n",
      "compito     1\n",
      "di          1\n",
      "fornire     1\n",
      "“           1\n",
      "supporto    1\n",
      "scientifico  1\n",
      "al          1\n",
      "soggetto    1\n",
      "attuato     1\n",
      "##re        1\n",
      "ai          1\n",
      "fini        1\n",
      "dell        1\n",
      "’           1\n",
      "adozione    1\n",
      "delle       1\n",
      "sue         1\n",
      "determina   1\n",
      "##zioni     1\n",
      "”           1\n",
      ".           1\n",
      "unita       1\n",
      "di          1\n",
      "crisi       1\n",
      "e           1\n",
      "task        1\n",
      "force       1\n",
      "sono        1\n",
      "organi      1\n",
      "regionali   1\n",
      "costituiti  1\n",
      "a           1\n",
      "seguito     1\n",
      "della       1\n",
      "crisi       1\n",
      "del         1\n",
      "corona      1\n",
      "##virus     1\n",
      "per         1\n",
      "governare   1\n",
      "il          1\n",
      "sistema     1\n",
      "di          1\n",
      "risposta    1\n",
      "all         1\n",
      "’           1\n",
      "emergenza   1\n",
      ".           1\n",
      "l           1\n",
      "’           1\n",
      "unita       1\n",
      "di          1\n",
      "crisi       1\n",
      ",           1\n",
      "in          1\n",
      "ogni        1\n",
      "regione     1\n",
      ",           1\n",
      "e           1\n",
      "il          1\n",
      "livello     1\n",
      "regionale   1\n",
      "della       1\n",
      "catena      1\n",
      "di          1\n",
      "comando     1\n",
      "stabilita   1\n",
      "dalla       1\n",
      "protezione  1\n",
      "civile      1\n",
      "nazionale   1\n",
      "con         1\n",
      "disposizione  1\n",
      "del         1\n",
      "4           1\n",
      "marzo       1\n",
      "2020        1\n",
      ".           1\n",
      "ogni        1\n",
      "regione     1\n",
      "ne          1\n",
      "disciplina  1\n",
      "la          1\n",
      "composizione  1\n",
      "con         1\n",
      "un          1\n",
      "proprio     1\n",
      "atto        1\n",
      ".           1\n",
      "task        1\n",
      "force       1\n",
      ",           1\n",
      "comitati    1\n",
      "scientifici  1\n",
      "e           1\n",
      "altri       1\n",
      "organi      1\n",
      "vari        1\n",
      "sono        1\n",
      "invece      1\n",
      "costituiti  1\n",
      "su          1\n",
      "iniziativa  1\n",
      "della       1\n",
      "regione     1\n",
      "per         1\n",
      "contribuire  1\n",
      "alla        1\n",
      "gestione    1\n",
      "dell        1\n",
      "’           1\n",
      "emergenza   1\n",
      ".           0\n",
      "[SEP]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n"
     ]
    }
   ],
   "source": [
    "for token, label in zip(test_dataset.tokenizer.convert_ids_to_tokens(test_dataset[197][\"input_ids\"]), test_dataset[197][\"labels\"]):\n",
    "  print('{0:10}  {1}'.format(token, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-base-italian-xxl-uncased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at dbmdz/bert-base-italian-xxl-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_name, num_labels=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "# Deterministic mode\n",
    "def seed_everything(seed=1464):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "\n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n",
    "\n",
    "def plot_loss(loss, val_loss):\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    plt.xticks(range(1, len(loss)+1))\n",
    "    plt.plot(range(1, len(loss)+1), loss, label='train')\n",
    "    plt.plot(range(1, len(val_loss)+1), val_loss, label='val')\n",
    "    plt.title('loss')\n",
    "    plt.legend()\n",
    "    # plt.savefig('loss.png')\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_f1(f1, val_f1):\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    plt.xticks(range(1, len(f1)+1))\n",
    "    plt.plot(range(1, len(f1)+1), f1, label='train')\n",
    "    plt.plot(range(1, len(val_f1)+1), val_f1, label='val')\n",
    "    plt.title('f1')\n",
    "    plt.legend()\n",
    "    # plt.savefig('f1.png')\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(y_true, pred, labels):\n",
    "    fig, ax = plt.subplots(figsize=(20, 20))\n",
    "    disp = ConfusionMatrixDisplay.from_predictions(\n",
    "        y_true, pred, display_labels=labels, normalize='true', values_format='.2f')\n",
    "    disp.plot(cmap=\"Blues\", values_format='.2g',\n",
    "              xticks_rotation='vertical', ax=ax)\n",
    "    return disp.figure_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neptune.new as neptune\n",
    "\n",
    "\n",
    "class NeptuneLogger():\n",
    "    def __init__(self) -> None:\n",
    "        # Neptune initialization\n",
    "        self.run = neptune.init(\n",
    "            project=\"mibo8/Rep\",\n",
    "            api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJmZmRkYThiZi1mZGNlLTRlMTktODQwNS1hNWFlMWQ2Mjc4N2IifQ==\",\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "from torch.nn import utils\n",
    "\n",
    "import torchmetrics\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW\n",
    "from transformers import get_constant_schedule_with_warmup\n",
    "\n",
    "\n",
    "import neptune.new as neptune\n",
    "\n",
    "\n",
    "class IE_MPTrainer():\n",
    "    def __init__(self, batch_size, lr, n_epochs) -> None:\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = lr\n",
    "        self.n_epochs = n_epochs\n",
    "\n",
    "        self.logger = NeptuneLogger()\n",
    "\n",
    "    def fit(self, model, train_dataset, val_dataset):\n",
    "        self.logger.run['model'] = model_name\n",
    "\n",
    "        params_info = {\n",
    "            'learning_rate': self.learning_rate,\n",
    "            'batch_size': self.batch_size,\n",
    "            'n_epochs': self.n_epochs\n",
    "        }\n",
    "        #self.logger.run['params'] = params_info\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        # ----------TRAINING\n",
    "\n",
    "        # Measure the total training time for the whole run.\n",
    "        total_t0 = time.time()\n",
    "\n",
    "        epochs_train_loss = []\n",
    "        epochs_val_loss = []\n",
    "\n",
    "        epochs = self.n_epochs\n",
    "\n",
    "        # Creation of Pytorch DataLoaders with shuffle=True for the traing phase\n",
    "        train_dataloader = DataLoader(\n",
    "            train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        validation_dataloader = DataLoader(\n",
    "            val_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "        # Adam algorithm optimized for tranfor architectures\n",
    "        optimizer = AdamW(model.parameters(), lr=self.learning_rate)\n",
    "        #scheduler = get_constant_schedule_with_warmup(optimizer, num_warmup_steps=300)\n",
    "\n",
    "        # Scaler for mixed precision\n",
    "        scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "        # Setup for training with gpu\n",
    "        device = torch.device(\n",
    "            'cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "        model.to(device)\n",
    "\n",
    "        # For each epoch...\n",
    "        for epoch_i in range(0, epochs):\n",
    "\n",
    "            # ========================================\n",
    "            #               Training\n",
    "            # ========================================\n",
    "\n",
    "            # Perform one full pass over the training set.\n",
    "\n",
    "            print(\"\")\n",
    "            print(\n",
    "                '======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "            print('Training...')\n",
    "\n",
    "            # Measure how long the training epoch takes.\n",
    "            t0 = time.time()\n",
    "\n",
    "            # Reset the total loss for this epoch.\n",
    "            total_train_loss = 0\n",
    "\n",
    "            # Put the model into training mode: Dropout layers are active\n",
    "            model.train()\n",
    "\n",
    "            # For each batch of training data...\n",
    "            for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "                # Progress update every 40 batches.\n",
    "                if step % 10 == 0 and not step == 0:\n",
    "                    # Compute time in minutes.\n",
    "                    elapsed = format_time(time.time() - t0)\n",
    "\n",
    "                    # Report progress.\n",
    "                    print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(\n",
    "                        step, len(train_dataloader), elapsed))\n",
    "\n",
    "                # Unpack this training batch from the dataloader.\n",
    "                #\n",
    "                #  copy each tensor to the GPU using the 'to()' method\n",
    "                #\n",
    "                # 'batch' contains three pytorch tensors:\n",
    "                #   [0]: input ids\n",
    "                #   [1]: attention masks\n",
    "                #   [2]: labels\n",
    "                b_input_ids = batch['input_ids'].to(device)\n",
    "                b_input_mask = batch['attention_mask'].to(device)\n",
    "                b_labels = batch['labels'].to(device)\n",
    "\n",
    "                # clear any previously calculated gradients before performing a\n",
    "                # backward pass\n",
    "                model.zero_grad()\n",
    "\n",
    "                # Perform a forward pass in mixed precision\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    outputs = model(b_input_ids,\n",
    "                                    attention_mask=b_input_mask,\n",
    "                                    labels=b_labels)\n",
    "\n",
    "                    loss = outputs[0]\n",
    "                    logits = outputs[1]\n",
    "\n",
    "                # Move logits and labels to CPU\n",
    "                logits = logits.detach().cpu()\n",
    "                label_ids = b_labels.to('cpu')\n",
    "\n",
    "                # Perform a backward pass to compute the gradients in MIXED precision\n",
    "                scaler.scale(loss).backward()\n",
    "\n",
    "                # Accumulate the training loss over all of the batches so that we can\n",
    "                # calculate the average loss at the end.\n",
    "                total_train_loss += loss.item()\n",
    "\n",
    "                # Unscales the gradients of optimizer's assigned params in-place before the gradient clipping\n",
    "                scaler.unscale_(optimizer)\n",
    "\n",
    "                # Clip the norm of the gradients to 1.0.\n",
    "                # This helps and prevent the \"exploding gradients\" problem.\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "                # Update parameters and take a step using the computed gradient in MIXED precision\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                # scheduler.step()\n",
    "\n",
    "            # Compute the average loss over all of the batches.\n",
    "            avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "            epochs_train_loss.append(avg_train_loss)\n",
    "\n",
    "            # Measure how long this epoch took.\n",
    "            training_time = format_time(time.time() - t0)\n",
    "\n",
    "            print(\"\")\n",
    "            print(\"  Average training loss: {0:.3f}\".format(avg_train_loss))\n",
    "            print(\"  Training epoch took: {:}\".format(training_time))\n",
    "\n",
    "            # ========================================\n",
    "            #               Validation\n",
    "            # ========================================\n",
    "            # After the completion of each training epoch, measure performance on\n",
    "            # the validation set.\n",
    "\n",
    "            print(\"\")\n",
    "            print(\"Running Validation...\")\n",
    "\n",
    "            t0 = time.time()\n",
    "\n",
    "            # Put the model in evaluation mode: the dropout layers behave differently\n",
    "            model.eval()\n",
    "\n",
    "            total_val_loss = 0\n",
    "\n",
    "            # Evaluate data for one epoch\n",
    "            for batch in validation_dataloader:\n",
    "\n",
    "                # Unpack this training batch from our dataloader.\n",
    "                #\n",
    "                # copy each tensor to the GPU using the 'to()' method\n",
    "                #\n",
    "                # 'batch' contains three pytorch tensors:\n",
    "                #   [0]: input ids\n",
    "                #   [1]: attention masks\n",
    "                #   [2]: labels\n",
    "                b_input_ids = batch['input_ids'].to(device)\n",
    "                b_input_mask = batch['attention_mask'].to(device)\n",
    "                b_labels = batch['labels'].to(device)\n",
    "\n",
    "                # Tell pytorch not to bother with constructing the compute graph during\n",
    "                # the forward pass, since this is only needed for training.\n",
    "                with torch.no_grad():\n",
    "\n",
    "                    # Forward pass, calculate logits\n",
    "                    # argmax(logits) = argmax(Softmax(logits))\n",
    "                    outputs = model(b_input_ids,\n",
    "                                    attention_mask=b_input_mask,\n",
    "                                    labels=b_labels)\n",
    "                    loss = outputs[0]\n",
    "                    logits = outputs[1]\n",
    "\n",
    "                # Accumulate the validation loss.\n",
    "                total_val_loss += loss.item()\n",
    "\n",
    "                # Move logits and labels to CPU\n",
    "                logits = logits.detach().cpu()\n",
    "                label_ids = b_labels.to('cpu')\n",
    "\n",
    "            print('VALIDATION: ')\n",
    "\n",
    "            # Compute the average loss over all of the batches.\n",
    "            avg_val_loss = total_val_loss / len(validation_dataloader)\n",
    "            epochs_val_loss.append(avg_val_loss)\n",
    "\n",
    "            # Measure how long the validation run took.\n",
    "            validation_time = format_time(time.time() - t0)\n",
    "\n",
    "            print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "            print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "        loss_fig = plot_loss(epochs_train_loss, epochs_val_loss)\n",
    "\n",
    "        self.logger.run[\"loss\"].upload(neptune.types.File.as_image(loss_fig))\n",
    "        print(\"\")\n",
    "        print(\"Training complete!\")\n",
    "\n",
    "        print(\"Total training took {:} (h:mm:ss)\".format(\n",
    "            format_time(time.time()-total_t0)))\n",
    "\n",
    "    def test(self, model, test_dataset):\n",
    "        # ========================================\n",
    "        #               Test\n",
    "        # ========================================\n",
    "        test_dataloader = DataLoader(\n",
    "            test_dataset, batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "        # Setup for testing with gpu\n",
    "        device = torch.device(\n",
    "            'cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "        print(\"\")\n",
    "        print(\"Running Test...\")\n",
    "        t0 = time.time()\n",
    "\n",
    "        # Save prediction for confusion matrix\n",
    "        pred = []\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        total_test_loss = 0\n",
    "\n",
    "        # Evaluate data for one epoch\n",
    "        for batch in test_dataloader:\n",
    "            b_input_ids = batch['input_ids'].to(device)\n",
    "            b_input_mask = batch['attention_mask'].to(device)\n",
    "            b_labels = batch['labels'].to(device)\n",
    "            b_special_tokens_mask = batch['special_tokens_mask'].to(device)\n",
    "            with torch.no_grad():\n",
    "\n",
    "                # Forward pass, calculate logits\n",
    "                # argmax(logits) = argmax(Softmax(logits))\n",
    "                outputs = model(b_input_ids,\n",
    "                                attention_mask=b_input_mask,\n",
    "                                labels=b_labels)\n",
    "                loss = outputs[0]\n",
    "                logits = outputs[1]\n",
    "\n",
    "            # Accumulate the test loss.\n",
    "            total_test_loss += loss.item()\n",
    "\n",
    "            # Move logits and labels to CPU\n",
    "            logits = logits.detach().cpu()  # shape (batch_size, seq_len, num_labels)\n",
    "            label_ids = b_labels.to('cpu')\n",
    "\n",
    "            batch_pred = logits.softmax(dim=-1)\n",
    "            full_pred = batch_pred.argmax(dim=-1)\n",
    "\n",
    "            for i, sample_pred in enumerate(full_pred):\n",
    "                active_pred = []\n",
    "                for j, e in enumerate(b_special_tokens_mask[i]):\n",
    "                    if(e == 0):\n",
    "                        active_pred.append(int(sample_pred[j]))\n",
    "                pred.append(active_pred)\n",
    "\n",
    "        avg_test_loss = total_test_loss / len(test_dataloader)\n",
    "        #self.logger.run['test/loss'] = avg_test_loss\n",
    "        test_time = format_time(time.time() - t0)\n",
    "\n",
    "        print(\"  Test Loss: {0:.2f}\".format(avg_test_loss))\n",
    "        print(\"  Test took: {:}\".format(test_time))\n",
    "\n",
    "        return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "https://app.neptune.ai/mibo8/Rep/e/REP-215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Info (NVML): NVML Shared Library Not Found. GPU usage metrics may not be reported. For more information, see https://docs.neptune.ai/you-should-know/what-can-you-log-and-display#hardware-consumption\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remember to stop your run once you’ve finished logging your metadata (https://docs.neptune.ai/api-reference/run#.stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 1e-5\n",
    "batch_size = 2\n",
    "n_epochs = 1\n",
    "trainer = IE_MPTrainer(batch_size, learning_rate, n_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer.fit(model, train_dataset, val_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running Test...\n",
      "  Test Loss: 0.68\n",
      "  Test took: 0:02:29\n"
     ]
    }
   ],
   "source": [
    "pred = trainer.test(model, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_to_bounds(pred:list) -> list:\n",
    "    bounds = []\n",
    "    start = 0\n",
    "    end = 0\n",
    "    for i,e in enumerate(pred):\n",
    "        if e == 0:\n",
    "            end = i\n",
    "            bounds.append((start, end))\n",
    "            start = end + 1\n",
    "    return bounds\n",
    "\n",
    "\n",
    "bert_preds = []\n",
    "for e in pred:\n",
    "    bert_preds.append(prediction_to_bounds(e))\n",
    "\n",
    "gt_bounds = []\n",
    "for e in test_dataset:\n",
    "    gt_bounds.append(prediction_to_bounds(e['labels']))\n",
    "test_dataset.df['Token_bounds'] = gt_bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for i in range(len(test_dataset.df.index)):\n",
    "    if len(test_dataset.df.iloc[i]['Token_bounds']) != len(test_dataset.df.iloc[i]['Repertori']):\n",
    "        counter += 1\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dichiarazione di intenti', 'sancire', 'giustificazione', 'sancire', 'commento', 'giudizio']\n",
      "[(0, 5), (6, 16), (17, 29), (30, 48), (49, 57), (58, 60)]\n"
     ]
    }
   ],
   "source": [
    "print(test_dataset.df.iloc[0]['Repertori'])\n",
    "print(test_dataset.df.iloc[0]['Token_bounds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# A è B sono tupe con i bound dello span\n",
    "def IoU(A, B):\n",
    "    '''\n",
    "    Given two intervals, find the intersection over union between them.\n",
    "    \n",
    "    :param A: The first bounding box\n",
    "    :param B: The bounding box\n",
    "    :return: the intersection over union of the two bounding boxes.\n",
    "    '''\n",
    "    if A == B:\n",
    "        return 1\n",
    "    start = max(A[0], B[0])\n",
    "    end = min(A[1], B[1])\n",
    "    if(start > end):\n",
    "        return 0\n",
    "    intersection = end - start\n",
    "    return intersection / (A[1] - A[0] + B[1] - B[0] - intersection)\n",
    "\n",
    "def compute_IoUs(pred_bounds, gt_spans):\n",
    "    '''\n",
    "    Given a list of predicted spans and a list of ground truth spans, \n",
    "    compute the intersection over union for each pair of spans\n",
    "    \n",
    "    :param pred_bounds: a tuple of (start, end) denoting the predicted answer\n",
    "    :param gt_spans: a list of tuples of the form (start, end) representing the spans of each ground\n",
    "    truth annotation\n",
    "    :return: a list of IoUs for each ground truth span.\n",
    "    '''\n",
    "    IoUs = []\n",
    "    for gt_bounds in gt_spans:\n",
    "        IoUs.append(IoU(pred_bounds, gt_bounds)) \n",
    "    return IoUs\n",
    "\n",
    "def normalize(text_spans_dict, gt_spans):\n",
    "    normalized = []\n",
    "    for i in range(len(text_spans_dict)):\n",
    "        #normalized is not empty\n",
    "        if normalized:\n",
    "            if normalized[-1]['Repertorio'] == text_spans_dict[i]['Repertorio']:\n",
    "                new_span = (normalized[-1]['Bounds'][0], text_spans_dict[i]['Bounds'][1])\n",
    "                new_span_features = {\n",
    "                    'Bounds' : new_span, \n",
    "                    'IoU' : None,\n",
    "                    'Repertorio' : text_spans_dict[i]['Repertorio']\n",
    "                    }\n",
    "                del normalized[-1]\n",
    "                normalized.append(new_span_features)\n",
    "            else:\n",
    "                normalized.append(text_spans_dict[i])\n",
    "        else:\n",
    "            normalized.append(text_spans_dict[i])\n",
    "        \n",
    "    \n",
    "    for i in range(len(normalized)):\n",
    "        normalized[i]['IoU'] = max(compute_IoUs(normalized[i]['Bounds'], gt_spans['Token_bounds']))\n",
    "    return normalized\n",
    "    \n",
    "\n",
    "metrics = []\n",
    "normalized_metrics = []\n",
    "for i, pred_bounds in enumerate(bert_preds):\n",
    "    text_IoUs = []\n",
    "    for pred_span in pred_bounds:\n",
    "        IoUs = compute_IoUs(pred_span, test_dataset.df.iloc[i]['Token_bounds'])   \n",
    "        best = np.argmax(IoUs)\n",
    "        span_features = {\n",
    "            'Bounds' : pred_span, \n",
    "            'IoU' : IoUs[best],\n",
    "            'Repertorio' : test_dataset.df.iloc[i]['Repertori'][best]\n",
    "            }\n",
    "\n",
    "        text_IoUs.append(span_features)\n",
    "    metrics.append(text_IoUs)\n",
    "    normalized_metrics.append(normalize(text_IoUs, test_dataset.df.iloc[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------\n",
      "Risultati labels GT e stralci non uniti\n",
      "Numero stralci nel dataset: 647\n",
      "Numero stralci predetti: 8806\n",
      "Numero stralci con lunghezza minima =  0 :  8806\n",
      "Media IoU: 0.039310584196279676\n",
      "Percentuale span perfetti:  0.00034067681126504656\n"
     ]
    }
   ],
   "source": [
    "print('----------------------------------------------------------')\n",
    "print('Risultati labels GT e stralci non uniti')\n",
    "\n",
    "\n",
    "n_spans = 0\n",
    "for e in test_dataset.df['Token_bounds']:\n",
    "    n_spans += len(e)\n",
    "print('Numero stralci nel dataset:', str(n_spans))\n",
    "\n",
    "n_spans = 0\n",
    "for e in metrics:\n",
    "    n_spans += len(e)\n",
    "print('Numero stralci predetti:', str(n_spans))\n",
    "\n",
    "mean = 0\n",
    "long_spans = 0\n",
    "min_lenght = 0\n",
    "perfect_spans = 0\n",
    "for text in metrics:\n",
    "    for span in text:\n",
    "        if span['Bounds'][1] - span['Bounds'][0] >= min_lenght:\n",
    "            long_spans += 1\n",
    "            mean += span['IoU']\n",
    "            if span['IoU'] == 1:\n",
    "                perfect_spans += 1\n",
    "perfect_spans_perc = perfect_spans / long_spans\n",
    "mean_IoU = mean / long_spans\n",
    "print('Numero stralci con lunghezza minima = ',\n",
    "      str(min_lenght), ': ', str(long_spans))\n",
    "print('Media IoU:', str(mean_IoU))\n",
    "print('Percentuale span perfetti: ', str(perfect_spans_perc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------\n",
      "Risultati labels GT e stralci uniti\n",
      "Numero stralci nel dataset: 647\n",
      "Numero stralci predetti: 2116\n",
      "Numero stralci con lunghezza minima =  0 :  2116\n",
      "Media IoU: 0.23227367464258894\n",
      "Percentuale span perfetti:  0.02835538752362949\n"
     ]
    }
   ],
   "source": [
    "print('----------------------------------------------------------')\n",
    "print('Risultati labels GT e stralci uniti')\n",
    "\n",
    "\n",
    "n_spans = 0\n",
    "for e in test_df['Token_bounds']:\n",
    "    n_spans += len(e)\n",
    "print('Numero stralci nel dataset:', str(n_spans))\n",
    "\n",
    "n_spans = 0\n",
    "for e in normalized_metrics:\n",
    "    n_spans += len(e)\n",
    "print('Numero stralci predetti:', str(n_spans))\n",
    "\n",
    "mean = 0\n",
    "long_spans = 0\n",
    "min_lenght = 0\n",
    "perfect_spans = 0\n",
    "for text in normalized_metrics:\n",
    "    for span in text:\n",
    "        if span['Bounds'][1] - span['Bounds'][0] >= min_lenght:\n",
    "            long_spans += 1\n",
    "            mean += span['IoU']\n",
    "            if span['IoU'] == 1:\n",
    "                perfect_spans += 1\n",
    "perfect_spans_perc = perfect_spans / long_spans\n",
    "mean_IoU = mean / long_spans\n",
    "print('Numero stralci con lunghezza minima = ',\n",
    "      str(min_lenght), ': ', str(long_spans))\n",
    "print('Media IoU:', str(mean_IoU))\n",
    "print('Percentuale span perfetti: ', str(perfect_spans_perc))\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "531bf1ddd0b9ee64f0e2ebd6527c520de4e2159f2ffcc26ba5e4b06431092b93"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
