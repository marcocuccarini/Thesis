{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/studio-lab-user/.conda/envs/default/bin/python\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.is_available()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('../data/Original_csv/Hyperion.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def find_char_bounds(spans: list, text: str) -> list:\n",
    "    '''\n",
    "    Given a list of spans and a text, find the start and end indices of each span in the text.\n",
    "    Indeces are computed counting chars\n",
    "    \n",
    "    :param spans: a list of strings to search for\n",
    "    :type spans: list\n",
    "    :param text: the text to search\n",
    "    :type text: str\n",
    "    :return: A list of tuples, where each tuple contains the start and end index of a span.\n",
    "    '''\n",
    "    bounds = []\n",
    "    last_char = 0\n",
    "    for span in spans:\n",
    "        start = text.find(span)\n",
    "        if start == -1:\n",
    "            start = last_char + 1\n",
    "        bounds.append((start, start + len(span)))\n",
    "        last_char = start + len(span)-1\n",
    "    return bounds\n",
    "\n",
    "'''\n",
    "Given a list of spans and a text, find the start and end indices of each span in the text.\n",
    "Indeces are computed counting WORDS.\n",
    "\n",
    ":param spans: a list of strings, each string is a span of text\n",
    ":type spans: list\n",
    ":param text: the text to be searched\n",
    ":type text: str\n",
    ":return: A list of tuples, where each tuple is the start and end index of a word in the text.\n",
    "'''\n",
    "def find_word_bounds(spans: list, text: str) -> list:\n",
    "    \n",
    "    bounds = []\n",
    "    end = 0\n",
    "    for span in spans:\n",
    "        s = span.translate(str.maketrans('', '', string.punctuation))\n",
    "        word_list = s.split()\n",
    "        text_list = text.translate(str.maketrans('', '', string.punctuation)).split()\n",
    "        try:\n",
    "            start = text_list.index(word_list[0], end)\n",
    "        except:\n",
    "            if not bounds:\n",
    "                start = 0\n",
    "            else:\n",
    "                \n",
    "                start = bounds[-1][1] + 1\n",
    "        end = start + len(word_list) - 1\n",
    "            \n",
    "        bounds.append((start, end))\n",
    "    return bounds\n",
    "\n",
    "def find_segmentation(bounds: list, text: str) -> str:\n",
    "    segmentation = ['0' for i in range(len(text))]\n",
    "    for bound in bounds:\n",
    "        if bound[1] < len(text):\n",
    "            segmentation[bound[1]] = '1'\n",
    "        else:\n",
    "            segmentation[-1] = '1'\n",
    "    return ''.join(segmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import re\n",
    "\n",
    "def clean_text(text:str) -> str:\n",
    "    #delete double punctuation\n",
    "    text =  re.sub(r'[\\?\\.\\!]+(?=[\\?\\.\\!])', '', text)\n",
    "    # add space between a word and punctuation\n",
    "    text = re.sub('(?<! )(?=[.,!?()])|(?<=[.,!?()])(?! )', r' ', text)\n",
    "    return text\n",
    "\n",
    "dataset = []\n",
    "\n",
    "for row in df.itertuples():\n",
    "    text = row.Testo\n",
    "    \n",
    "    if pd.isna(text):\n",
    "        sample['Stralci'].append(clean_text(row.Stralcio))\n",
    "        sample['Repertori'].append(row.Repertorio)\n",
    "\n",
    "    else:\n",
    "        sample = {}\n",
    "        sample['Testo'] = clean_text(text)\n",
    "        sample['Stralci'] = [clean_text(row.Stralcio)]\n",
    "\n",
    "        sample['Repertori'] = [row.Repertorio]\n",
    "        dataset.append(sample)\n",
    "\n",
    "\n",
    "for i,sample in enumerate(dataset):\n",
    "    sample['Bounds'] = find_char_bounds(sample['Stralci'], sample['Testo'])\n",
    "    sample['Word_bounds'] = find_word_bounds(sample['Stralci'], sample['Testo'])\n",
    "    sample['Segmentation'] = find_segmentation(sample['Bounds'], sample['Testo'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "IE_dict = {\n",
    "    'Testo': [sample['Testo'] for sample in dataset],\n",
    "    'Segmentation': [sample['Segmentation'] for sample in dataset],\n",
    "    'Bounds': [sample['Bounds'] for sample in dataset],\n",
    "    'Word_bounds': [sample['Word_bounds'] for sample in dataset],\n",
    "    'Repertori': [sample['Repertori'] for sample in dataset],\n",
    "    'Stralci': [sample['Stralci'] for sample in dataset]\n",
    "}\n",
    "IE_df = pd.DataFrame(IE_dict)\n",
    "#IE_df = IE_df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "class IE_Hyperion_dataset(Dataset):\n",
    "    def __init__(self, df, tokenizer_name):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "        self.df = df\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.df['Testo'].iloc[idx]\n",
    "        encoding = self.tokenizer(text,\n",
    "                                  # is_pretokenized=True,\n",
    "                                  return_special_tokens_mask=True,\n",
    "                                  return_offsets_mapping=True,\n",
    "                                  add_special_tokens=True,\n",
    "                                  return_attention_mask=True,\n",
    "                                  padding='max_length',\n",
    "                                  truncation=True,\n",
    "                                  )\n",
    "        char_labels = list(self.df['Segmentation'].iloc[idx])\n",
    "        ends = [i for i in range(len(char_labels)) if char_labels[i] == '1']\n",
    "\n",
    "        last_token_idx = max(index for index, item in enumerate(encoding['special_tokens_mask']) if item == 0)\n",
    "\n",
    "        encoded_labels = np.ones(len(encoding['input_ids']), dtype=int) * -100\n",
    "        x = ends.pop(0)\n",
    "        for i,e in enumerate(encoding['offset_mapping']):\n",
    "            if e[1] != 0:\n",
    "                # overwrite label\n",
    "                if x >= e[0] and x <= e[1]:# Doubt if insert < e[1] because of offset mapping composition\n",
    "                    encoded_labels[i] = 1\n",
    "                    if ends: \n",
    "                        x = ends.pop(0)\n",
    "                    else:\n",
    "                        x = -1\n",
    "                else:\n",
    "                    encoded_labels[i] = 0\n",
    "\n",
    "        encoded_labels[last_token_idx] = 1\n",
    "\n",
    "\n",
    "        item = {key: torch.as_tensor(val) for key, val in encoding.items()}\n",
    "        item['labels'] = torch.as_tensor(encoded_labels)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL Dataset: (15332, 6)\n",
      "TRAIN Dataset: (9813, 6)\n",
      "VALIDATION Dataset: (2453, 6)\n",
      "TEST Dataset: (3066, 6)\n"
     ]
    }
   ],
   "source": [
    "model_name = \"dbmdz/bert-base-italian-xxl-uncased\"\n",
    "train_size = 0.8\n",
    "train_df = IE_df.sample(frac=train_size, random_state=200)\n",
    "test_df = IE_df.drop(train_df.index).reset_index(drop=True)\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "val_size = 0.2\n",
    "val_df = train_df.sample(frac=val_size, random_state=200)\n",
    "train_df = train_df.drop(val_df.index).reset_index(drop=True)\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "\n",
    "print(\"FULL Dataset: {}\".format(IE_df.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_df.shape))\n",
    "print(\"VALIDATION Dataset: {}\".format(val_df.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_df.shape))\n",
    "\n",
    "train_dataset = IE_Hyperion_dataset(train_df, model_name)\n",
    "val_dataset = IE_Hyperion_dataset(val_df, model_name)\n",
    "test_dataset = IE_Hyperion_dataset(test_df, model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]       -100\n",
      "corona      0\n",
      "##virus     0\n",
      "/           0\n",
      "dalla       0\n",
      "regione     0\n",
      "veneto      0\n",
      "altri       0\n",
      "12          0\n",
      "milioni     0\n",
      "di          0\n",
      "euro        0\n",
      "per         0\n",
      "sostegno    0\n",
      "a           0\n",
      "famiglie    0\n",
      "e           0\n",
      "lavoratori  0\n",
      ".           1\n",
      "stiamo      0\n",
      "proseguendo  0\n",
      "il          0\n",
      "lavoro      0\n",
      "di          0\n",
      "individuare  0\n",
      "e           0\n",
      "liberare    0\n",
      "fondi       0\n",
      "regionali   0\n",
      "da          0\n",
      "destinare   0\n",
      "ad          0\n",
      "interventi  0\n",
      "a           0\n",
      "sostegno    0\n",
      "di          0\n",
      "famiglie    0\n",
      "e           0\n",
      "tessuto     0\n",
      "economico   0\n",
      "a           0\n",
      "fronte      0\n",
      "dell        0\n",
      "’           0\n",
      "emergenza   0\n",
      "sanitaria   0\n",
      "in          0\n",
      "corso       0\n",
      ".           1\n",
      "ieri        0\n",
      "la          0\n",
      "giunta      0\n",
      "del         0\n",
      "veneto      0\n",
      "ha          0\n",
      "presentato  0\n",
      "in          0\n",
      "consiglio   0\n",
      "un          0\n",
      "emendamento  0\n",
      "alla        0\n",
      "variazione  0\n",
      "di          0\n",
      "bilancio    0\n",
      "che         0\n",
      ",           0\n",
      "aggiungendo  0\n",
      "12          0\n",
      "milioni     0\n",
      "ai          0\n",
      "5           0\n",
      "gia         0\n",
      "preventiva  0\n",
      "##ti        0\n",
      ",           0\n",
      "consenti    0\n",
      "##ra        0\n",
      "di          0\n",
      "raggiungere  0\n",
      "la          0\n",
      "somma       0\n",
      "di          0\n",
      "oltre       0\n",
      "18          0\n",
      "milioni     0\n",
      "di          0\n",
      "euro        0\n",
      ".           0\n",
      "le          0\n",
      "somme       0\n",
      "individuate  0\n",
      "andranno    0\n",
      "destinate   0\n",
      "per         0\n",
      "il          0\n",
      "sostegno    0\n",
      "concreto    0\n",
      "alle        0\n",
      "famiglie    0\n",
      "ed          0\n",
      "ai          0\n",
      "lavoratori  0\n",
      ",           0\n",
      "senza       0\n",
      "dimenticare  0\n",
      "i           0\n",
      "servizi     0\n",
      "assiste     0\n",
      "##n         0\n",
      "##ziali     0\n",
      "agli        0\n",
      "anziani     0\n",
      "e           0\n",
      "le          0\n",
      "infrastrutture  0\n",
      "perche      0\n",
      "stiamo      0\n",
      "pensando    0\n",
      "al          0\n",
      "superamento  0\n",
      "di          0\n",
      "questa      0\n",
      "situazione  0\n",
      "ma          0\n",
      "anche       0\n",
      "alla        0\n",
      "ripar       0\n",
      "##tenza     0\n",
      ":           1\n",
      "[SEP]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n"
     ]
    }
   ],
   "source": [
    "for token, label in zip(test_dataset.tokenizer.convert_ids_to_tokens(test_dataset[10][\"input_ids\"]), test_dataset[10][\"labels\"]):\n",
    "  print('{0:10}  {1}'.format(token, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-base-italian-xxl-uncased were not used when initializing BertForTokenClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at dbmdz/bert-base-italian-xxl-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_name, num_labels=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "# Deterministic mode\n",
    "def seed_everything(seed=1464):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "\n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n",
    "\n",
    "def plot_loss(loss, val_loss):\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    plt.xticks(range(1, len(loss)+1))\n",
    "    plt.plot(range(1, len(loss)+1), loss, label='train')\n",
    "    plt.plot(range(1, len(val_loss)+1), val_loss, label='val')\n",
    "    plt.title('loss')\n",
    "    plt.legend()\n",
    "    # plt.savefig('loss.png')\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_f1(f1, val_f1):\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    plt.xticks(range(1, len(f1)+1))\n",
    "    plt.plot(range(1, len(f1)+1), f1, label='train')\n",
    "    plt.plot(range(1, len(val_f1)+1), val_f1, label='val')\n",
    "    plt.title('f1')\n",
    "    plt.legend()\n",
    "    # plt.savefig('f1.png')\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(y_true, pred, labels):\n",
    "    fig, ax = plt.subplots(figsize=(20, 20))\n",
    "    disp = ConfusionMatrixDisplay.from_predictions(\n",
    "        y_true, pred, display_labels=labels, normalize='true', values_format='.2f')\n",
    "    disp.plot(cmap=\"Blues\", values_format='.2g',\n",
    "              xticks_rotation='vertical', ax=ax)\n",
    "    return disp.figure_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neptune.new as neptune\n",
    "\n",
    "\n",
    "class NeptuneLogger():\n",
    "    def __init__(self) -> None:\n",
    "        # Neptune initialization\n",
    "        self.run = neptune.init(\n",
    "            project=\"mibo8/Rep\",\n",
    "            api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJmZmRkYThiZi1mZGNlLTRlMTktODQwNS1hNWFlMWQ2Mjc4N2IifQ==\",\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "from torch.nn import utils\n",
    "\n",
    "import torchmetrics\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW\n",
    "from transformers import get_constant_schedule_with_warmup\n",
    "\n",
    "\n",
    "import neptune.new as neptune\n",
    "\n",
    "\n",
    "class IE_MPTrainer():\n",
    "    def __init__(self, batch_size, lr, n_epochs) -> None:\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = lr\n",
    "        self.n_epochs = n_epochs\n",
    "\n",
    "        self.logger = NeptuneLogger()\n",
    "\n",
    "    def fit(self, model, train_dataset, val_dataset):\n",
    "        self.logger.run['model'] = model_name\n",
    "\n",
    "        params_info = {\n",
    "            'learning_rate': self.learning_rate,\n",
    "            'batch_size': self.batch_size,\n",
    "            'n_epochs': self.n_epochs\n",
    "        }\n",
    "        #self.logger.run['params'] = params_info\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        # ----------TRAINING\n",
    "\n",
    "        # Measure the total training time for the whole run.\n",
    "        total_t0 = time.time()\n",
    "\n",
    "        epochs_train_loss = []\n",
    "        epochs_val_loss = []\n",
    "\n",
    "        epochs = self.n_epochs\n",
    "\n",
    "        # Creation of Pytorch DataLoaders with shuffle=True for the traing phase\n",
    "        train_dataloader = DataLoader(\n",
    "            train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        validation_dataloader = DataLoader(\n",
    "            val_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "        # Adam algorithm optimized for tranfor architectures\n",
    "        optimizer = AdamW(model.parameters(), lr=self.learning_rate)\n",
    "        #scheduler = get_constant_schedule_with_warmup(optimizer, num_warmup_steps=300)\n",
    "\n",
    "        # Scaler for mixed precision\n",
    "        scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "        # Setup for training with gpu\n",
    "        device = torch.device(\n",
    "            'cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "        model.to(device)\n",
    "\n",
    "        # For each epoch...\n",
    "        for epoch_i in range(0, epochs):\n",
    "\n",
    "            # ========================================\n",
    "            #               Training\n",
    "            # ========================================\n",
    "\n",
    "            # Perform one full pass over the training set.\n",
    "\n",
    "            print(\"\")\n",
    "            print(\n",
    "                '======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "            print('Training...')\n",
    "\n",
    "            # Measure how long the training epoch takes.\n",
    "            t0 = time.time()\n",
    "\n",
    "            # Reset the total loss for this epoch.\n",
    "            total_train_loss = 0\n",
    "\n",
    "            # Put the model into training mode: Dropout layers are active\n",
    "            model.train()\n",
    "\n",
    "            # For each batch of training data...\n",
    "            for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "                # Progress update every 40 batches.\n",
    "                if step % 10 == 0 and not step == 0:\n",
    "                    # Compute time in minutes.\n",
    "                    elapsed = format_time(time.time() - t0)\n",
    "\n",
    "                    # Report progress.\n",
    "                    print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(\n",
    "                        step, len(train_dataloader), elapsed))\n",
    "\n",
    "                # Unpack this training batch from the dataloader.\n",
    "                #\n",
    "                #  copy each tensor to the GPU using the 'to()' method\n",
    "                #\n",
    "                # 'batch' contains three pytorch tensors:\n",
    "                #   [0]: input ids\n",
    "                #   [1]: attention masks\n",
    "                #   [2]: labels\n",
    "                b_input_ids = batch['input_ids'].to(device)\n",
    "                b_input_mask = batch['attention_mask'].to(device)\n",
    "                b_labels = batch['labels'].to(device)\n",
    "\n",
    "                # clear any previously calculated gradients before performing a\n",
    "                # backward pass\n",
    "                model.zero_grad()\n",
    "\n",
    "                # Perform a forward pass in mixed precision\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    outputs = model(b_input_ids,\n",
    "                                    attention_mask=b_input_mask,\n",
    "                                    labels=b_labels)\n",
    "\n",
    "                    loss = outputs[0]\n",
    "                    logits = outputs[1]\n",
    "\n",
    "                # Move logits and labels to CPU\n",
    "                logits = logits.detach().cpu()\n",
    "                label_ids = b_labels.to('cpu')\n",
    "\n",
    "                # Perform a backward pass to compute the gradients in MIXED precision\n",
    "                scaler.scale(loss).backward()\n",
    "\n",
    "                # Accumulate the training loss over all of the batches so that we can\n",
    "                # calculate the average loss at the end.\n",
    "                total_train_loss += loss.item()\n",
    "\n",
    "                # Unscales the gradients of optimizer's assigned params in-place before the gradient clipping\n",
    "                scaler.unscale_(optimizer)\n",
    "\n",
    "                # Clip the norm of the gradients to 1.0.\n",
    "                # This helps and prevent the \"exploding gradients\" problem.\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "                # Update parameters and take a step using the computed gradient in MIXED precision\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                # scheduler.step()\n",
    "\n",
    "            # Compute the average loss over all of the batches.\n",
    "            avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "            epochs_train_loss.append(avg_train_loss)\n",
    "\n",
    "            # Measure how long this epoch took.\n",
    "            training_time = format_time(time.time() - t0)\n",
    "\n",
    "            print(\"\")\n",
    "            print(\"  Average training loss: {0:.3f}\".format(avg_train_loss))\n",
    "            print(\"  Training epoch took: {:}\".format(training_time))\n",
    "\n",
    "            # ========================================\n",
    "            #               Validation\n",
    "            # ========================================\n",
    "            # After the completion of each training epoch, measure performance on\n",
    "            # the validation set.\n",
    "\n",
    "            print(\"\")\n",
    "            print(\"Running Validation...\")\n",
    "\n",
    "            t0 = time.time()\n",
    "\n",
    "            # Put the model in evaluation mode: the dropout layers behave differently\n",
    "            model.eval()\n",
    "\n",
    "            total_val_loss = 0\n",
    "\n",
    "            # Evaluate data for one epoch\n",
    "            for batch in validation_dataloader:\n",
    "\n",
    "                # Unpack this training batch from our dataloader.\n",
    "                #\n",
    "                # copy each tensor to the GPU using the 'to()' method\n",
    "                #\n",
    "                # 'batch' contains three pytorch tensors:\n",
    "                #   [0]: input ids\n",
    "                #   [1]: attention masks\n",
    "                #   [2]: labels\n",
    "                b_input_ids = batch['input_ids'].to(device)\n",
    "                b_input_mask = batch['attention_mask'].to(device)\n",
    "                b_labels = batch['labels'].to(device)\n",
    "\n",
    "                # Tell pytorch not to bother with constructing the compute graph during\n",
    "                # the forward pass, since this is only needed for training.\n",
    "                with torch.no_grad():\n",
    "\n",
    "                    # Forward pass, calculate logits\n",
    "                    # argmax(logits) = argmax(Softmax(logits))\n",
    "                    outputs = model(b_input_ids,\n",
    "                                    attention_mask=b_input_mask,\n",
    "                                    labels=b_labels)\n",
    "                    loss = outputs[0]\n",
    "                    logits = outputs[1]\n",
    "\n",
    "                # Accumulate the validation loss.\n",
    "                total_val_loss += loss.item()\n",
    "\n",
    "                # Move logits and labels to CPU\n",
    "                logits = logits.detach().cpu()\n",
    "                label_ids = b_labels.to('cpu')\n",
    "\n",
    "            print('VALIDATION: ')\n",
    "\n",
    "            # Compute the average loss over all of the batches.\n",
    "            avg_val_loss = total_val_loss / len(validation_dataloader)\n",
    "            epochs_val_loss.append(avg_val_loss)\n",
    "\n",
    "            # Measure how long the validation run took.\n",
    "            validation_time = format_time(time.time() - t0)\n",
    "\n",
    "            print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "            print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "        loss_fig = plot_loss(epochs_train_loss, epochs_val_loss)\n",
    "\n",
    "        self.logger.run[\"loss\"].upload(neptune.types.File.as_image(loss_fig))\n",
    "        print(\"\")\n",
    "        print(\"Training complete!\")\n",
    "\n",
    "        print(\"Total training took {:} (h:mm:ss)\".format(\n",
    "            format_time(time.time()-total_t0)))\n",
    "\n",
    "    def test(self, model, test_dataset):\n",
    "        # ========================================\n",
    "        #               Test\n",
    "        # ========================================\n",
    "        test_dataloader = DataLoader(\n",
    "            test_dataset, batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "        # Setup for testing with gpu\n",
    "        device = torch.device(\n",
    "            'cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "        print(\"\")\n",
    "        print(\"Running Test...\")\n",
    "        t0 = time.time()\n",
    "\n",
    "        # Save prediction for confusion matrix\n",
    "        preds = []\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        total_test_loss = 0\n",
    "\n",
    "        # Evaluate data for one epoch\n",
    "        for batch in test_dataloader:\n",
    "            b_input_ids = batch['input_ids'].to(device)\n",
    "            b_input_mask = batch['attention_mask'].to(device)\n",
    "            b_labels = batch['labels'].to(device)\n",
    "            b_special_tokens_mask = batch['special_tokens_mask'].to(device)\n",
    "            with torch.no_grad():\n",
    "\n",
    "                # Forward pass, calculate logits\n",
    "                # argmax(logits) = argmax(Softmax(logits))\n",
    "                outputs = model(b_input_ids,\n",
    "                                attention_mask=b_input_mask,\n",
    "                                labels=b_labels)\n",
    "                loss = outputs[0]\n",
    "                logits = outputs[1]\n",
    "\n",
    "            # Accumulate the test loss.\n",
    "            total_test_loss += loss.item()\n",
    "\n",
    "            # Move logits and labels to CPU\n",
    "            logits = logits.detach().cpu()  # shape (batch_size, seq_len, num_labels\n",
    "            full_probs = logits.softmax(dim=-1)\n",
    "\n",
    "            for i, sample_prob in enumerate(full_probs):\n",
    "                active_prob = []\n",
    "                for j, e in enumerate(b_special_tokens_mask[i]):\n",
    "                    if(e == 0):\n",
    "                        active_prob.append(sample_prob[j].tolist())\n",
    "                preds.append(active_prob)\n",
    "\n",
    "        avg_test_loss = total_test_loss / len(test_dataloader)\n",
    "        #self.logger.run['test/loss'] = avg_test_loss\n",
    "        test_time = format_time(time.time() - t0)\n",
    "\n",
    "        print(\"  Test Loss: {0:.2f}\".format(avg_test_loss))\n",
    "        print(\"  Test took: {:}\".format(test_time))\n",
    "\n",
    "        return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "https://app.neptune.ai/mibo8/Rep/e/REP-281\n",
      "Remember to stop your run once you’ve finished logging your metadata (https://docs.neptune.ai/api-reference/run#.stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 1e-5\n",
    "batch_size = 18\n",
    "n_epochs = 2\n",
    "trainer = IE_MPTrainer(batch_size, learning_rate, n_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 2 ========\n",
      "Training...\n",
      "  Batch    10  of    546.    Elapsed: 0:00:07.\n",
      "  Batch    20  of    546.    Elapsed: 0:00:13.\n",
      "  Batch    30  of    546.    Elapsed: 0:00:20.\n",
      "  Batch    40  of    546.    Elapsed: 0:00:26.\n",
      "  Batch    50  of    546.    Elapsed: 0:00:33.\n",
      "  Batch    60  of    546.    Elapsed: 0:00:39.\n",
      "  Batch    70  of    546.    Elapsed: 0:00:46.\n",
      "  Batch    80  of    546.    Elapsed: 0:00:52.\n",
      "  Batch    90  of    546.    Elapsed: 0:00:59.\n",
      "  Batch   100  of    546.    Elapsed: 0:01:05.\n",
      "  Batch   110  of    546.    Elapsed: 0:01:12.\n",
      "  Batch   120  of    546.    Elapsed: 0:01:18.\n",
      "  Batch   130  of    546.    Elapsed: 0:01:25.\n",
      "  Batch   140  of    546.    Elapsed: 0:01:31.\n",
      "  Batch   150  of    546.    Elapsed: 0:01:38.\n",
      "  Batch   160  of    546.    Elapsed: 0:01:44.\n",
      "  Batch   170  of    546.    Elapsed: 0:01:51.\n",
      "  Batch   180  of    546.    Elapsed: 0:01:58.\n",
      "  Batch   190  of    546.    Elapsed: 0:02:04.\n",
      "  Batch   200  of    546.    Elapsed: 0:02:11.\n",
      "  Batch   210  of    546.    Elapsed: 0:02:17.\n",
      "  Batch   220  of    546.    Elapsed: 0:02:24.\n",
      "  Batch   230  of    546.    Elapsed: 0:02:30.\n",
      "  Batch   240  of    546.    Elapsed: 0:02:37.\n",
      "  Batch   250  of    546.    Elapsed: 0:02:43.\n",
      "  Batch   260  of    546.    Elapsed: 0:02:50.\n",
      "  Batch   270  of    546.    Elapsed: 0:02:57.\n",
      "  Batch   280  of    546.    Elapsed: 0:03:03.\n",
      "  Batch   290  of    546.    Elapsed: 0:03:10.\n",
      "  Batch   300  of    546.    Elapsed: 0:03:16.\n",
      "  Batch   310  of    546.    Elapsed: 0:03:23.\n",
      "  Batch   320  of    546.    Elapsed: 0:03:30.\n",
      "  Batch   330  of    546.    Elapsed: 0:03:36.\n",
      "  Batch   340  of    546.    Elapsed: 0:03:43.\n",
      "  Batch   350  of    546.    Elapsed: 0:03:50.\n",
      "  Batch   360  of    546.    Elapsed: 0:03:57.\n",
      "  Batch   370  of    546.    Elapsed: 0:04:03.\n",
      "  Batch   380  of    546.    Elapsed: 0:04:10.\n",
      "  Batch   390  of    546.    Elapsed: 0:04:17.\n",
      "  Batch   400  of    546.    Elapsed: 0:04:24.\n",
      "  Batch   410  of    546.    Elapsed: 0:04:30.\n",
      "  Batch   420  of    546.    Elapsed: 0:04:37.\n",
      "  Batch   430  of    546.    Elapsed: 0:04:44.\n",
      "  Batch   440  of    546.    Elapsed: 0:04:51.\n",
      "  Batch   450  of    546.    Elapsed: 0:04:59.\n",
      "  Batch   460  of    546.    Elapsed: 0:05:06.\n",
      "  Batch   470  of    546.    Elapsed: 0:05:13.\n",
      "  Batch   480  of    546.    Elapsed: 0:05:20.\n",
      "  Batch   490  of    546.    Elapsed: 0:05:27.\n",
      "  Batch   500  of    546.    Elapsed: 0:05:35.\n",
      "  Batch   510  of    546.    Elapsed: 0:05:43.\n",
      "  Batch   520  of    546.    Elapsed: 0:05:50.\n",
      "  Batch   530  of    546.    Elapsed: 0:05:58.\n",
      "  Batch   540  of    546.    Elapsed: 0:06:05.\n",
      "\n",
      "  Average training loss: 0.056\n",
      "  Training epoch took: 0:06:09\n",
      "\n",
      "Running Validation...\n",
      "VALIDATION: \n",
      "  Validation Loss: 0.04\n",
      "  Validation took: 0:01:40\n",
      "\n",
      "======== Epoch 2 / 2 ========\n",
      "Training...\n",
      "  Batch    10  of    546.    Elapsed: 0:00:08.\n",
      "  Batch    20  of    546.    Elapsed: 0:00:15.\n",
      "  Batch    30  of    546.    Elapsed: 0:00:23.\n",
      "  Batch    40  of    546.    Elapsed: 0:00:30.\n",
      "  Batch    50  of    546.    Elapsed: 0:00:38.\n",
      "  Batch    60  of    546.    Elapsed: 0:00:45.\n",
      "  Batch    70  of    546.    Elapsed: 0:00:53.\n",
      "  Batch    80  of    546.    Elapsed: 0:01:00.\n",
      "  Batch    90  of    546.    Elapsed: 0:01:08.\n",
      "  Batch   100  of    546.    Elapsed: 0:01:16.\n",
      "  Batch   110  of    546.    Elapsed: 0:01:23.\n",
      "  Batch   120  of    546.    Elapsed: 0:01:31.\n",
      "  Batch   130  of    546.    Elapsed: 0:01:38.\n",
      "  Batch   140  of    546.    Elapsed: 0:01:46.\n",
      "  Batch   150  of    546.    Elapsed: 0:01:53.\n",
      "  Batch   160  of    546.    Elapsed: 0:02:01.\n",
      "  Batch   170  of    546.    Elapsed: 0:02:08.\n",
      "  Batch   180  of    546.    Elapsed: 0:02:16.\n",
      "  Batch   190  of    546.    Elapsed: 0:02:23.\n",
      "  Batch   200  of    546.    Elapsed: 0:02:30.\n",
      "  Batch   210  of    546.    Elapsed: 0:02:37.\n",
      "  Batch   220  of    546.    Elapsed: 0:02:44.\n",
      "  Batch   230  of    546.    Elapsed: 0:02:50.\n",
      "  Batch   240  of    546.    Elapsed: 0:02:57.\n",
      "  Batch   250  of    546.    Elapsed: 0:03:04.\n",
      "  Batch   260  of    546.    Elapsed: 0:03:10.\n",
      "  Batch   270  of    546.    Elapsed: 0:03:17.\n",
      "  Batch   280  of    546.    Elapsed: 0:03:24.\n",
      "  Batch   290  of    546.    Elapsed: 0:03:30.\n",
      "  Batch   300  of    546.    Elapsed: 0:03:37.\n",
      "  Batch   310  of    546.    Elapsed: 0:03:44.\n",
      "  Batch   320  of    546.    Elapsed: 0:03:50.\n",
      "  Batch   330  of    546.    Elapsed: 0:03:57.\n",
      "  Batch   340  of    546.    Elapsed: 0:04:03.\n",
      "  Batch   350  of    546.    Elapsed: 0:04:10.\n",
      "  Batch   360  of    546.    Elapsed: 0:04:17.\n",
      "  Batch   370  of    546.    Elapsed: 0:04:23.\n",
      "  Batch   380  of    546.    Elapsed: 0:04:30.\n",
      "  Batch   390  of    546.    Elapsed: 0:04:36.\n",
      "  Batch   400  of    546.    Elapsed: 0:04:43.\n",
      "  Batch   410  of    546.    Elapsed: 0:04:50.\n",
      "  Batch   420  of    546.    Elapsed: 0:04:56.\n",
      "  Batch   430  of    546.    Elapsed: 0:05:03.\n",
      "  Batch   440  of    546.    Elapsed: 0:05:10.\n",
      "  Batch   450  of    546.    Elapsed: 0:05:16.\n",
      "  Batch   460  of    546.    Elapsed: 0:05:23.\n",
      "  Batch   470  of    546.    Elapsed: 0:05:30.\n",
      "  Batch   480  of    546.    Elapsed: 0:05:36.\n",
      "  Batch   490  of    546.    Elapsed: 0:05:43.\n",
      "  Batch   500  of    546.    Elapsed: 0:05:49.\n",
      "  Batch   510  of    546.    Elapsed: 0:05:56.\n",
      "  Batch   520  of    546.    Elapsed: 0:06:03.\n",
      "  Batch   530  of    546.    Elapsed: 0:06:09.\n",
      "  Batch   540  of    546.    Elapsed: 0:06:16.\n",
      "\n",
      "  Average training loss: 0.037\n",
      "  Training epoch took: 0:06:19\n",
      "\n",
      "Running Validation...\n",
      "VALIDATION: \n",
      "  Validation Loss: 0.04\n",
      "  Validation took: 0:01:19\n",
      "\n",
      "Training complete!\n",
      "Total training took 0:15:34 (h:mm:ss)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(model, train_dataset, val_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running Test...\n",
      "  Test Loss: 0.04\n",
      "  Test took: 0:02:14\n"
     ]
    }
   ],
   "source": [
    "probs = trainer.test(model, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.9953232407569885, 0.004676723387092352],\n",
       " [0.9996240139007568, 0.0003759980609174818],\n",
       " [0.9997623562812805, 0.00023758996394462883],\n",
       " [0.9985178112983704, 0.0014821626245975494],\n",
       " [0.9998207688331604, 0.00017925549764186144],\n",
       " [0.9998719692230225, 0.00012801247066818178],\n",
       " [0.9998674392700195, 0.0001325589546468109],\n",
       " [0.9997474551200867, 0.00025254933279938996],\n",
       " [0.9994669556617737, 0.0005330339772626758],\n",
       " [0.9998395442962646, 0.0001604035060154274],\n",
       " [0.9999138116836548, 8.623889152659103e-05],\n",
       " [0.9999092817306519, 9.073099499801174e-05],\n",
       " [0.9998987913131714, 0.00010122188541572541],\n",
       " [0.9993031024932861, 0.000696889532264322],\n",
       " [0.9995790123939514, 0.00042097410187125206],\n",
       " [0.9834581613540649, 0.016541877761483192],\n",
       " [0.7750673890113831, 0.22493261098861694],\n",
       " [0.9991229176521301, 0.0008770853746682405],\n",
       " [0.9995493292808533, 0.00045067566679790616],\n",
       " [0.999046266078949, 0.0009536725119687617],\n",
       " [0.9996585845947266, 0.0003413928207010031],\n",
       " [0.9995318651199341, 0.00046810333151370287],\n",
       " [0.9998608827590942, 0.00013907026732340455],\n",
       " [0.9998406171798706, 0.00015934235125314444],\n",
       " [0.9998548030853271, 0.0001452221185900271],\n",
       " [0.999885082244873, 0.00011487189476611093],\n",
       " [0.999799907207489, 0.00020009014406241477],\n",
       " [0.9997342228889465, 0.0002658173907548189],\n",
       " [0.9998956918716431, 0.00010432826820760965],\n",
       " [0.999908447265625, 9.157771273748949e-05],\n",
       " [0.9998915195465088, 0.00010847902740351856],\n",
       " [0.9998258948326111, 0.00017411363660357893],\n",
       " [0.9955359697341919, 0.004464029334485531],\n",
       " [0.9264624714851379, 0.07353750616312027],\n",
       " [0.9984501600265503, 0.0015498558059334755],\n",
       " [0.9998770952224731, 0.00012289575533941388],\n",
       " [0.9999018907546997, 9.811905329115689e-05],\n",
       " [0.9999109506607056, 8.909078314900398e-05],\n",
       " [0.9999047517776489, 9.527163638267666e-05],\n",
       " [0.9999010562896729, 9.893022070173174e-05],\n",
       " [0.9988135099411011, 0.001186491921544075],\n",
       " [0.9997751116752625, 0.00022486766101792455],\n",
       " [0.9998723268508911, 0.00012769608292728662],\n",
       " [0.999697208404541, 0.00030286714900285006],\n",
       " [0.9993137121200562, 0.0006862430018372834],\n",
       " [0.9592634439468384, 0.040736593306064606],\n",
       " [0.5659231543540955, 0.43407684564590454],\n",
       " [0.9995669722557068, 0.0004329734656494111],\n",
       " [0.9998397827148438, 0.00016018218593671918],\n",
       " [0.9998358488082886, 0.0001640752743696794],\n",
       " [0.9981708526611328, 0.0018291684100404382],\n",
       " [0.9998277425765991, 0.000172180516528897],\n",
       " [0.9996256828308105, 0.00037426111521199346],\n",
       " [0.9997528195381165, 0.0002471388434059918],\n",
       " [0.9998816251754761, 0.00011838721547974274],\n",
       " [0.9998364448547363, 0.00016356651030946523],\n",
       " [0.9995799660682678, 0.00042008538730442524],\n",
       " [0.9951010346412659, 0.00489902775734663],\n",
       " [0.9998655319213867, 0.00013444680371321738],\n",
       " [0.9998949766159058, 0.0001049755301210098],\n",
       " [0.9999088048934937, 9.116087312577292e-05],\n",
       " [0.9998897314071655, 0.00011024430568795651],\n",
       " [0.9991583824157715, 0.0008416544878855348],\n",
       " [0.9997623562812805, 0.00023769533436279744],\n",
       " [0.9998675584793091, 0.00013239776308182627],\n",
       " [0.9997585415840149, 0.0002414645132375881],\n",
       " [0.9998065829277039, 0.0001934517058543861],\n",
       " [0.9837358593940735, 0.016264064237475395],\n",
       " [0.7285515666007996, 0.2714485228061676],\n",
       " [0.9951537847518921, 0.004846274387091398],\n",
       " [0.9997237324714661, 0.0002762711374089122],\n",
       " [0.9997811913490295, 0.0002187827049056068],\n",
       " [0.9998592138290405, 0.00014076544903218746],\n",
       " [0.9998255372047424, 0.0001745258632581681],\n",
       " [0.9998871088027954, 0.00011289023677818477],\n",
       " [0.9997156262397766, 0.0002843723341356963],\n",
       " [0.9962775111198425, 0.003722472582012415],\n",
       " [0.9995357990264893, 0.0004642092972062528],\n",
       " [0.9996980428695679, 0.0003019612340722233],\n",
       " [0.9995865225791931, 0.00041343370685353875],\n",
       " [0.9997329115867615, 0.000267044291831553],\n",
       " [0.9997716546058655, 0.00022839124721940607],\n",
       " [0.9664648175239563, 0.03353520482778549],\n",
       " [0.6655779480934143, 0.3344220817089081],\n",
       " [0.9968991279602051, 0.003100838977843523],\n",
       " [0.9996328353881836, 0.00036715331953018904],\n",
       " [0.9993082284927368, 0.0006917877472005785],\n",
       " [0.9997105002403259, 0.00028944466612301767],\n",
       " [0.9995160102844238, 0.0004840019973926246],\n",
       " [0.9957004189491272, 0.004299606662243605],\n",
       " [0.9970279335975647, 0.0029720610473304987],\n",
       " [0.9997573494911194, 0.00024267344269901514],\n",
       " [0.9997709393501282, 0.0002290467673446983],\n",
       " [0.9996064305305481, 0.0003935497079510242],\n",
       " [0.9992707371711731, 0.0007292725495062768],\n",
       " [0.9977756142616272, 0.0022244045976549387],\n",
       " [0.9783287048339844, 0.021671272814273834],\n",
       " [0.9988196492195129, 0.0011802988592535257],\n",
       " [0.9889692068099976, 0.011030765250325203],\n",
       " [0.546733558177948, 0.4532664716243744],\n",
       " [0.9935681223869324, 0.006431950721889734],\n",
       " [0.9989326596260071, 0.0010673352517187595],\n",
       " [0.997920572757721, 0.002079480793327093],\n",
       " [0.9639564156532288, 0.03604354336857796],\n",
       " [0.9870734214782715, 0.012926537543535233],\n",
       " [0.9880481958389282, 0.01195188332349062],\n",
       " [0.999262273311615, 0.0007376724388450384],\n",
       " [0.999649167060852, 0.00035082301474176347],\n",
       " [0.9998759031295776, 0.0001240886194864288],\n",
       " [0.9982831478118896, 0.001716879545710981],\n",
       " [0.9997822642326355, 0.00021776709763798863],\n",
       " [0.9995965361595154, 0.0004034844459965825],\n",
       " [0.9996843338012695, 0.00031562792719341815],\n",
       " [0.999826967716217, 0.00017300865147262812],\n",
       " [0.9996461868286133, 0.0003538402961567044],\n",
       " [0.999852180480957, 0.00014783353253733367],\n",
       " [0.9989522695541382, 0.0010476873721927404],\n",
       " [0.9997283816337585, 0.0002716579183470458],\n",
       " [0.9981783628463745, 0.0018217016477137804],\n",
       " [0.999043881893158, 0.0009560442413203418],\n",
       " [0.9852348566055298, 0.014765082858502865],\n",
       " [0.6124376654624939, 0.3875623643398285],\n",
       " [0.9991406202316284, 0.0008593770326115191],\n",
       " [0.9997276663780212, 0.0002722937206272036],\n",
       " [0.9997833371162415, 0.00021667209512088448],\n",
       " [0.9996544122695923, 0.0003455467231106013],\n",
       " [0.9987851977348328, 0.0012148561654612422],\n",
       " [0.9949295520782471, 0.005070468410849571],\n",
       " [0.9994468092918396, 0.0005531787173822522],\n",
       " [0.9998133778572083, 0.00018669637211132795],\n",
       " [0.9997681975364685, 0.00023182606673799455],\n",
       " [0.9998785257339478, 0.00012150608381489292],\n",
       " [0.999518632888794, 0.0004813701962120831],\n",
       " [0.9994246959686279, 0.0005752758588641882],\n",
       " [0.999554455280304, 0.00044548307778313756],\n",
       " [0.9907492995262146, 0.009250691160559654],\n",
       " [0.9909167885780334, 0.009083247743546963],\n",
       " [0.9995212554931641, 0.000478782836580649],\n",
       " [0.9995375871658325, 0.0004624600987881422],\n",
       " [0.9998480081558228, 0.00015200389316305518],\n",
       " [0.9996660947799683, 0.0003339081595186144],\n",
       " [0.9997996687889099, 0.00020034061162732542],\n",
       " [0.9994515776634216, 0.0005484005669131875],\n",
       " [0.9997813105583191, 0.0002187370409956202],\n",
       " [0.9996230602264404, 0.0003769696340896189],\n",
       " [0.9921769499778748, 0.007823006249964237],\n",
       " [0.9518843293190002, 0.04811573773622513],\n",
       " [0.9996932744979858, 0.0003067676443606615],\n",
       " [0.99985671043396, 0.00014324605581350625],\n",
       " [0.9998666048049927, 0.00013343172031454742],\n",
       " [0.9998741149902344, 0.0001258248812519014],\n",
       " [0.9996494054794312, 0.0003506429784465581],\n",
       " [0.9970689415931702, 0.002931094728410244],\n",
       " [0.9766953587532043, 0.023304685950279236],\n",
       " [0.9979273080825806, 0.0020727296359837055],\n",
       " [0.9988786578178406, 0.0011213216930627823],\n",
       " [0.9849794507026672, 0.015020559541881084],\n",
       " [0.9997925162315369, 0.00020748328824993223],\n",
       " [0.9998831748962402, 0.00011683508637361228],\n",
       " [0.9998037219047546, 0.0001962602837011218],\n",
       " [0.9993869066238403, 0.0006130533874966204],\n",
       " [0.9997978806495667, 0.00020214814867358655],\n",
       " [0.9994320273399353, 0.0005679522291757166],\n",
       " [0.9811660647392273, 0.018833912909030914],\n",
       " [0.6819471716880798, 0.31805282831192017],\n",
       " [0.9965952038764954, 0.0034047814551740885],\n",
       " [0.9997937083244324, 0.00020635576220229268],\n",
       " [0.9993763566017151, 0.0006236988701857626],\n",
       " [0.9998514652252197, 0.00014849229773972183],\n",
       " [0.9997013211250305, 0.0002986968320328742],\n",
       " [0.9998137354850769, 0.00018626074597705156],\n",
       " [0.999893307685852, 0.00010667688911780715],\n",
       " [0.9998936653137207, 0.00010628150630509481],\n",
       " [0.9997982382774353, 0.00020174762175884098],\n",
       " [0.9998654127120972, 0.00013455658336170018],\n",
       " [0.9998921155929565, 0.00010792764805955812],\n",
       " [0.999715268611908, 0.0002847575524356216],\n",
       " [0.999924898147583, 7.509917486459017e-05],\n",
       " [0.9994736313819885, 0.0005264279316179454],\n",
       " [0.999671220779419, 0.0003287876315880567],\n",
       " [0.9987607002258301, 0.0012393520446494222],\n",
       " [0.9761943817138672, 0.02380562387406826],\n",
       " [0.9010134339332581, 0.09898662567138672],\n",
       " [0.9992677569389343, 0.0007322580786421895],\n",
       " [0.9993867874145508, 0.0006132193957455456],\n",
       " [0.9998354911804199, 0.00016447564121335745],\n",
       " [0.9997691512107849, 0.00023088329180609435],\n",
       " [0.9995985627174377, 0.0004015053855255246],\n",
       " [0.9997604489326477, 0.0002395043266005814],\n",
       " [0.9993508458137512, 0.0006491572712548077],\n",
       " [0.9830179214477539, 0.016982126981019974],\n",
       " [0.5649312138557434, 0.4350687861442566],\n",
       " [0.9996826648712158, 0.00031739711994305253],\n",
       " [0.9998418092727661, 0.0001582218537805602],\n",
       " [0.9997437596321106, 0.00025621691020205617],\n",
       " [0.9997512698173523, 0.00024875678354874253],\n",
       " [0.9988747239112854, 0.0011252396507188678],\n",
       " [0.9984490871429443, 0.0015508802607655525],\n",
       " [0.9986464381217957, 0.0013535447651520371],\n",
       " [0.9998383522033691, 0.00016168033471331],\n",
       " [0.9998376369476318, 0.00016238780517596751],\n",
       " [0.9998050332069397, 0.0001949880097527057],\n",
       " [0.9996040463447571, 0.0003959547320846468],\n",
       " [0.9984108209609985, 0.0015891545917838812],\n",
       " [0.9949159622192383, 0.005084058735519648],\n",
       " [0.9995742440223694, 0.00042572643724270165],\n",
       " [0.9995307922363281, 0.0004691770300269127],\n",
       " [0.9997490048408508, 0.00025093878502957523],\n",
       " [0.9990231990814209, 0.0009768534218892455],\n",
       " [0.9997748732566833, 0.00022518027981277555],\n",
       " [0.9998807907104492, 0.00011925215949304402],\n",
       " [0.9998189806938171, 0.00018106504285242409],\n",
       " [0.9998576641082764, 0.00014229799853637815],\n",
       " [0.9998125433921814, 0.0001875227753771469],\n",
       " [0.9959914088249207, 0.004008562304079533],\n",
       " [0.9969711303710938, 0.003028862876817584],\n",
       " [0.9681147933006287, 0.03188517689704895],\n",
       " [0.9994290471076965, 0.0005709046963602304],\n",
       " [0.9996664524078369, 0.00033354543847963214],\n",
       " [0.9996373653411865, 0.00036261166678741574],\n",
       " [0.9997612833976746, 0.00023876698105596006],\n",
       " [0.999846339225769, 0.00015362420526798815],\n",
       " [0.9998703002929688, 0.0001296480040764436],\n",
       " [0.9998730421066284, 0.00012690451694652438],\n",
       " [0.999651312828064, 0.000348632805980742],\n",
       " [0.9995875954627991, 0.0004124354454688728],\n",
       " [0.9998558759689331, 0.00014410752919502556],\n",
       " [0.9998012185096741, 0.0001987871655728668],\n",
       " [0.9999127388000488, 8.722740312805399e-05],\n",
       " [0.9992496371269226, 0.0007503167726099491],\n",
       " [0.9992477893829346, 0.0007522731903009117],\n",
       " [0.9998292922973633, 0.00017067980661522597],\n",
       " [0.9998643398284912, 0.00013562626554630697],\n",
       " [0.9998738765716553, 0.00012605015945155174],\n",
       " [0.999864935874939, 0.00013504349044524133],\n",
       " [0.9997774958610535, 0.00022250819893088192],\n",
       " [0.9996150732040405, 0.0003849258355330676],\n",
       " [0.9874522089958191, 0.012547743506729603],\n",
       " [0.633001983165741, 0.3669980764389038],\n",
       " [0.0019327520858496428, 0.998067319393158]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_segmentation(probs, threshold):  #one sample\n",
    "    if threshold < 0 or threshold > 1:\n",
    "        return None\n",
    "    segmentation = []\n",
    "    for prob in probs:\n",
    "        if prob[1] >= threshold:\n",
    "            segmentation.append(1)\n",
    "        else:\n",
    "            segmentation.append(0)\n",
    "    return segmentation\n",
    "\n",
    "def prediction_to_bounds(pred:list) -> list:\n",
    "    bounds = []\n",
    "    start = 0\n",
    "    end = 0\n",
    "    for i,e in enumerate(pred):\n",
    "        if e == 0:\n",
    "            end = i\n",
    "            bounds.append((start, end))\n",
    "            start = end + 1\n",
    "    if not bounds:\n",
    "        bounds.append((0, len(pred)))\n",
    "    return bounds\n",
    "\n",
    "# It discards the first -100 of the network output\n",
    "def token_gt_bounds(pred:list) -> list:\n",
    "    bounds = []\n",
    "    start = 0\n",
    "    end = 0\n",
    "    for i,e in enumerate(pred[1:]):\n",
    "        if e == 0:\n",
    "            end = i\n",
    "            bounds.append((start, end))\n",
    "            start = end + 1\n",
    "    if not bounds:\n",
    "        bounds.append((0, len(pred)))\n",
    "    return bounds\n",
    "\n",
    "def split_by_prediction(pred:list, tok_ids:list, tokenizer) -> list:\n",
    "    start = 0\n",
    "    end = 0\n",
    "    spans = []\n",
    "    for i,e in enumerate(pred):\n",
    "        if e == 1:\n",
    "            end = i\n",
    "            span = tokenizer.decode(tok_ids[start:end +1], skip_special_tokens= True, clean_up_tokenization_spaces= False)\n",
    "            spans.append(span)\n",
    "            start = end + 1\n",
    "            end = end + 1\n",
    "    if not spans:\n",
    "        spans.append(tokenizer.decode(tok_ids, skip_special_tokens= True, clean_up_tokenization_spaces= False))\n",
    "    return spans\n",
    "    \n",
    "     \n",
    "\n",
    "preds = [decode_segmentation(e, 0.2) for e in probs]\n",
    "\n",
    "bert_preds = []\n",
    "for i,e in enumerate(preds):\n",
    "    spans = split_by_prediction(e, test_dataset[i]['input_ids'], test_dataset.tokenizer)\n",
    "    bert_preds.append(find_word_bounds(spans, test_dataset.df.iloc[i]['Testo']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# A è B sono tupe con i bound dello span\n",
    "def IoU(A, B):\n",
    "    '''\n",
    "    Given two intervals, find the intersection over union between them.\n",
    "    \n",
    "    :param A: The first bounding box\n",
    "    :param B: The bounding box\n",
    "    :return: the intersection over union of the two bounding boxes.\n",
    "    '''\n",
    "    if A == B:\n",
    "        return 1\n",
    "    start = max(A[0], B[0])\n",
    "    end = min(A[1], B[1])\n",
    "    if(start > end):\n",
    "        return 0\n",
    "    intersection = end - start\n",
    "    return intersection / (A[1] - A[0] + B[1] - B[0] - intersection)\n",
    "\n",
    "def compute_IoUs(pred_bounds, gt_spans):\n",
    "    '''\n",
    "    Given a list of predicted spans and a list of ground truth spans, \n",
    "    compute the intersection over union for each pair of spans\n",
    "    \n",
    "    :param pred_bounds: a tuple of (start, end) denoting the predicted answer\n",
    "    :param gt_spans: a list of tuples of the form (start, end) representing the spans of each ground\n",
    "    truth annotation\n",
    "    :return: a list of IoUs for each ground truth span.\n",
    "    '''\n",
    "    IoUs = []\n",
    "    for gt_bounds in gt_spans:\n",
    "        IoUs.append(IoU(pred_bounds, gt_bounds)) \n",
    "    return IoUs\n",
    "\n",
    "def normalize(text_spans_dict, gt_spans):\n",
    "    normalized = []\n",
    "    for i in range(len(text_spans_dict)):\n",
    "        #normalized is not empty\n",
    "        if normalized:\n",
    "            if normalized[-1]['Repertorio'] == text_spans_dict[i]['Repertorio']:\n",
    "                new_span = (normalized[-1]['Bounds'][0], text_spans_dict[i]['Bounds'][1])\n",
    "                new_span_features = {\n",
    "                    'Bounds' : new_span, \n",
    "                    'IoU' : None,\n",
    "                    'Repertorio' : text_spans_dict[i]['Repertorio']\n",
    "                    }\n",
    "                del normalized[-1]\n",
    "                normalized.append(new_span_features)\n",
    "            else:\n",
    "                normalized.append(text_spans_dict[i])\n",
    "        else:\n",
    "            normalized.append(text_spans_dict[i])\n",
    "        \n",
    "    \n",
    "    for i in range(len(normalized)):\n",
    "        normalized[i]['IoU'] = max(compute_IoUs(normalized[i]['Bounds'], gt_spans['Word_bounds']))\n",
    "    return normalized\n",
    "    \n",
    "\n",
    "metrics = []\n",
    "normalized_metrics = []\n",
    "for i, pred_bounds in enumerate(bert_preds):\n",
    "    text_IoUs = []\n",
    "    for pred_span in pred_bounds:\n",
    "        IoUs = compute_IoUs(pred_span, test_dataset.df.iloc[i]['Word_bounds'])   \n",
    "        best = np.argmax(IoUs)\n",
    "        span_features = {\n",
    "            'Bounds' : pred_span, \n",
    "            'IoU' : IoUs[best],\n",
    "            'Repertorio' : test_dataset.df.iloc[i]['Repertori'][best]\n",
    "            }\n",
    "\n",
    "        text_IoUs.append(span_features)\n",
    "    metrics.append(text_IoUs)\n",
    "    normalized_metrics.append(normalize(text_IoUs, test_dataset.df.iloc[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------\n",
      "Risultati labels GT e stralci non uniti\n",
      "Numero stralci nel dataset: 7239\n",
      "Numero stralci predetti: 6900\n",
      "Numero stralci con lunghezza minima =  0 :  6884\n",
      "Media IoU: 0.49624251244499556\n",
      "Percentuale span perfetti:  0.16705403834979662\n"
     ]
    }
   ],
   "source": [
    "print('----------------------------------------------------------')\n",
    "print('Risultati labels GT e stralci non uniti')\n",
    "\n",
    "\n",
    "n_spans = 0\n",
    "for e in test_dataset.df['Stralci']:\n",
    "    n_spans += len(e)\n",
    "print('Numero stralci nel dataset:', str(n_spans))\n",
    "\n",
    "n_spans = 0\n",
    "for e in metrics:\n",
    "    n_spans += len(e)\n",
    "print('Numero stralci predetti:', str(n_spans))\n",
    "\n",
    "mean = 0\n",
    "long_spans = 0\n",
    "min_lenght = 0\n",
    "perfect_spans = 0\n",
    "for text in metrics:\n",
    "    for span in text:\n",
    "        if span['Bounds'][1] - span['Bounds'][0] >= min_lenght:\n",
    "            long_spans += 1\n",
    "            mean += span['IoU']\n",
    "            if span['IoU'] == 1:\n",
    "                perfect_spans += 1\n",
    "perfect_spans_perc = perfect_spans / long_spans\n",
    "mean_IoU = mean / long_spans\n",
    "print('Numero stralci con lunghezza minima = ',\n",
    "      str(min_lenght), ': ', str(long_spans))\n",
    "print('Media IoU:', str(mean_IoU))\n",
    "print('Percentuale span perfetti: ', str(perfect_spans_perc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------\n",
      "Risultati labels GT e stralci uniti\n",
      "Numero stralci nel dataset: 7239\n",
      "Numero stralci predetti: 4494\n",
      "Numero stralci con lunghezza minima =  0 :  4488\n",
      "Media IoU: 0.7587487833438041\n",
      "Percentuale span perfetti:  0.37655971479500894\n"
     ]
    }
   ],
   "source": [
    "print('----------------------------------------------------------')\n",
    "print('Risultati labels GT e stralci uniti')\n",
    "\n",
    "\n",
    "n_spans = 0\n",
    "for e in test_dataset.df['Stralci']:\n",
    "    n_spans += len(e)\n",
    "print('Numero stralci nel dataset:', str(n_spans))\n",
    "\n",
    "n_spans = 0\n",
    "for e in normalized_metrics:\n",
    "    n_spans += len(e)\n",
    "print('Numero stralci predetti:', str(n_spans))\n",
    "\n",
    "mean = 0\n",
    "long_spans = 0\n",
    "min_lenght = 0\n",
    "perfect_spans = 0\n",
    "for text in normalized_metrics:\n",
    "    for span in text:\n",
    "        if span['Bounds'][1] - span['Bounds'][0] >= min_lenght:\n",
    "            long_spans += 1\n",
    "            mean += span['IoU']\n",
    "            if span['IoU'] == 1:\n",
    "                perfect_spans += 1\n",
    "perfect_spans_perc = perfect_spans / long_spans\n",
    "mean_IoU = mean / long_spans\n",
    "print('Numero stralci con lunghezza minima = ',\n",
    "      str(min_lenght), ': ', str(long_spans))\n",
    "print('Media IoU:', str(mean_IoU))\n",
    "print('Percentuale span perfetti: ', str(perfect_spans_perc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "925faa4ca74e07e17e8807425b2222c7f6b32ec00bddad3c89cd83a7cae0c688"
  },
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
