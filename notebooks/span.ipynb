{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('../data/Original_csv/Hyperion.csv', na_filter=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "#df.drop('Domanda', inplace=True, axis=1)\n",
    "#df.drop('Repertorio', inplace=True, axis=1)\n",
    "\n",
    "#filter = df[\"Repertorio\"] != \"\"\n",
    "#df = df[filter]\n",
    "\n",
    "# lower entire dataset\n",
    "#df = df.applymap(str.lower)\n",
    "\n",
    "#df['Repertorio'].replace('implicazioni','implicazione', inplace=True)\n",
    "#df['Repertorio'].replace('previsioni','previsione', inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "dataset = []\n",
    "sample = {}\n",
    "\n",
    "for row in df.itertuples():\n",
    "    if row.Testo:\n",
    "        dataset.append(sample)\n",
    "        sample = {}\n",
    "        sample['Testo'] = row.Testo.replace(\"\\n\", \" \")\n",
    "        sample['Stralci'] = [row.Stralcio.replace(\"\\n\", \" \")]\n",
    "        sample['Repertori'] = [row.Repertorio]\n",
    "        \n",
    "    if not row.Testo:\n",
    "        sample['Stralci'].append(row.Stralcio.replace(\"\\n\", \" \"))\n",
    "        sample['Repertori'].append(row.Repertorio)\n",
    "del dataset[0]\n",
    "\n",
    "#Find bounds starting froma text\n",
    "def find_char_bounds(spans: list, text: str) -> list:\n",
    "    bounds = []\n",
    "    last_char = 0\n",
    "    for span in spans:\n",
    "        start = text.find(span)\n",
    "        if start == -1:\n",
    "            start = last_char + 1\n",
    "        bounds.append((start, start + len(span)))\n",
    "        last_char = start + len(span)\n",
    "    return bounds\n",
    "\n",
    "def find_word_bounds(spans: list, text: str) -> list:\n",
    "    bounds = []\n",
    "    start = 0\n",
    "    for span in spans:\n",
    "        s = span.translate(str.maketrans('', '', string.punctuation))\n",
    "        word_list = s.split()\n",
    "        text_list = text.translate(str.maketrans('', '', string.punctuation)).split()\n",
    "        try:\n",
    "            start = text_list.index(word_list[0], start)\n",
    "        except:\n",
    "            if not bounds:\n",
    "                start = 0\n",
    "            else:\n",
    "                start = bounds[-1][1] +1\n",
    "            \n",
    "        bounds.append((start, start + len(word_list)))\n",
    "    return bounds\n",
    "\n",
    "\n",
    "for sample in dataset:\n",
    "    #sample['Bounds'] = find_char_bounds(sample['Stralci'], sample['Testo'])\n",
    "    sample['Bounds'] = find_word_bounds(sample['Stralci'], sample['Testo'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/michele/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 5), (5, 13), (13, 23), (23, 38), (38, 45), (45, 47)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0, 5), (5, 13), (13, 23), (23, 38), (38, 45), (45, 47)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk import sent_tokenize\n",
    "import re\n",
    "\n",
    "nltk_pred = []\n",
    "\n",
    "for sample in dataset:\n",
    "    tokens = sent_tokenize(sample['Testo'])\n",
    "    spans = []\n",
    "    bounds = []\n",
    "    for x in tokens:\n",
    "        #spans += re.findall('.*?[.:!?;,]', x)\n",
    "        spans += re.split('[.:!?;,]', x)\n",
    "        spans = list(filter(None, spans))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #bounds += find_char_bounds(spans, sample['Testo'])\n",
    "    bounds += find_word_bounds(spans, sample['Testo'])\n",
    "    nltk_pred.append(bounds)\n",
    "\n",
    "print(dataset[0]['Bounds'])\n",
    "nltk_pred[0]\n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection_over_union(boxA, boxB):\n",
    "    # determine the (x, y)-coordinates of the intersection rectangle\n",
    "    xA = max(boxA[0], boxB[0])\n",
    "    #yA = max(boxA[1], boxB[1])\n",
    "    xB = min(boxA[1], boxB[1])\n",
    "    #yB = min(boxA[3], boxB[3])\n",
    "\n",
    "    # compute the area of intersection rectangle\n",
    "    interArea = abs(max((xB - xA, 0)))\n",
    "    if interArea == 0:\n",
    "        return 0\n",
    "    # compute the area of both the prediction and ground-truth\n",
    "    # rectangles\n",
    "    boxAArea = abs((boxA[1] - boxA[0]))\n",
    "    boxBArea = abs((boxB[1] - boxB[0]))\n",
    "\n",
    "    # compute the intersection over union by taking the intersection\n",
    "    # area and dividing it by the sum of prediction + ground-truth\n",
    "    # areas - the interesection area\n",
    "    iou = interArea / float(boxAArea + boxBArea - interArea)\n",
    "\n",
    "    # return the intersection over union value\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# A Ã¨ B sono tupe con i bound dello span\n",
    "def IoU(A, B):\n",
    "    if A == B:\n",
    "        return 1\n",
    "    start = max(A[0], B[0])\n",
    "    end = min(A[1], B[1])\n",
    "    if(start > end):\n",
    "        return 0\n",
    "    intersection = end - start\n",
    "    return intersection / (A[1] - A[0] + B[1] - B[0] - intersection)\n",
    "\n",
    "def compute_IoUs(pred_bounds, gt_spans):\n",
    "    IoUs = []\n",
    "    for gt_bounds in gt_spans:\n",
    "        IoUs.append(IoU(pred_bounds, gt_bounds)) \n",
    "    return IoUs\n",
    "\n",
    "#Input: text_spans_dict = [ {\n",
    "#           'Bounds' : (a,b), \n",
    "#           'IoU' : float,\n",
    "#           'Repertorio': 'string':\n",
    "#           } ]\n",
    "def normalize(text_spans_dict, gt_spans):\n",
    "    normalized = []\n",
    "    for i in range(len(text_spans_dict)):\n",
    "        #normalized is not empty\n",
    "        if normalized:\n",
    "            if normalized[-1]['Repertorio'] == text_spans_dict[i]['Repertorio']:\n",
    "                new_span = (normalized[-1]['Bounds'][0], text_spans_dict[i]['Bounds'][1])\n",
    "                new_span_features = {\n",
    "                    'Bounds' : new_span, \n",
    "                    'IoU' : None,\n",
    "                    'Repertorio' : text_spans_dict[i]['Repertorio']\n",
    "                    }\n",
    "                del normalized[-1]\n",
    "                normalized.append(new_span_features)\n",
    "            else:\n",
    "                normalized.append(text_spans_dict[i])\n",
    "        else:\n",
    "            normalized.append(text_spans_dict[i])\n",
    "        \n",
    "    \n",
    "    for i in range(len(normalized)):\n",
    "        normalized[i]['IoU'] = max(compute_IoUs(normalized[i]['Bounds'], gt_spans['Bounds']))\n",
    "    return normalized\n",
    "    \n",
    "\n",
    "metrics = []\n",
    "normalized_metrics = []\n",
    "for i, pred_bounds in enumerate(nltk_pred):\n",
    "    text_IoUs = []\n",
    "    for pred_span in pred_bounds:\n",
    "        IoUs = compute_IoUs(pred_span, dataset[i]['Bounds'])\n",
    "        best = np.argmax(IoUs)\n",
    "        span_features = {\n",
    "            'Bounds' : pred_span, \n",
    "            'IoU' : IoUs[best],\n",
    "            'Repertorio' : dataset[i]['Repertori'][best]\n",
    "            }\n",
    "\n",
    "        text_IoUs.append(span_features)\n",
    "    metrics.append(text_IoUs)\n",
    "    normalized_metrics.append(normalize(text_IoUs, dataset[i]))\n",
    "\n",
    "\n",
    "        \n",
    "#TODO non funziona normalizzazione\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'Bounds': (0, 8), 'IoU': 0.7272727272727273, 'Repertorio': 'Non risposta'}, {'Bounds': (8, 11), 'IoU': 0.2727272727272727, 'Repertorio': 'Non risposta'}, {'Bounds': (11, 14), 'IoU': 0.75, 'Repertorio': 'Commento'}, {'Bounds': (11, 12), 'IoU': 0.25, 'Repertorio': 'Commento'}, {'Bounds': (15, 23), 'IoU': 1, 'Repertorio': 'Giudizio'}]\n",
      "[{'Bounds': (0, 11), 'IoU': 1, 'Repertorio': 'Non risposta'}, {'Bounds': (11, 12), 'IoU': 0.25, 'Repertorio': 'Commento'}, {'Bounds': (15, 23), 'IoU': 1, 'Repertorio': 'Giudizio'}]\n",
      "{'Testo': 'Chi non vuole il MES scenda in piazza. Coronavirus o no. Basta social network. Basta. Siamo uomini e donne o larve da tastiera?', 'Stralci': ['Chi non vuole il MES scenda in piazza. Coronavirus o no. ', 'Basta social network. Basta', 'Siamo uomini e donne o larve da tastiera?'], 'Repertori': ['Non risposta', 'Commento', 'Giudizio'], 'Bounds': [(0, 11), (11, 15), (15, 23)]}\n"
     ]
    }
   ],
   "source": [
    "print(metrics[10])\n",
    "print(normalized_metrics[10])\n",
    "print(dataset[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero stralci nel dataset: 35471\n",
      "Numero stralci predetti: 32550\n",
      "Numero stralci con lunghezza minima =  0 :  32550\n",
      "Media IoU: 0.8713763257225484\n",
      "Percentuale span perfetti:  0.6668817204301075\n"
     ]
    }
   ],
   "source": [
    "n_spans = 0\n",
    "for e in dataset:\n",
    "    n_spans += len(e['Bounds'])\n",
    "print('Numero stralci nel dataset:', str(n_spans))\n",
    "\n",
    "n_spans = 0\n",
    "for e in normalized_metrics:\n",
    "    n_spans += len(e)\n",
    "print('Numero stralci predetti:', str(n_spans))\n",
    "\n",
    "mean = 0\n",
    "long_spans = 0\n",
    "min_lenght = 0\n",
    "perfect_spans =0\n",
    "for text in normalized_metrics:\n",
    "    for span in text:\n",
    "        if span['Bounds'][1] - span['Bounds'][0] >= min_lenght:\n",
    "            long_spans += 1\n",
    "            mean += span['IoU']\n",
    "            if span['IoU'] == 1:\n",
    "                perfect_spans += 1\n",
    "perfect_spans_perc = perfect_spans / long_spans\n",
    "mean_IoU = mean / long_spans\n",
    "print('Numero stralci con lunghezza minima = ', str(min_lenght), ': ', str(long_spans))\n",
    "print('Media IoU:', str(mean_IoU))\n",
    "print('Percentuale span perfetti: ', str(perfect_spans_perc))\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "531bf1ddd0b9ee64f0e2ebd6527c520de4e2159f2ffcc26ba5e4b06431092b93"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
