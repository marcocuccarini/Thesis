{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('../data/Original_csv/Hyperion.csv', na_filter=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "#df.drop('Domanda', inplace=True, axis=1)\n",
    "#df.drop('Repertorio', inplace=True, axis=1)\n",
    "\n",
    "#filter = df[\"Repertorio\"] != \"\"\n",
    "#df = df[filter]\n",
    "\n",
    "# lower entire dataset\n",
    "#df = df.applymap(str.lower)\n",
    "\n",
    "#df['Repertorio'].replace('implicazioni','implicazione', inplace=True)\n",
    "#df['Repertorio'].replace('previsioni','previsione', inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "dataset = []\n",
    "sample = {}\n",
    "\n",
    "for row in df.itertuples():\n",
    "    if row.Testo:\n",
    "        dataset.append(sample)\n",
    "        sample = {}\n",
    "        sample['Testo'] = row.Testo.replace(\"\\n\", \" \")\n",
    "        sample['Stralci'] = [row.Stralcio.replace(\"\\n\", \" \")]\n",
    "        sample['Repertori'] = [row.Repertorio]\n",
    "        \n",
    "    if not row.Testo:\n",
    "        sample['Stralci'].append(row.Stralcio.replace(\"\\n\", \" \"))\n",
    "        sample['Repertori'].append(row.Repertorio)\n",
    "del dataset[0]\n",
    "\n",
    "#Find bounds starting froma text\n",
    "def find_char_bounds(spans: list, text: str) -> list:\n",
    "    bounds = []\n",
    "    last_char = 0\n",
    "    for span in spans:\n",
    "        start = text.find(span)\n",
    "        if start == -1:\n",
    "            start = last_char + 1\n",
    "        bounds.append((start, start + len(span)))\n",
    "        last_char = start + len(span)\n",
    "    return bounds\n",
    "\n",
    "def find_word_bounds(spans: list, text: str) -> list:\n",
    "    bounds = []\n",
    "    end = 0\n",
    "    for span in spans:\n",
    "        s = span.translate(str.maketrans('', '', string.punctuation))\n",
    "        word_list = s.split()\n",
    "        text_list = text.translate(str.maketrans('', '', string.punctuation)).split()\n",
    "        try:\n",
    "            start = text_list.index(word_list[0], end)\n",
    "        except:\n",
    "            if not bounds:\n",
    "                start = 0\n",
    "            else:\n",
    "                \n",
    "                start = bounds[-1][1] + 1\n",
    "        end = start + len(word_list) - 1\n",
    "            \n",
    "        bounds.append((start, end))\n",
    "    return bounds\n",
    "\n",
    "\n",
    "for sample in dataset:\n",
    "    #sample['Bounds'] = find_char_bounds(sample['Stralci'], sample['Testo'])\n",
    "    sample['Bounds'] = find_word_bounds(sample['Stralci'], sample['Testo'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/michele/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk import sent_tokenize\n",
    "import re\n",
    "\n",
    "nltk_pred = []\n",
    "spans_pred = []\n",
    "\n",
    "for sample in dataset:\n",
    "    tokens = sent_tokenize(sample['Testo'])\n",
    "    spans = []\n",
    "    bounds = []\n",
    "    for x in tokens:\n",
    "        #spans += re.findall('.*?[.:!?;,]', x)\n",
    "        spans += re.split('[.:!?;,]', x)\n",
    "        spans = list(filter(None, spans)) # filter empty strings\n",
    "\n",
    "    #bounds += find_char_bounds(spans, sample['Testo'])\n",
    "    bounds += find_word_bounds(spans, sample['Testo'])\n",
    "    nltk_pred.append(bounds)\n",
    "    spans_pred.append(spans) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zaia, 0,4% positivi su 1200 lavoratori In settimana screening su altri 13 mila di 79 aziende VENEZIA, 10 MAG - Cinque lavoratori su 1.200, di otto aziende padovane, sono risultati positivi al Coronavirus. Lo 0,4% ha sottolineato il presidente del Veneto, Luca Zaia, il quale ha annunciato che in settimana partirà uno screening in altre 79 aziende regionali per complessivi 13.000 dipendenti.     \"E' un progetto pilota - ha ricordato l'assessore alla sanità del Veneto, Manuela Lanzarin - al quale avevano aderito 8 aziende del padovano e ora la richiesta si è allargata alle altre\". L'assessore ha infine rilevato che \"si stanno concludendo i test svolti a farmacisti e loro collaboratori, personale delle forze dell' ordine, volontari della protezione Civile, sanitari. Potremo così avere un dato preciso per fasce di ogni singola categoria\".\n",
      "[(0, 39), (40, 60), (61, 116), (117, 128)]\n",
      "[(0, 0), (1, 1), (2, 17), (17, 22), (23, 23), (23, 26), (27, 31), (32, 33), (34, 40), (40, 41), (42, 59), (60, 61), (61, 71), (72, 90), (91, 106), (107, 111), (112, 115), (116, 116), (117, 128)]\n",
      "['Zaia', ' 0', '4% positivi su 1200 lavoratori In settimana screening su altri 13 mila di 79 aziende VENEZIA', ' 10 MAG - Cinque lavoratori su 1', '200', ' di otto aziende padovane', ' sono risultati positivi al Coronavirus', 'Lo 0', '4% ha sottolineato il presidente del Veneto', ' Luca Zaia', ' il quale ha annunciato che in settimana partirà uno screening in altre 79 aziende regionali per complessivi 13', '000 dipendenti', '\"E\\' un progetto pilota - ha ricordato l\\'assessore alla sanità del Veneto', ' Manuela Lanzarin - al quale avevano aderito 8 aziende del padovano e ora la richiesta si è allargata alle altre\"', 'L\\'assessore ha infine rilevato che \"si stanno concludendo i test svolti a farmacisti e loro collaboratori', \" personale delle forze dell' ordine\", ' volontari della protezione Civile', ' sanitari', 'Potremo così avere un dato preciso per fasce di ogni singola categoria\"']\n"
     ]
    }
   ],
   "source": [
    "print(dataset[1100]['Testo'])\n",
    "print(dataset[1100]['Bounds'])\n",
    "print(nltk_pred[1100])\n",
    "print(spans_pred[1100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# A è B sono tupe con i bound dello span\n",
    "def IoU(A, B):\n",
    "    if A == B:\n",
    "        return 1\n",
    "    start = max(A[0], B[0])\n",
    "    end = min(A[1], B[1])\n",
    "    if(start > end):\n",
    "        return 0\n",
    "    intersection = end - start\n",
    "    return intersection / (A[1] - A[0] + B[1] - B[0] - intersection)\n",
    "\n",
    "def compute_IoUs(pred_bounds, gt_spans):\n",
    "    IoUs = []\n",
    "    for gt_bounds in gt_spans:\n",
    "        IoUs.append(IoU(pred_bounds, gt_bounds)) \n",
    "    return IoUs\n",
    "\n",
    "#Input: text_spans_dict = [ {\n",
    "#           'Bounds' : (a,b), \n",
    "#           'IoU' : float,\n",
    "#           'Repertorio': 'string':\n",
    "#           } ]\n",
    "def normalize(text_spans_dict, gt_spans):\n",
    "    normalized = []\n",
    "    for i in range(len(text_spans_dict)):\n",
    "        #normalized is not empty\n",
    "        if normalized:\n",
    "            if normalized[-1]['Repertorio'] == text_spans_dict[i]['Repertorio']:\n",
    "                new_span = (normalized[-1]['Bounds'][0], text_spans_dict[i]['Bounds'][1])\n",
    "                new_span_features = {\n",
    "                    'Bounds' : new_span, \n",
    "                    'IoU' : None,\n",
    "                    'Repertorio' : text_spans_dict[i]['Repertorio']\n",
    "                    }\n",
    "                del normalized[-1]\n",
    "                normalized.append(new_span_features)\n",
    "            else:\n",
    "                normalized.append(text_spans_dict[i])\n",
    "        else:\n",
    "            normalized.append(text_spans_dict[i])\n",
    "        \n",
    "    \n",
    "    for i in range(len(normalized)):\n",
    "        normalized[i]['IoU'] = max(compute_IoUs(normalized[i]['Bounds'], gt_spans['Bounds']))\n",
    "    return normalized\n",
    "    \n",
    "\n",
    "metrics = []\n",
    "normalized_metrics = []\n",
    "for i, pred_bounds in enumerate(nltk_pred):\n",
    "    text_IoUs = []\n",
    "    for pred_span in pred_bounds:\n",
    "        IoUs = compute_IoUs(pred_span, dataset[i]['Bounds'])\n",
    "        best = np.argmax(IoUs)\n",
    "        span_features = {\n",
    "            'Bounds' : pred_span, \n",
    "            'IoU' : IoUs[best],\n",
    "            'Repertorio' : dataset[i]['Repertori'][best]\n",
    "            }\n",
    "\n",
    "        text_IoUs.append(span_features)\n",
    "    metrics.append(text_IoUs)\n",
    "    normalized_metrics.append(normalize(text_IoUs, dataset[i]))\n",
    "\n",
    "\n",
    "        \n",
    "#TODO non funziona normalizzazione\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'Bounds': (0, 7), 'IoU': 0.7, 'Repertorio': 'Non risposta'}, {'Bounds': (8, 10), 'IoU': 0.2, 'Repertorio': 'Non risposta'}, {'Bounds': (11, 13), 'IoU': 0.6666666666666666, 'Repertorio': 'Commento'}, {'Bounds': (14, 14), 'IoU': 0, 'Repertorio': 'Non risposta'}, {'Bounds': (15, 22), 'IoU': 1, 'Repertorio': 'Giudizio'}]\n",
      "[{'Bounds': (0, 10), 'IoU': 1, 'Repertorio': 'Non risposta'}, {'Bounds': (11, 13), 'IoU': 0.6666666666666666, 'Repertorio': 'Commento'}, {'Bounds': (14, 14), 'IoU': 0, 'Repertorio': 'Non risposta'}, {'Bounds': (15, 22), 'IoU': 1, 'Repertorio': 'Giudizio'}]\n",
      "{'Testo': 'Chi non vuole il MES scenda in piazza. Coronavirus o no. Basta social network. Basta. Siamo uomini e donne o larve da tastiera?', 'Stralci': ['Chi non vuole il MES scenda in piazza. Coronavirus o no. ', 'Basta social network. Basta', 'Siamo uomini e donne o larve da tastiera?'], 'Repertori': ['Non risposta', 'Commento', 'Giudizio'], 'Bounds': [(0, 10), (11, 14), (15, 22)]}\n"
     ]
    }
   ],
   "source": [
    "print(metrics[10])\n",
    "print(normalized_metrics[10])\n",
    "print(dataset[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero stralci nel dataset: 35471\n",
      "Numero stralci predetti: 34244\n",
      "Numero stralci con lunghezza minima =  0 :  34190\n",
      "Media IoU: 0.8214227711044331\n",
      "Percentuale span perfetti:  0.6230184264404797\n"
     ]
    }
   ],
   "source": [
    "n_spans = 0\n",
    "for e in dataset:\n",
    "    n_spans += len(e['Bounds'])\n",
    "print('Numero stralci nel dataset:', str(n_spans))\n",
    "\n",
    "n_spans = 0\n",
    "for e in normalized_metrics:\n",
    "    n_spans += len(e)\n",
    "print('Numero stralci predetti:', str(n_spans))\n",
    "\n",
    "mean = 0\n",
    "long_spans = 0\n",
    "min_lenght = 0\n",
    "perfect_spans =0\n",
    "for text in normalized_metrics:\n",
    "    for span in text:\n",
    "        if span['Bounds'][1] - span['Bounds'][0] >= min_lenght:\n",
    "            long_spans += 1\n",
    "            mean += span['IoU']\n",
    "            if span['IoU'] == 1:\n",
    "                perfect_spans += 1\n",
    "perfect_spans_perc = perfect_spans / long_spans\n",
    "mean_IoU = mean / long_spans\n",
    "print('Numero stralci con lunghezza minima = ', str(min_lenght), ': ', str(long_spans))\n",
    "print('Media IoU:', str(mean_IoU))\n",
    "print('Percentuale span perfetti: ', str(perfect_spans_perc))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Span Classification\n",
    "\n",
    "I provide in input non normalized spans, to predict the label, then I apply normalization to spans exploiting predicted labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained('MiBo/RepML')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"m-polignano-uniba/bert_uncased_L-12_H-768_A-12_italian_alb3rt0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(spans_pred[0]))\n",
    "print(len(metrics[0]))\n",
    "type(metrics[0][0]['Repertorio'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151230\n",
      "151230\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "spans = []\n",
    "labels = []\n",
    "for span in spans_pred:\n",
    "    spans += span\n",
    "for text in metrics:\n",
    "    for span in text:\n",
    "        labels.append(span['Repertorio'])\n",
    "print(len(spans))\n",
    "print(len(labels))\n",
    "\n",
    "df = pd.DataFrame.from_records({\n",
    "    'Stralcio' : spans,\n",
    "    'Repertorio' : labels\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn import preprocessing\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
    "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
    "from ekphrasis.dicts.emoticons import emoticons\n",
    "\n",
    "LABELS = [\n",
    "                'anticipazione',\n",
    "                'causa',\n",
    "                'commento',\n",
    "                'conferma',\n",
    "                'considerazione',\n",
    "                'contrapposizione',\n",
    "                'deresponsabilizzazione',\n",
    "                'descrizione',\n",
    "                'dichiarazione di intenti',\n",
    "                'generalizzazione',\n",
    "                'giudizio',\n",
    "                'giustificazione',\n",
    "                'implicazione',\n",
    "                'non risposta',\n",
    "                'opinione',\n",
    "                'possibilità',\n",
    "                'prescrizione',\n",
    "                'previsione',\n",
    "                'proposta',\n",
    "                'ridimensionamento',\n",
    "                'sancire',\n",
    "                'specificazione',\n",
    "                'valutazione'\n",
    "        ]\n",
    "\n",
    "class HyperionDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "\n",
    "    def __init__(self, df, tokenizer_name):\n",
    "        #fill_null_features(df)\n",
    "        df = filter_empty_labels(df)\n",
    "        #df = twitter_preprocess(df)\n",
    "        df = to_lower_case(df)\n",
    "        uniform_labels(df)          \n",
    "        tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "        self.encodings = tokenize(df, tokenizer)\n",
    "        self.labels = encode_labels(df).tolist()    \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        \n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def labels_list(self):\n",
    "        return LABELS\n",
    "\n",
    "\n",
    "\n",
    "# Dataset loading and preprocessing\n",
    "def fill_null_features(df):\n",
    "        for c in ['Domanda','Testo']:\n",
    "            for i in range(0,len(df.index)):  \n",
    "                if not df[c][i]:\n",
    "                    j=i\n",
    "                    while j>0: \n",
    "                        j-=1\n",
    "                        if df[c][j]:\n",
    "                            df[c][i] = df[c][j]\n",
    "                            break\n",
    "\n",
    "#Delete examples with empty label\n",
    "def filter_empty_labels(df):\n",
    "    filter = df[\"Repertorio\"] != \"\"\n",
    "    return df[filter]\n",
    "\n",
    "#Convert to lower case\n",
    "def to_lower_case(df):\n",
    "    return df.applymap(str.lower)\n",
    "\n",
    "\n",
    "#Lables uniformation uncased\n",
    "def uniform_labels(df):\n",
    "    df['Repertorio'].replace('implicazioni','implicazione', inplace=True)\n",
    "    df['Repertorio'].replace('previsioni','previsione', inplace=True)\n",
    "\n",
    "def tokenize(df, tokenizer):\n",
    "    return tokenizer(\n",
    "        df['Stralcio'].tolist(),\n",
    "        #df['Domanda'].tolist(),\n",
    "        max_length=512,\n",
    "        add_special_tokens=True,\n",
    "        return_attention_mask=True,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "def encode_labels(df):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(LABELS)\n",
    "    return le.transform(df['Repertorio'])\n",
    "\n",
    "def decode_labels(encoded_labels):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(LABELS)\n",
    "    return le.inverse_transform(encoded_labels)\n",
    "\n",
    "def twitter_preprocess(df):\n",
    "    text_processor = TextPreProcessor(\n",
    "    # terms that will be normalized\n",
    "    normalize=['url', 'email', 'percent', 'money', 'phone', 'user',\n",
    "        'time', 'date', 'number'],\n",
    "    # terms that will be annotated\n",
    "    annotate={\"hashtag\"},\n",
    "    fix_html=True,  # fix HTML tokens\n",
    "    \n",
    "    unpack_hashtags=True,  # perform word segmentation on hashtags\n",
    "    \n",
    "    # select a tokenizer. You can use SocialTokenizer, or pass your own\n",
    "    # the tokenizer, should take as input a string and return a list of tokens\n",
    "    tokenizer=SocialTokenizer(lowercase=True).tokenize,\n",
    "    \n",
    "    # list of dictionaries, for replacing tokens extracted from the text,\n",
    "    # with other expressions. You can pass more than one dictionaries.\n",
    "    dicts=[emoticons]\n",
    "    )\n",
    "\n",
    "    processed = []\n",
    "\n",
    "    for s in df['Stralcio']:\n",
    "        s = str(\" \".join(text_processor.pre_process_doc(s)))\n",
    "        s = re.sub(r\"[^a-zA-ZÀ-ú</>!?♥♡\\s\\U00010000-\\U0010ffff]\", ' ', s)\n",
    "        s = re.sub(r\"\\s+\", ' ', s)\n",
    "        s = re.sub(r'(\\w)\\1{2,}',r'\\1\\1', s)\n",
    "        s = re.sub ( r'^\\s' , '' , s )\n",
    "        s = re.sub ( r'\\s$' , '' , s )\n",
    "        processed.append(s)\n",
    "\n",
    "    df['Stralcio'] = processed\n",
    "\n",
    "    return df\n",
    "    \n",
    "\n",
    "def train_val_split(df, tok_name,  val_perc=0.2, subsample = False):\n",
    "    gb = df.groupby('Repertorio')\n",
    "    train_list = []\n",
    "    val_list = []\n",
    "    for x in gb.groups:\n",
    "        if subsample:\n",
    "            class_df = gb.get_group(x).head(50)\n",
    "        else:\n",
    "            class_df = gb.get_group(x)\n",
    "\n",
    "        # Validation set creation\n",
    "        val = class_df.sample(frac=val_perc)\n",
    "        train = pd.concat([class_df,val]).drop_duplicates(keep=False)\n",
    "\n",
    "        #train_list.append(train.head(500))\n",
    "        train_list.append(train)\n",
    "        val_list.append(val)\n",
    "\n",
    "\n",
    "    train_df = pd.concat(train_list)\n",
    "    val_df = pd.concat(val_list)\n",
    "    return HyperionDataset(train_df, tok_name), HyperionDataset(val_df, tok_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Repertorio</th>\n",
       "      <th>Stralcio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dichiarazione di intenti</td>\n",
       "      <td>Dunque vediamo se ho capito</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sancire</td>\n",
       "      <td>Conte le canta a Salvini e alla Meloni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Giustificazione</td>\n",
       "      <td>questi reagiscono e le cantano a loro volta a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sancire</td>\n",
       "      <td>Mentana si smarca da Conte e finisce con foto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Commento</td>\n",
       "      <td>Siamo tornati per un attimo alla normalità</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Repertorio                                           Stralcio\n",
       "0  Dichiarazione di intenti                        Dunque vediamo se ho capito\n",
       "1                   Sancire             Conte le canta a Salvini e alla Meloni\n",
       "2           Giustificazione   questi reagiscono e le cantano a loro volta a...\n",
       "3                   Sancire   Mentana si smarca da Conte e finisce con foto...\n",
       "4                  Commento         Siamo tornati per un attimo alla normalità"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn import preprocessing\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
    "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
    "from ekphrasis.dicts.emoticons import emoticons\n",
    "\n",
    "LABELS = [\n",
    "                'anticipazione',\n",
    "                'causa',\n",
    "                'commento',\n",
    "                'conferma',\n",
    "                'considerazione',\n",
    "                'contrapposizione',\n",
    "                'deresponsabilizzazione',\n",
    "                'descrizione',\n",
    "                'dichiarazione di intenti',\n",
    "                'generalizzazione',\n",
    "                'giudizio',\n",
    "                'giustificazione',\n",
    "                'implicazione',\n",
    "                'non risposta',\n",
    "                'opinione',\n",
    "                'possibilità',\n",
    "                'prescrizione',\n",
    "                'previsione',\n",
    "                'proposta',\n",
    "                'ridimensionamento',\n",
    "                'sancire',\n",
    "                'specificazione',\n",
    "                'valutazione'\n",
    "        ]\n",
    "\n",
    "class HyperionDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "\n",
    "    def __init__(self, df, tokenizer_name):\n",
    "        #fill_null_features(df)\n",
    "        df = filter_empty_labels(df)\n",
    "        #df = twitter_preprocess(df)\n",
    "        df = to_lower_case(df)\n",
    "        uniform_labels(df)          \n",
    "        tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "        self.encodings = tokenize(df, tokenizer)\n",
    "        self.labels = encode_labels(df).tolist()    \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        \n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def labels_list(self):\n",
    "        return LABELS\n",
    "\n",
    "\n",
    "\n",
    "# Dataset loading and preprocessing\n",
    "def fill_null_features(df):\n",
    "        for c in ['Domanda','Testo']:\n",
    "            for i in range(0,len(df.index)):  \n",
    "                if not df[c][i]:\n",
    "                    j=i\n",
    "                    while j>0: \n",
    "                        j-=1\n",
    "                        if df[c][j]:\n",
    "                            df[c][i] = df[c][j]\n",
    "                            break\n",
    "\n",
    "#Delete examples with empty label\n",
    "def filter_empty_labels(df):\n",
    "    filter = df[\"Repertorio\"] != \"\"\n",
    "    return df[filter]\n",
    "\n",
    "#Convert to lower case\n",
    "def to_lower_case(df):\n",
    "    return df.applymap(str.lower)\n",
    "\n",
    "\n",
    "#Lables uniformation uncased\n",
    "def uniform_labels(df):\n",
    "    df['Repertorio'].replace('implicazioni','implicazione', inplace=True)\n",
    "    df['Repertorio'].replace('previsioni','previsione', inplace=True)\n",
    "\n",
    "def tokenize(df, tokenizer):\n",
    "    return tokenizer(\n",
    "        df['Stralcio'].tolist(),\n",
    "        #df['Domanda'].tolist(),\n",
    "        max_length=512,\n",
    "        add_special_tokens=True,\n",
    "        return_attention_mask=True,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "def encode_labels(df):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(LABELS)\n",
    "    return le.transform(df['Repertorio'])\n",
    "\n",
    "def decode_labels(encoded_labels):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(LABELS)\n",
    "    return le.inverse_transform(encoded_labels)\n",
    "\n",
    "def twitter_preprocess(df):\n",
    "    text_processor = TextPreProcessor(\n",
    "    # terms that will be normalized\n",
    "    normalize=['url', 'email', 'percent', 'money', 'phone', 'user',\n",
    "        'time', 'date', 'number'],\n",
    "    # terms that will be annotated\n",
    "    annotate={\"hashtag\"},\n",
    "    fix_html=True,  # fix HTML tokens\n",
    "    \n",
    "    unpack_hashtags=True,  # perform word segmentation on hashtags\n",
    "    \n",
    "    # select a tokenizer. You can use SocialTokenizer, or pass your own\n",
    "    # the tokenizer, should take as input a string and return a list of tokens\n",
    "    tokenizer=SocialTokenizer(lowercase=True).tokenize,\n",
    "    \n",
    "    # list of dictionaries, for replacing tokens extracted from the text,\n",
    "    # with other expressions. You can pass more than one dictionaries.\n",
    "    dicts=[emoticons]\n",
    "    )\n",
    "\n",
    "    processed = []\n",
    "\n",
    "    for s in df['Stralcio']:\n",
    "        s = str(\" \".join(text_processor.pre_process_doc(s)))\n",
    "        s = re.sub(r\"[^a-zA-ZÀ-ú</>!?♥♡\\s\\U00010000-\\U0010ffff]\", ' ', s)\n",
    "        s = re.sub(r\"\\s+\", ' ', s)\n",
    "        s = re.sub(r'(\\w)\\1{2,}',r'\\1\\1', s)\n",
    "        s = re.sub ( r'^\\s' , '' , s )\n",
    "        s = re.sub ( r'\\s$' , '' , s )\n",
    "        processed.append(s)\n",
    "\n",
    "    df['Stralcio'] = processed\n",
    "\n",
    "    return df\n",
    "    \n",
    "\n",
    "def train_val_split(df, tok_name,  val_perc=0.2, subsample = False):\n",
    "    gb = df.groupby('Repertorio')\n",
    "    train_list = []\n",
    "    val_list = []\n",
    "    for x in gb.groups:\n",
    "        if subsample:\n",
    "            class_df = gb.get_group(x).head(50)\n",
    "        else:\n",
    "            class_df = gb.get_group(x)\n",
    "\n",
    "        # Validation set creation\n",
    "        val = class_df.sample(frac=val_perc)\n",
    "        train = pd.concat([class_df,val]).drop_duplicates(keep=False)\n",
    "\n",
    "        #train_list.append(train.head(500))\n",
    "        train_list.append(train)\n",
    "        val_list.append(val)\n",
    "\n",
    "\n",
    "    train_df = pd.concat(train_list)\n",
    "    val_df = pd.concat(val_list)\n",
    "    return HyperionDataset(train_df, tok_name), HyperionDataset(val_df, tok_name)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "531bf1ddd0b9ee64f0e2ebd6527c520de4e2159f2ffcc26ba5e4b06431092b93"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
