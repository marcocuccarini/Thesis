{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "stralcio + domanda -> repertorio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Neptune initialization\n",
    "run = neptune.init(\n",
    "    project=\"mibo8/Rep\",\n",
    "    api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJmZmRkYThiZi1mZGNlLTRlMTktODQwNS1hNWFlMWQ2Mjc4N2IifQ==\",\n",
    ")  # your credentials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset loading and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_null_features(df):\n",
    "    for c in ['Domanda','Testo']:\n",
    "        for i in range(0,len(df.index)):  \n",
    "            if not df[c][i]:\n",
    "                j=i\n",
    "                while j>0: \n",
    "                    j-=1\n",
    "                    if df[c][j]:\n",
    "                        df[c][i] = df[c][j]\n",
    "                        break\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero stralci: 35474\n",
      "Numero stralci dopo eliminazione: 35148\n"
     ]
    }
   ],
   "source": [
    "#Hyperion dataset\n",
    "df = pd.read_csv('../data/Original_csv/Hyperion.csv', na_filter=False)\n",
    "df = fill_null_features(df)\n",
    "\n",
    "print('Numero stralci: ' + str(len(df.index)))\n",
    "filter = df[\"Stralcio\"] != \"\"\n",
    "df = df[filter]\n",
    "filter = df[\"Repertorio\"] != \"\"\n",
    "df = df[filter]\n",
    "print('Numero stralci dopo eliminazione: ' + str(len(df.index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lables uniformation\n",
    "\n",
    "df['Repertorio'].replace('Implicazioni','Implicazione', inplace=True)\n",
    "df['Repertorio'].replace('Previsioni','Previsione', inplace=True)\n",
    "df['Repertorio'].replace('causa','Causa', inplace=True)\n",
    "df['Repertorio'].replace('commento','Commento', inplace=True)\n",
    "df['Repertorio'].replace('contrapposizione','Contrapposizione', inplace=True)\n",
    "df['Repertorio'].replace('generalizzazione','Generalizzazione', inplace=True)\n",
    "df['Repertorio'].replace('giudizio','Giudizio', inplace=True)\n",
    "df['Repertorio'].replace('prescrizione','Prescrizione', inplace=True)\n",
    "df['Repertorio'].replace('previsione','Previsione', inplace=True)\n",
    "df['Repertorio'].replace('sancire','Sancire', inplace=True)\n",
    "df['Repertorio'].replace('specificazione','Specificazione', inplace=True)\n",
    "df['Repertorio'].replace('valutazione','Valutazione', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class HyperionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        \n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X = Domanda + stralcio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "  \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\", num_labels=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "X_encodings = tokenizer(\n",
    "            df['Stralcio'][:300].tolist(),\n",
    "            add_special_tokens=True,\n",
    "            return_attention_mask=True,\n",
    "            padding=True,\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "\n",
    "labels = [\n",
    "    'Anticipazione',\n",
    "    'Causa',\n",
    "    'Commento',\n",
    "    'Conferma',\n",
    "    'Considerazione',\n",
    "    'Contrapposizione',\n",
    "    'Deresponsabilizzazione',\n",
    "    'Descrizione',\n",
    "    'Dichiarazione di intenti',\n",
    "    'Generalizzazione',\n",
    "    'Giudizio',\n",
    "    'Giustificazione',\n",
    "    'Implicazione',\n",
    "    'Non risposta',\n",
    "    'Opinione',\n",
    "    'Possibilit√†',\n",
    "    'Prescrizione',\n",
    "    'Previsione',\n",
    "    'Proposta',\n",
    "    'Ridimensionamento',\n",
    "    'Sancire',\n",
    "    'Specificazione',\n",
    "    'Valutazione']\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(labels)\n",
    "\n",
    "dataset = HyperionDataset(X_encodings,le.transform(df['Repertorio'][:300]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_perc = 0.7\n",
    "val_set_perc = 0.1\n",
    "test_set_perc = 0.2\n",
    "\n",
    "dataset_info = {\n",
    "    'training_set_size' : train_set_perc,\n",
    "    'validation_set_size' : val_set_perc,\n",
    "    'test_set_size' : test_set_perc\n",
    "}\n",
    "run['dataset'] = dataset_info\n",
    "\n",
    "\n",
    "train_dataset_size = int(len(dataset) * train_set_perc)\n",
    "val_dataset_size = int(len(dataset) * val_set_perc)\n",
    "test_dataset_size = len(dataset) - train_dataset_size - val_dataset_size # 0.2\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_dataset_size, val_dataset_size, test_dataset_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  torchmetrics\n",
    "\n",
    "# Metrics initialization\n",
    "metric_collection = torchmetrics.MetricCollection([\n",
    "    torchmetrics.Accuracy(num_classes=23, multiclass=True, average='none'),\n",
    "    torchmetrics.F1(num_classes=23, multiclass=True, average='none'),\n",
    "    torchmetrics.Precision(num_classes=23, multiclass=True, average='none'),\n",
    "    torchmetrics.Recall(num_classes=23, multiclass=True, average='none')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 1e-5\n",
    "batch_size = 8\n",
    "n_epochs = 1\n",
    "\n",
    "params_info = {\n",
    "    'learning_rate' : learning_rate,\n",
    "    'batch_size' : batch_size,\n",
    "    'n_epochs' : batch_size\n",
    "}\n",
    "run['params'] = params_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 1 ========\n",
      "Training...\n",
      "  Batch    10  of     27.    Elapsed: 0:00:59.\n",
      "  Batch    20  of     27.    Elapsed: 0:01:58.\n",
      "MetricCollection(\n",
      "  (Accuracy): Accuracy()\n",
      "  (F1): F1()\n",
      "  (Precision): Precision()\n",
      "  (Recall): Recall()\n",
      ")\n",
      "\n",
      "  Average training loss: 2.840\n",
      "  Training epoch took: 0:02:36\n",
      "\n",
      "Running Validation...\n",
      "VALIDATION: \n",
      "MetricCollection(\n",
      "  (Accuracy): Accuracy()\n",
      "  (F1): F1()\n",
      "  (Precision): Precision()\n",
      "  (Recall): Recall()\n",
      ")\n",
      "  Validation Loss: 2.60\n",
      "  Validation took: 0:00:05\n",
      "\n",
      "Training complete!\n",
      "Total training took 0:02:41 (h:mm:ss)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import  AdamW\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Measure the total training time for the whole run.\n",
    "total_t0 = time.time()\n",
    "\n",
    "epochs = n_epochs\n",
    "\n",
    "# Creation of Pytorch DataLoaders with shuffle=True for the traing phase\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "validation_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "#Adam algorithm optimized for tranfor architectures\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Setup for training with gpu\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "# For each epoch...\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    # Perform one full pass over the training set.\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_train_loss = 0\n",
    "\n",
    "    # Put the model into training mode: Dropout layers are active\n",
    "    model.train()\n",
    "    \n",
    "    # For each batch of training data...\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        # Progress update every 40 batches.\n",
    "        if step % 10 == 0 and not step == 0:\n",
    "            # Compute time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        # Unpack this training batch from the dataloader. \n",
    "        #\n",
    "        #  copy each tensor to the GPU using the 'to()' method\n",
    "        #\n",
    "        # 'batch' contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch['input_ids'].to(device)\n",
    "        b_input_mask = batch['attention_mask'].to(device)\n",
    "        b_labels = batch['labels'].to(device)\n",
    "\n",
    "        # clear any previously calculated gradients before performing a\n",
    "        # backward pass\n",
    "        model.zero_grad()        \n",
    "\n",
    "        # Perform a forward pass (evaluate the model on this training batch).\n",
    "        # This will return the loss (rather than the model output) because we\n",
    "        # have provided the `labels`.\n",
    "        outputs = model(b_input_ids, \n",
    "                             attention_mask=b_input_mask, \n",
    "                             labels=b_labels)\n",
    "        \n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "\n",
    "        # Accumulate the training loss over all of the batches so that we can\n",
    "        # calculate the average loss at the end.\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu()\n",
    "        label_ids = b_labels.to('cpu')\n",
    "        \"\"\"\n",
    "        # metric on current batch\n",
    "        batch_accuracy = accuracy(logits.softmax(dim=1), label_ids)\n",
    "        batch_f1 = f1(logits.softmax(dim=1), label_ids)\n",
    "        batch_precision = precision(logits.softmax(dim=1), label_ids)\n",
    "        batch_recall = recall(logits.softmax(dim=1), label_ids)\n",
    "        \"\"\"\n",
    "        batch_metric = metric_collection.update(logits.softmax(dim=1), label_ids)\n",
    "        #print(batch_metric)\n",
    "        \"\"\"\n",
    "        print(logits.softmax(dim=1))\n",
    "        print('......')\n",
    "        print(label_ids)\n",
    "        print(batch_accuracy)\n",
    "        \"\"\"\n",
    "        # Perform a backward pass to compute the gradients.\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the norm of the gradients to 1.0.\n",
    "        # This helps and prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Update parameters and take a step using the computed gradient\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "    # Compute the average loss over all of the batches.\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    \"\"\"\n",
    "    # Compute final metrics\n",
    "    epoch_accuracy = accuracy.compute()\n",
    "    epoch_f1 = f1.compute()\n",
    "    epoch_precision = precision.compute()\n",
    "    epoch_recall = recall.compute()\n",
    "    print(\"Accuracy: {0:.2f}\".format(epoch_accuracy))  \n",
    "    print(\"F1 score: {0:.2f}\".format(epoch_f1))  \n",
    "    print(\"Precision: {0:.2f}\".format(epoch_precision))\n",
    "    print(\"Recall: {0:.2f}\".format(epoch_recall)) \n",
    "    \"\"\"\n",
    "    final_metrics = metric_collection.compute()\n",
    "    print(final_metrics)\n",
    "    \n",
    "    # Measure how long this epoch took.\n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.3f}\".format(avg_train_loss))\n",
    "    print(\"  Training epoch took: {:}\".format(training_time))\n",
    "\n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure performance on\n",
    "    # the validation set.\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "    \"\"\"\n",
    "    # Reset metric's interanl state\n",
    "    accuracy.reset()\n",
    "    f1.reset()\n",
    "    precision.reset()\n",
    "    recall.reset()\n",
    "    \"\"\"\n",
    "    metric_collection.reset()\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Put the model in evaluation mode: the dropout layers behave differently\n",
    "    model.eval()\n",
    "\n",
    "    total_val_loss = 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "        \n",
    "        # Unpack this training batch from our dataloader. \n",
    "        #\n",
    "        # copy each tensor to the GPU using the 'to()' method\n",
    "        #\n",
    "        # 'batch' contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch['input_ids'].to(device)\n",
    "        b_input_mask = batch['attention_mask'].to(device)\n",
    "        b_labels = batch['labels'].to(device)\n",
    "        \n",
    "        # Tell pytorch not to bother with constructing the compute graph during\n",
    "        # the forward pass, since this is only needed for training.\n",
    "        with torch.no_grad():        \n",
    "\n",
    "            # Forward pass, calculate logits\n",
    "            # argmax(logits) = argmax(Softmax(logits))\n",
    "            outputs = model(b_input_ids, \n",
    "                                   attention_mask=b_input_mask,\n",
    "                                   labels=b_labels)\n",
    "            loss = outputs[0]\n",
    "            logits = outputs[1]\n",
    "            \n",
    "        # Accumulate the validation loss.\n",
    "        total_val_loss += loss.item()\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu()\n",
    "        label_ids = b_labels.to('cpu')\n",
    "\n",
    "        # metric on current batch\n",
    "        \"\"\"\n",
    "        batch_accuracy = accuracy(logits.softmax(dim=1), label_ids)\n",
    "        batch_f1 = f1(logits.softmax(dim=1), label_ids)\n",
    "        batch_precision = precision(logits.softmax(dim=1), label_ids)\n",
    "        batch_recall = recall(logits.softmax(dim=1), label_ids)\n",
    "        \"\"\"\n",
    "        batch_metric = metric_collection.update(logits.softmax(dim=1), label_ids)\n",
    "\n",
    "    # Report the final metrics for this validation phase.\n",
    "    # metric on all batches using custom accumulation from torchmetrics library\n",
    "    \"\"\"\n",
    "    epoch_accuracy = accuracy.compute()\n",
    "    epoch_f1 = f1.compute()\n",
    "    epoch_precision = precision.compute()\n",
    "    epoch_recall = recall.compute()\n",
    "    print(\"Validation Accuracy: {0:.2f}\".format(epoch_accuracy))  \n",
    "    print(\"Validation F1 score: {0:.2f}\".format(epoch_f1))  \n",
    "    print(\"Validation Precision: {0:.2f}\".format(epoch_precision))\n",
    "    print(\"Validation Recall: {0:.2f}\".format(epoch_recall)) \n",
    "    \"\"\"\n",
    "    final_metrics = metric_collection.compute()\n",
    "    print('VALIDATION: ')\n",
    "    print(final_metrics)\n",
    "    # Compute the average loss over all of the batches.\n",
    "    avg_val_loss = total_val_loss / len(validation_dataloader)\n",
    "    \n",
    "    # Measure how long the validation run took.\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "    \n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "#               Test\n",
    "# ========================================\n",
    "# Measure performance on\n",
    "# the validation set.\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "print(\"\")\n",
    "print(\"Running Tets...\")\n",
    "\"\"\"\n",
    "# Reset metric's interanl state\n",
    "accuracy.reset()\n",
    "f1.reset()\n",
    "precision.reset()\n",
    "recall.reset()\n",
    "\"\"\"\n",
    "metric_collection.reset()\n",
    "t0 = time.time()\n",
    "\n",
    "# Put the model in evaluation mode: the dropout layers behave differently\n",
    "model.eval()\n",
    "\n",
    "total_test_loss = 0\n",
    "\n",
    "# Evaluate data for one epoch\n",
    "for batch in test_dataloader:\n",
    "    \n",
    "    # Unpack this training batch from our dataloader. \n",
    "    #\n",
    "    # copy each tensor to the GPU using the 'to()' method\n",
    "    #\n",
    "    # 'batch' contains three pytorch tensors:\n",
    "    #   [0]: input ids \n",
    "    #   [1]: attention masks\n",
    "    #   [2]: labels \n",
    "    b_input_ids = batch['input_ids'].to(device)\n",
    "    b_input_mask = batch['attention_mask'].to(device)\n",
    "    b_labels = batch['labels'].to(device)\n",
    "    \n",
    "    # Tell pytorch not to bother with constructing the compute graph during\n",
    "    # the forward pass, since this is only needed for training.\n",
    "    with torch.no_grad():        \n",
    "\n",
    "        # Forward pass, calculate logits\n",
    "        # argmax(logits) = argmax(Softmax(logits))\n",
    "        outputs = model(b_input_ids, \n",
    "                                attention_mask=b_input_mask,\n",
    "                                labels=b_labels)\n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "        \n",
    "    # Accumulate the validation loss.\n",
    "    total_test_loss += loss.item()\n",
    "\n",
    "    # Move logits and labels to CPU\n",
    "    logits = logits.detach().cpu()\n",
    "    label_ids = b_labels.to('cpu')\n",
    "\n",
    "    # metric on current batch\n",
    "    \"\"\"\n",
    "    batch_accuracy = accuracy(logits.softmax(dim=1), label_ids)\n",
    "    batch_f1 = f1(logits.softmax(dim=1), label_ids)\n",
    "    batch_precision = precision(logits.softmax(dim=1), label_ids)\n",
    "    batch_recall = recall(logits.softmax(dim=1), label_ids)\n",
    "    \"\"\"\n",
    "    batch_metric = metric_collection.update(logits.softmax(dim=1), label_ids)\n",
    "\n",
    "# Report the final metrics for this validation phase.\n",
    "# metric on all batches using custom accumulation from torchmetrics library\n",
    "\"\"\"\n",
    "epoch_accuracy = accuracy.compute()\n",
    "epoch_f1 = f1.compute()\n",
    "epoch_precision = precision.compute()\n",
    "epoch_recall = recall.compute()\n",
    "print(\"Validation Accuracy: {0:.2f}\".format(epoch_accuracy))  \n",
    "print(\"Validation F1 score: {0:.2f}\".format(epoch_f1))  \n",
    "print(\"Validation Precision: {0:.2f}\".format(epoch_precision))\n",
    "print(\"Validation Recall: {0:.2f}\".format(epoch_recall)) \n",
    "\"\"\"\n",
    "test_metrics = metric_collection.compute()\n",
    "print(' Test metrics: ')\n",
    "print(final_metrics)\n",
    "\n",
    "run['metrics'] = final_metrics\n",
    "# Compute the average loss over all of the batches.\n",
    "avg_test_loss = total_test_loss / len(test_dataloader)\n",
    "\n",
    "run['test/loss'] = avg_test_loss\n",
    "\n",
    "# Measure how long the validation run took.\n",
    "test_time = format_time(time.time() - t0)\n",
    "\n",
    "print(\"  Test Loss: {0:.2f}\".format(avg_test_loss))\n",
    "print(\"  Test took: {:}\".format(test_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Accuracy': tensor([0.0000, 0.0000, 0.3333]), 'F1': tensor([0.0000, 0.0000, 0.2500]), 'Precision': tensor([0.0000, 0.0000, 0.2000]), 'Recall': tensor([0.0000, 0.0000, 0.3333])}\n",
      "{'Accuracy': tensor([0.3333, 0.0000, 0.5000]), 'F1': tensor([0.4000, 0.0000, 0.5000]), 'Precision': tensor([0.5000, 0.0000, 0.5000]), 'Recall': tensor([0.3333, 0.0000, 0.5000])}\n",
      "{'Accuracy': tensor([0.1429, 0.0000, 0.4286]), 'F1': tensor([0.2000, 0.0000, 0.3750]), 'Precision': tensor([0.3333, 0.0000, 0.3333]), 'Recall': tensor([0.1429, 0.0000, 0.4286])}\n"
     ]
    }
   ],
   "source": [
    "target = torch.tensor([0, 2, 0, 2, 0, 1, 0, 2])\n",
    "preds = torch.tensor([2, 1, 2, 0, 1, 2, 2, 2])\n",
    "\n",
    "import  torchmetrics\n",
    "\n",
    "# Metrics initialization\n",
    "metric_collection = torchmetrics.MetricCollection([\n",
    "    torchmetrics.Accuracy(num_classes=3, multiclass=True, average='none'),\n",
    "    torchmetrics.F1(num_classes=3, multiclass=True, average='none'),\n",
    "    torchmetrics.Precision(num_classes=3, multiclass=True, average='none'),\n",
    "    torchmetrics.Recall(num_classes=3, multiclass=True, average='none')\n",
    "])\n",
    "\n",
    "print(metric_collection(preds, target))\n",
    "\n",
    "target = torch.tensor([2, 2, 0, 2, 0, 1, 0, 2])\n",
    "preds = torch.tensor([2, 1, 2, 0, 1, 2, 0, 2])\n",
    "\n",
    "print(metric_collection(preds, target))\n",
    "\n",
    "\n",
    "print(metric_collection.compute())"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "531bf1ddd0b9ee64f0e2ebd6527c520de4e2159f2ffcc26ba5e4b06431092b93"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
