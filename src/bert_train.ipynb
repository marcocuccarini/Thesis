{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "stralcio + domanda -> repertorio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import neptune.new as neptune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/mibo8/Rep/e/REP-17\n",
      "Remember to stop your run once you’ve finished logging your metadata (https://docs.neptune.ai/api-reference/run#stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n"
     ]
    }
   ],
   "source": [
    "#Neptune initialization\n",
    "run = neptune.init(\n",
    "    project=\"mibo8/Rep\",\n",
    "    api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJmZmRkYThiZi1mZGNlLTRlMTktODQwNS1hNWFlMWQ2Mjc4N2IifQ==\",\n",
    ")  # your credentials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset loading and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_null_features(df):\n",
    "    for c in ['Domanda','Testo']:\n",
    "        for i in range(0,len(df.index)):  \n",
    "            if not df[c][i]:\n",
    "                j=i\n",
    "                while j>0: \n",
    "                    j-=1\n",
    "                    if df[c][j]:\n",
    "                        df[c][i] = df[c][j]\n",
    "                        break\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero stralci: 35474\n",
      "Numero stralci dopo eliminazione: 35148\n"
     ]
    }
   ],
   "source": [
    "#Hyperion dataset\n",
    "df = pd.read_csv('../data/Original_csv/Hyperion.csv', na_filter=False)\n",
    "df = fill_null_features(df)\n",
    "\n",
    "print('Numero stralci: ' + str(len(df.index)))\n",
    "filter = df[\"Repertorio\"] != \"\"\n",
    "df = df[filter]\n",
    "print('Numero stralci dopo eliminazione: ' + str(len(df.index)))\n",
    "\n",
    "# lower entire dataset\n",
    "df = df.applymap(str.lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lables uniformation uncased\n",
    "\n",
    "df['Repertorio'].replace('implicazioni','implicazione', inplace=True)\n",
    "df['Repertorio'].replace('previsioni','previsione', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class HyperionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        \n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X = Domanda + stralcio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "  \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-uncased\")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-multilingual-uncased\", num_labels=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "X_encodings = tokenizer(\n",
    "            df['Stralcio'][:1000].tolist(),\n",
    "            add_special_tokens=True,\n",
    "            return_attention_mask=True,\n",
    "            padding=True,\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "\n",
    "labels = [\n",
    "    'anticipazione',\n",
    "    'causa',\n",
    "    'commento',\n",
    "    'conferma',\n",
    "    'considerazione',\n",
    "    'contrapposizione',\n",
    "    'deresponsabilizzazione',\n",
    "    'descrizione',\n",
    "    'dichiarazione di intenti',\n",
    "    'generalizzazione',\n",
    "    'giudizio',\n",
    "    'giustificazione',\n",
    "    'implicazione',\n",
    "    'non risposta',\n",
    "    'opinione',\n",
    "    'possibilità',\n",
    "    'prescrizione',\n",
    "    'previsione',\n",
    "    'proposta',\n",
    "    'ridimensionamento',\n",
    "    'sancire',\n",
    "    'specificazione',\n",
    "    'valutazione']\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(labels)\n",
    "\n",
    "dataset = HyperionDataset(X_encodings,le.transform(df['Repertorio'][:1000]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_perc = 0.7\n",
    "val_set_perc = 0.1\n",
    "test_set_perc = 0.2\n",
    "\n",
    "dataset_info = {\n",
    "    'training_set_size' : train_set_perc,\n",
    "    'validation_set_size' : val_set_perc,\n",
    "    'test_set_size' : test_set_perc\n",
    "}\n",
    "run['dataset'] = dataset_info\n",
    "\n",
    "\n",
    "train_dataset_size = int(len(dataset) * train_set_perc)\n",
    "val_dataset_size = int(len(dataset) * val_set_perc)\n",
    "test_dataset_size = len(dataset) - train_dataset_size - val_dataset_size # 0.2\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_dataset_size, val_dataset_size, test_dataset_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  torchmetrics\n",
    "\n",
    "# Metrics initialization\n",
    "metric_collection = torchmetrics.MetricCollection({\n",
    "\n",
    "    'accuracy_micro' : torchmetrics.Accuracy(num_classes=23, multiclass=True, average='micro'),\n",
    "    'accuracy_macro' : torchmetrics.Accuracy(num_classes=23, multiclass=True, average='macro'),\n",
    "    'accuracy_weighted' : torchmetrics.Accuracy(num_classes=23, multiclass=True, average='weighted'),\n",
    "    'accuracy_none' : torchmetrics.Accuracy(num_classes=23, multiclass=True, average='none'),\n",
    "\n",
    "    'f1_micro' : torchmetrics.F1(num_classes=23, multiclass=True, average='micro'),\n",
    "    'f1_macro' : torchmetrics.F1(num_classes=23, multiclass=True, average='macro'),\n",
    "    'f1_weighted' : torchmetrics.F1(num_classes=23, multiclass=True, average='weighted'),\n",
    "    'f1_none' : torchmetrics.F1(num_classes=23, multiclass=True, average='none'),\n",
    "\n",
    "    'precision_micro' : torchmetrics.Precision(num_classes=23, multiclass=True, average='micro'),\n",
    "    'precision_macro' : torchmetrics.Precision(num_classes=23, multiclass=True, average='macro'),\n",
    "    'precision_weighted' : torchmetrics.Precision(num_classes=23, multiclass=True, average='weighted'),\n",
    "    'precision_none' : torchmetrics.Precision(num_classes=23, multiclass=True, average='none'),\n",
    "\n",
    "    'recall_micro' : torchmetrics.Recall(num_classes=23, multiclass=True, average='micro'),\n",
    "    'recall_macro' : torchmetrics.Recall(num_classes=23, multiclass=True, average='macro'),\n",
    "    'recall_weighted' : torchmetrics.Recall(num_classes=23, multiclass=True, average='weighted'),\n",
    "    'recall_none' : torchmetrics.Recall(num_classes=23, multiclass=True, average='none')\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 1e-5\n",
    "batch_size = 4\n",
    "n_epochs = 1\n",
    "\n",
    "params_info = {\n",
    "    'learning_rate' : learning_rate,\n",
    "    'batch_size' : batch_size,\n",
    "    'n_epochs' : n_epochs\n",
    "}\n",
    "run['params'] = params_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Repertorio\n",
       "anticipazione                 53\n",
       "causa                        473\n",
       "commento                    4403\n",
       "conferma                     639\n",
       "considerazione               312\n",
       "contrapposizione            1413\n",
       "deresponsabilizzazione       638\n",
       "descrizione                 4882\n",
       "dichiarazione di intenti     643\n",
       "generalizzazione            1066\n",
       "giudizio                    2764\n",
       "giustificazione              440\n",
       "implicazione                 706\n",
       "non risposta                1163\n",
       "opinione                    1343\n",
       "possibilità                  616\n",
       "prescrizione                1920\n",
       "previsione                  1170\n",
       "proposta                     256\n",
       "ridimensionamento           1102\n",
       "sancire                     5627\n",
       "specificazione              1396\n",
       "valutazione                 2123\n",
       "Name: Repertorio, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('Repertorio')['Repertorio'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 1 ========\n",
      "Training...\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import  AdamW\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Measure the total training time for the whole run.\n",
    "total_t0 = time.time()\n",
    "\n",
    "epochs = n_epochs\n",
    "\n",
    "# Creation of Pytorch DataLoaders with shuffle=True for the traing phase\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "validation_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "#Adam algorithm optimized for tranfor architectures\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Setup for training with gpu\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "# For each epoch...\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    # Perform one full pass over the training set.\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_train_loss = 0\n",
    "\n",
    "    # Put the model into training mode: Dropout layers are active\n",
    "    model.train()\n",
    "    \n",
    "    # For each batch of training data...\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        # Progress update every 40 batches.\n",
    "        if step % 10 == 0 and not step == 0:\n",
    "            # Compute time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        # Unpack this training batch from the dataloader. \n",
    "        #\n",
    "        #  copy each tensor to the GPU using the 'to()' method\n",
    "        #\n",
    "        # 'batch' contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch['input_ids'].to(device)\n",
    "        b_input_mask = batch['attention_mask'].to(device)\n",
    "        b_labels = batch['labels'].to(device)\n",
    "\n",
    "        # clear any previously calculated gradients before performing a\n",
    "        # backward pass\n",
    "        model.zero_grad()        \n",
    "\n",
    "        # Perform a forward pass (evaluate the model on this training batch).\n",
    "        # This will return the loss (rather than the model output) because we\n",
    "        # have provided the `labels`.\n",
    "        outputs = model(b_input_ids, \n",
    "                             attention_mask=b_input_mask, \n",
    "                             labels=b_labels)\n",
    "        \n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "\n",
    "        # Accumulate the training loss over all of the batches so that we can\n",
    "        # calculate the average loss at the end.\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu()\n",
    "        label_ids = b_labels.to('cpu')\n",
    "\n",
    "        batch_metric = metric_collection.update(logits.softmax(dim=1), label_ids)\n",
    "        #print(batch_metric)\n",
    "\n",
    "        # Perform a backward pass to compute the gradients.\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the norm of the gradients to 1.0.\n",
    "        # This helps and prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Update parameters and take a step using the computed gradient\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "    # Compute the average loss over all of the batches.\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "\n",
    "    final_metrics = metric_collection.compute()\n",
    "    print(final_metrics)\n",
    "    \n",
    "    # Measure how long this epoch took.\n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.3f}\".format(avg_train_loss))\n",
    "    print(\"  Training epoch took: {:}\".format(training_time))\n",
    "\n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure performance on\n",
    "    # the validation set.\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    metric_collection.reset()\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Put the model in evaluation mode: the dropout layers behave differently\n",
    "    model.eval()\n",
    "\n",
    "    total_val_loss = 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "        \n",
    "        # Unpack this training batch from our dataloader. \n",
    "        #\n",
    "        # copy each tensor to the GPU using the 'to()' method\n",
    "        #\n",
    "        # 'batch' contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch['input_ids'].to(device)\n",
    "        b_input_mask = batch['attention_mask'].to(device)\n",
    "        b_labels = batch['labels'].to(device)\n",
    "        \n",
    "        # Tell pytorch not to bother with constructing the compute graph during\n",
    "        # the forward pass, since this is only needed for training.\n",
    "        with torch.no_grad():        \n",
    "\n",
    "            # Forward pass, calculate logits\n",
    "            # argmax(logits) = argmax(Softmax(logits))\n",
    "            outputs = model(b_input_ids, \n",
    "                                   attention_mask=b_input_mask,\n",
    "                                   labels=b_labels)\n",
    "            loss = outputs[0]\n",
    "            logits = outputs[1]\n",
    "            \n",
    "        # Accumulate the validation loss.\n",
    "        total_val_loss += loss.item()\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu()\n",
    "        label_ids = b_labels.to('cpu')\n",
    "\n",
    "        # metric on current batch\n",
    "        batch_metric = metric_collection.update(logits.softmax(dim=1), label_ids)\n",
    "\n",
    "    # Report the final metrics for this validation phase.\n",
    "    # metric on all batches using custom accumulation from torchmetrics library\n",
    "\n",
    "    final_metrics = metric_collection.compute()\n",
    "    print('VALIDATION: ')\n",
    "    print(final_metrics)\n",
    "    # Compute the average loss over all of the batches.\n",
    "    avg_val_loss = total_val_loss / len(validation_dataloader)\n",
    "    \n",
    "    # Measure how long the validation run took.\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "    \n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([21, 21, 21, 21])\n",
      "tensor([14, 18,  7, 20])\n"
     ]
    }
   ],
   "source": [
    "print(logits.softmax(dim=1).argmax(dim=-1))\n",
    "print(label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DataLoader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_25184/1054990993.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# the validation set.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtest_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DataLoader' is not defined"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "#               Test\n",
    "# ========================================\n",
    "# Measure performance on\n",
    "# the validation set.\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "print(\"\")\n",
    "print(\"Running Tets...\")\n",
    "\n",
    "metric_collection.reset()\n",
    "t0 = time.time()\n",
    "\n",
    "# Put the model in evaluation mode: the dropout layers behave differently\n",
    "model.eval()\n",
    "\n",
    "total_test_loss = 0\n",
    "\n",
    "# Evaluate data for one epoch\n",
    "for batch in test_dataloader:\n",
    "    \n",
    "    # Unpack this training batch from our dataloader. \n",
    "    #\n",
    "    # copy each tensor to the GPU using the 'to()' method\n",
    "    #\n",
    "    # 'batch' contains three pytorch tensors:\n",
    "    #   [0]: input ids \n",
    "    #   [1]: attention masks\n",
    "    #   [2]: labels \n",
    "    b_input_ids = batch['input_ids'].to(device)\n",
    "    b_input_mask = batch['attention_mask'].to(device)\n",
    "    b_labels = batch['labels'].to(device)\n",
    "    \n",
    "    # Tell pytorch not to bother with constructing the compute graph during\n",
    "    # the forward pass, since this is only needed for training.\n",
    "    with torch.no_grad():        \n",
    "\n",
    "        # Forward pass, calculate logits\n",
    "        # argmax(logits) = argmax(Softmax(logits))\n",
    "        outputs = model(b_input_ids, \n",
    "                                attention_mask=b_input_mask,\n",
    "                                labels=b_labels)\n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "        \n",
    "    # Accumulate the validation loss.\n",
    "    total_test_loss += loss.item()\n",
    "\n",
    "    # Move logits and labels to CPU\n",
    "    logits = logits.detach().cpu()\n",
    "    label_ids = b_labels.to('cpu')\n",
    "\n",
    "    # metric on current batch\n",
    "\n",
    "    batch_metric = metric_collection.update(logits.softmax(dim=1), label_ids)\n",
    "\n",
    "# Report the final metrics for this validation phase.\n",
    "# metric on all batches using custom accumulation from torchmetrics library\n",
    "\n",
    "test_metrics = metric_collection.compute()\n",
    "print(' Test metrics: ')\n",
    "print(final_metrics)\n",
    "\n",
    "run['metrics'] = final_metrics\n",
    "# Compute the average loss over all of the batches.\n",
    "avg_test_loss = total_test_loss / len(test_dataloader)\n",
    "\n",
    "run['test/loss'] = avg_test_loss\n",
    "\n",
    "# Measure how long the validation run took.\n",
    "test_time = format_time(time.time() - t0)\n",
    "\n",
    "print(\"  Test Loss: {0:.2f}\".format(avg_test_loss))\n",
    "print(\"  Test took: {:}\".format(test_time))\n",
    "\n",
    "torch.save(model.state_dict(), PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Waiting for the remaining 1 operations to synchronize with Neptune. Do not kill this process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All 1 operations synced, thanks for waiting!\n"
     ]
    }
   ],
   "source": [
    "run.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7500)\n",
      "tensor(0.7667)\n",
      "tensor(0.7500)\n",
      "tensor(0.7500)\n",
      "tensor(0.7667)\n",
      "tensor(0.7500)\n",
      "tensor(0.7500)\n",
      "tensor(0.7667)\n",
      "tensor(0.7500)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "import  torchmetrics\n",
    "# Metrics initialization\n",
    "metric_collection = torchmetrics.MetricCollection({\n",
    "\n",
    "    'accuracy_micro' : torchmetrics.Accuracy(num_classes=3, multiclass=True, average='micro'),\n",
    "    'accuracy_macro' : torchmetrics.Accuracy(num_classes=3, multiclass=True, average='macro'),\n",
    "    'accuracy_weighted' : torchmetrics.Accuracy(num_classes=3, multiclass=True, average='weighted'),\n",
    "    'accuracy_none' : torchmetrics.Accuracy(num_classes=3, multiclass=True, average='none'),\n",
    "\n",
    "    'f1_micro' : torchmetrics.F1(num_classes=3, multiclass=True, average='micro'),\n",
    "    'f1_macro' : torchmetrics.F1(num_classes=3, multiclass=True, average='macro'),\n",
    "    'f1_weighted' : torchmetrics.F1(num_classes=3, multiclass=True, average='weighted'),\n",
    "    'f1_none' : torchmetrics.F1(num_classes=3, multiclass=True, average='none'),\n",
    "\n",
    "    'precision_micro' : torchmetrics.Precision(num_classes=3, multiclass=True, average='micro'),\n",
    "    'precision_macro' : torchmetrics.Precision(num_classes=3, multiclass=True, average='macro'),\n",
    "    'precision_weighted' : torchmetrics.Precision(num_classes=3, multiclass=True, average='weighted'),\n",
    "    'precision_none' : torchmetrics.Precision(num_classes=3, multiclass=True, average='none'),\n",
    "\n",
    "    'recall_micro' : torchmetrics.Recall(num_classes=3, multiclass=True, average='micro'),\n",
    "    'recall_macro' : torchmetrics.Recall(num_classes=3, multiclass=True, average='macro'),\n",
    "    'recall_weighted' : torchmetrics.Recall(num_classes=3, multiclass=True, average='weighted'),\n",
    "    'recall_none' : torchmetrics.Recall(num_classes=3, multiclass=True, average='none')\n",
    "})\n",
    "metric_collection.reset()\n",
    "\n",
    "#target = torch.tensor([0, 2, 0, 2, 0, 1, 0, 2])\n",
    "#preds = torch.tensor([2, 1, 2, 0, 1, 2, 2, 2])\n",
    "\n",
    "#metric_collection.update(preds, target)\n",
    "\n",
    "target =torch.tensor([2, 2, 2, 2, 1, 1, 0, 2])\n",
    "preds = torch.tensor([2, 1, 2, 2, 1, 2, 0, 2])\n",
    "\n",
    "metric_collection.update(preds, target)\n",
    "\n",
    "metric = metric_collection.compute()\n",
    "print(metric['accuracy_micro'])     \n",
    "print(metric['accuracy_macro'])\n",
    "print(metric['accuracy_weighted'])     \n",
    "print(metric['precision_micro'])    \n",
    "print(metric['precision_macro'])  \n",
    "print(metric['precision_weighted']) # 1/2*1/8 + 1/2*2/8 + 4/5*5/8\n",
    "print(metric['recall_micro'])\n",
    "print(metric['recall_macro'])\n",
    "print(metric['recall_weighted'])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "531bf1ddd0b9ee64f0e2ebd6527c520de4e2159f2ffcc26ba5e4b06431092b93"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
